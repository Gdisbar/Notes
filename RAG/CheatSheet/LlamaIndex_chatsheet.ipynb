{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "slf_hzxjR3iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGFmxNnfRFsl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex Cheatsheet - Open Source Models + Pinecone\n",
        "\n",
        "## Installation & Setup\n",
        "\n",
        "```bash\n",
        "# Core installations\n",
        "pip install llama-index\n",
        "pip install pinecone-client\n",
        "pip install sentence-transformers\n",
        "pip install transformers torch\n",
        "\n",
        "# For specific model providers\n",
        "pip install llama-index-llms-ollama\n",
        "pip install llama-index-llms-huggingface\n",
        "pip install llama-index-embeddings-huggingface\n",
        "pip install llama-index-vector-stores-pinecone\n",
        "```\n",
        "\n",
        "## Basic Configuration\n",
        "\n",
        "```python\n",
        "import os\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# Setup Pinecone\n",
        "pc = Pinecone(api_key=\"your-pinecone-api-key\")\n",
        "index = pc.Index(\"your-index-name\")\n",
        "\n",
        "# Configure global settings\n",
        "Settings.llm = Ollama(model=\"llama3.1:8b\", request_timeout=120.0)\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Setup vector store\n",
        "vector_store = PineconeVectorStore(\n",
        "    pinecone_index=index,\n",
        "    namespace=\"default\"\n",
        ")\n",
        "```\n",
        "\n",
        "## Open Source Model Options\n",
        "\n",
        "### LLMs via Ollama\n",
        "```python\n",
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "# Popular models\n",
        "llama_3_1 = Ollama(model=\"llama3.1:8b\")\n",
        "codellama = Ollama(model=\"codellama:7b\")\n",
        "mistral = Ollama(model=\"mistral:7b\")\n",
        "phi3 = Ollama(model=\"phi3:mini\")\n",
        "```\n",
        "\n",
        "### LLMs via HuggingFace\n",
        "```python\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"microsoft/DialoGPT-medium\",\n",
        "    tokenizer_name=\"microsoft/DialoGPT-medium\",\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"do_sample\": False}\n",
        ")\n",
        "```\n",
        "\n",
        "### Embedding Models\n",
        "```python\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Popular embedding models\n",
        "bge_small = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "bge_large = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n",
        "e5_base = HuggingFaceEmbedding(model_name=\"intfloat/e5-base-v2\")\n",
        "sentence_bert = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "```\n",
        "\n",
        "## Use Case 1: Basic RAG with Document Loading\n",
        "\n",
        "```python\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "\n",
        "# Load documents\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
        "\n",
        "# Create storage context with Pinecone\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Create index\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context\n",
        ")\n",
        "\n",
        "# Query\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is the main topic of these documents?\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "## Use Case 2: Chat Engine for Conversational AI\n",
        "\n",
        "```python\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "\n",
        "# Create chat engine with memory\n",
        "memory = ChatMemoryBuffer.from_defaults(token_limit=3000)\n",
        "\n",
        "chat_engine = index.as_chat_engine(\n",
        "    chat_mode=\"condense_plus_context\",\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Multi-turn conversation\n",
        "response1 = chat_engine.chat(\"Tell me about machine learning\")\n",
        "response2 = chat_engine.chat(\"What are its main applications?\")\n",
        "response3 = chat_engine.chat(\"How does it relate to the previous topic?\")\n",
        "```\n",
        "\n",
        "## Use Case 3: Multi-Document Analysis\n",
        "\n",
        "```python\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Load multiple document types\n",
        "documents = []\n",
        "documents.extend(SimpleDirectoryReader(\"./pdfs\").load_data())\n",
        "documents.extend(SimpleDirectoryReader(\"./text_files\").load_data())\n",
        "\n",
        "# Custom chunking\n",
        "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# Create index with custom nodes\n",
        "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "# Advanced querying\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=10,\n",
        "    response_mode=\"tree_summarize\"\n",
        ")\n",
        "\n",
        "response = query_engine.query(\"Compare the key findings across all documents\")\n",
        "```\n",
        "\n",
        "## Use Case 4: Code Analysis and Q&A\n",
        "\n",
        "```python\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "# Use CodeLlama for code analysis\n",
        "Settings.llm = Ollama(model=\"codellama:7b\")\n",
        "\n",
        "# Load code files\n",
        "code_documents = SimpleDirectoryReader(\n",
        "    \"./src\",\n",
        "    file_extractor={\n",
        "        \".py\": \"python\",\n",
        "        \".js\": \"javascript\",\n",
        "        \".java\": \"java\"\n",
        "    }\n",
        ").load_data()\n",
        "\n",
        "# Create specialized index for code\n",
        "code_index = VectorStoreIndex.from_documents(\n",
        "    code_documents,\n",
        "    storage_context=storage_context\n",
        ")\n",
        "\n",
        "query_engine = code_index.as_query_engine()\n",
        "response = query_engine.query(\"Explain the main functions in this codebase\")\n",
        "```\n",
        "\n",
        "## Use Case 5: Structured Data Extraction\n",
        "\n",
        "```python\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "class CompanyInfo(BaseModel):\n",
        "    name: str\n",
        "    industry: str\n",
        "    key_products: List[str]\n",
        "    revenue: str\n",
        "\n",
        "# Custom prompt for extraction\n",
        "extraction_prompt = PromptTemplate(\n",
        "    \"Extract company information from the following text:\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"Return the information in the specified JSON format.\"\n",
        ")\n",
        "\n",
        "output_parser = PydanticOutputParser(CompanyInfo)\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    output_parser=output_parser,\n",
        "    text_qa_template=extraction_prompt\n",
        ")\n",
        "\n",
        "response = query_engine.query(\"Extract company information from the documents\")\n",
        "```\n",
        "\n",
        "## Use Case 6: Multi-Modal RAG (Text + Images)\n",
        "\n",
        "```python\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.multi_modal_llms.ollama import OllamaMultiModal\n",
        "\n",
        "# Use multi-modal model\n",
        "mm_llm = OllamaMultiModal(model=\"llava:7b\")\n",
        "\n",
        "# Load documents with images\n",
        "documents = SimpleDirectoryReader(\n",
        "    \"./mixed_content\",\n",
        "    file_extractor={\n",
        "        \".jpg\": \"image\",\n",
        "        \".png\": \"image\",\n",
        "        \".pdf\": \"pdf\"\n",
        "    }\n",
        ").load_data()\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(llm=mm_llm)\n",
        "response = query_engine.query(\"Describe what you see in the images and how it relates to the text\")\n",
        "```\n",
        "\n",
        "## Use Case 7: Knowledge Graph Construction\n",
        "\n",
        "```python\n",
        "from llama_index.core import KnowledgeGraphIndex\n",
        "from llama_index.graph_stores.simple import SimpleGraphStore\n",
        "\n",
        "# Create knowledge graph\n",
        "graph_store = SimpleGraphStore()\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store,\n",
        "    graph_store=graph_store\n",
        ")\n",
        "\n",
        "kg_index = KnowledgeGraphIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        "    max_triplets_per_chunk=10\n",
        ")\n",
        "\n",
        "# Query the knowledge graph\n",
        "kg_query_engine = kg_index.as_query_engine(\n",
        "    include_text=False,\n",
        "    response_mode=\"tree_summarize\"\n",
        ")\n",
        "\n",
        "response = kg_query_engine.query(\"What are the relationships between different entities?\")\n",
        "```\n",
        "\n",
        "## Use Case 8: Document Comparison and Summarization\n",
        "\n",
        "```python\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# Create separate indices for different document sets\n",
        "index1 = VectorStoreIndex.from_documents(doc_set_1, storage_context=storage_context)\n",
        "index2 = VectorStoreIndex.from_documents(doc_set_2, storage_context=storage_context)\n",
        "\n",
        "# Create query engine tools\n",
        "tool1 = QueryEngineTool.from_defaults(\n",
        "    query_engine=index1.as_query_engine(),\n",
        "    description=\"Contains information about Product A\"\n",
        ")\n",
        "\n",
        "tool2 = QueryEngineTool.from_defaults(\n",
        "    query_engine=index2.as_query_engine(),\n",
        "    description=\"Contains information about Product B\"\n",
        ")\n",
        "\n",
        "# Sub-question query engine for comparison\n",
        "comparison_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=[tool1, tool2]\n",
        ")\n",
        "\n",
        "response = comparison_engine.query(\"Compare Product A and Product B features\")\n",
        "```\n",
        "\n",
        "## Advanced Pinecone Configuration\n",
        "\n",
        "```python\n",
        "# Create Pinecone index with specific configuration\n",
        "pc.create_index(\n",
        "    name=\"llamaindex-demo\",\n",
        "    dimension=384,  # Match your embedding model dimension\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(\n",
        "        cloud='aws',\n",
        "        region='us-east-1'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Advanced vector store setup with metadata filtering\n",
        "vector_store = PineconeVectorStore(\n",
        "    pinecone_index=index,\n",
        "    namespace=\"documents\",\n",
        "    text_key=\"content\",\n",
        "    metadata_filters={\"category\": \"technical\"}\n",
        ")\n",
        "```\n",
        "\n",
        "## Performance Optimization Tips\n",
        "\n",
        "```python\n",
        "# 1. Optimize chunk sizes\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "node_parser = SentenceSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=100,\n",
        "    separator=\" \"\n",
        ")\n",
        "\n",
        "# 2. Use async for better performance\n",
        "from llama_index.core import VectorStoreIndex\n",
        "import asyncio\n",
        "\n",
        "async def async_query():\n",
        "    query_engine = index.as_query_engine()\n",
        "    response = await query_engine.aquery(\"Your question here\")\n",
        "    return response\n",
        "\n",
        "# 3. Batch processing\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        node_parser,\n",
        "        Settings.embed_model\n",
        "    ],\n",
        "    vector_store=vector_store\n",
        ")\n",
        "\n",
        "pipeline.run(documents=documents)\n",
        "```\n",
        "\n",
        "## Monitoring and Debugging\n",
        "\n",
        "```python\n",
        "import logging\n",
        "\n",
        "# Enable debug logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Add callbacks for monitoring\n",
        "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
        "\n",
        "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
        "callback_manager = CallbackManager([llama_debug])\n",
        "\n",
        "Settings.callback_manager = callback_manager\n",
        "\n",
        "# Query with tracing\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Your question\")\n",
        "\n",
        "# Print event traces\n",
        "llama_debug.print_trace()\n",
        "```\n",
        "\n",
        "## Common Patterns\n",
        "\n",
        "### Pattern 1: Hybrid Search (Vector + Keyword)\n",
        "```python\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=20\n",
        ")\n",
        "\n",
        "reranker = SentenceTransformerRerank(\n",
        "    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
        "    top_n=5\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[reranker]\n",
        ")\n",
        "```\n",
        "\n",
        "### Pattern 2: Custom Response Synthesis\n",
        "```python\n",
        "from llama_index.core.response_synthesizers import TreeSummarize\n",
        "\n",
        "response_synthesizer = TreeSummarize(\n",
        "    summary_template=\"\"\"\n",
        "    Based on the context information:\n",
        "    {context_str}\n",
        "    \n",
        "    Please provide a comprehensive answer to: {query_str}\n",
        "    \n",
        "    Format your response with clear sections and bullet points where appropriate.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    response_synthesizer=response_synthesizer\n",
        ")\n",
        "```\n",
        "\n",
        "## Error Handling Best Practices\n",
        "\n",
        "```python\n",
        "try:\n",
        "    # Initialize components with error handling\n",
        "    if not pc.list_indexes():\n",
        "        pc.create_index(name=\"backup-index\", dimension=384, metric=\"cosine\")\n",
        "    \n",
        "    response = query_engine.query(\"Your question\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error during querying: {e}\")\n",
        "    # Fallback logic\n",
        "    response = \"I encountered an error processing your request.\"\n",
        "```\n",
        "\n",
        "This cheatsheet covers the most common LlamaIndex patterns using open-source models and Pinecone. Adjust the model names, chunk sizes, and configurations based on your specific requirements and hardware capabilities."
      ],
      "metadata": {
        "id": "WVFiLpeKRGIL"
      }
    }
  ]
}