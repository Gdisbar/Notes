{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08faf30d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2098d1ce",
   "metadata": {},
   "source": [
    "# Types of Naive Bays \n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "    1. Naive Bayes\n",
    "    1. Gaussian Naive Bayes\n",
    "    3. Multinomial Naive Bayes\n",
    "    4. Complement Naive Bayes\n",
    "    5. Bernoulli Naive Bayes\n",
    "    6. Categorical Naive Bayes\n",
    "    \n",
    "**Out-of-core naive Bayes model fitting :**\n",
    "\n",
    "Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, MultinomialNB, BernoulliNB, and GaussianNB expose a partial_fit method that can be used incrementally as done with other classifiers as demonstrated in Out-of-core classification of text documents. All naive Bayes classifiers support sample weighting.\n",
    "\n",
    "Contrary to the fit method, the first call to partial_fit needs to be passed the list of all the expected class labels.\n",
    "\n",
    "For an overview of available strategies in scikit-learn, see also the out-of-core [https://scikit-learn.org/stable/computing/scaling_strategies.html#scaling-strategies] learning documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"score = (y_true - y_pred) / len(y_true) \"\"\"\n",
    "    return round(float(sum(y_pred == y_true))/float(len(y_true)) * 100 ,2)\n",
    "\n",
    "def pre_processing(df):\n",
    "    \n",
    "    \"\"\" partioning data into features and target \"\"\"\n",
    "    X = df.drop([df.columns[-1]], axis = 1)\n",
    "    y = df[df.columns[-1]]\n",
    "    return X, y\n",
    "\n",
    "def train_test_split(x, y, test_size = 0.25, random_state = None):\n",
    "\n",
    "    \"\"\" partioning the data into train and test sets \"\"\"\n",
    "\n",
    "    x_test = x.sample(frac = test_size, random_state = random_state)\n",
    "    y_test = y[x_test.index]\n",
    "\n",
    "    x_train = x.drop(x_test.index)\n",
    "    y_train = y.drop(y_test.index)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89fc36",
   "metadata": {},
   "source": [
    "### Calculate Prior Probability of Classes P(y)\n",
    "\n",
    "\n",
    " #### Frequency table\n",
    "    \n",
    "    P(Play=Yes) = 9/14 = 0.64\n",
    "    P(Play=No) = 5/14 = 0.36\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd22ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_class_prior(self):\n",
    "\n",
    "    \"\"\" P(c) - Prior Class Probability\"\"\"\n",
    "\n",
    "    for outcome in np.unique(self.y_train):\n",
    "        outcome_count = sum(self.y_train == outcome)\n",
    "        self.class_priors[outcome] = outcome_count / self.train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df52ae",
   "metadata": {},
   "source": [
    "### Calculate the Likelihood Table for all features\n",
    "\n",
    "#Likelihood Table - Outlook\n",
    "\n",
    "|Play | Overcast| Rainy| Sunny|            |Play     | Cool    | Mild | Hot  |         \n",
    "|-----|---------|------|------|            |---------|---------|------|------|\n",
    "|Yes  | 4/9     | 2/9  | 3/9  |            |Yes      | 3/9     | 4/9  | 2/9  |\n",
    "|No   | 0/5     | 3/5  | 2/5  |            |No       | 1/5     | 2/5  | 2/5  |  \n",
    "|#Temp|    4/14 | 5/14 | 5/14 |            |#Humidity|    4/14 | 6/14 | 4/14 |   \n",
    "            \n",
    "      \n",
    "|Play   |High | Normal|     |Play  | f   |  t   |\n",
    "|-------|-----|-------|     |------|-----|------|\n",
    "|Yes    |3/9  | 6/9   |     |Yes   | 6/9 |  3/9 |\n",
    "|No     |4/5  | 1/5   |     |No    | 2/5 |  3/5 |\n",
    "|#Windy | 7/14| 7/14  |     |      | 8/14|  6/14|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36935340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_likelihoods(self):\n",
    "\n",
    "    for feature in self.features:\n",
    "\n",
    "        for outcome in np.unique(self.y_train):\n",
    "            outcome_count = sum(self.y_train == outcome)\n",
    "            feat_likelihood = self.X_train[feature][self.y_train[self.y_train == outcome].index.values.tolist()].value_counts().to_dict()\n",
    "\n",
    "            for feat_val, count in feat_likelihood.items():\n",
    "                self.likelihoods[feature][feat_val + '_' + outcome] = count/outcome_count\n",
    "\n",
    "\n",
    "def _calc_predictor_prior(self):\n",
    "\n",
    "    for feature in self.features:\n",
    "        feat_vals = self.X_train[feature].value_counts().to_dict()\n",
    "\n",
    "            for feat_val, count in feat_vals.items():\n",
    "                self.pred_priors[feature][feat_val] = count/self.train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae31679",
   "metadata": {},
   "source": [
    "***Now, Calculate Posterior Probability for each class using the Naive Bayesian equation. The Class with maximum probability is the outcome of the prediction.***\n",
    "\n",
    "***Query: Whether Players will play or not when the weather conditions are [Outlook=Rainy, Temp=Mild, Humidity=Normal, Windy=t]?***\n",
    "\n",
    "Calculation of Posterior Probability:\n",
    "\n",
    "P(y=Yes|x) = P(Yes|Rainy,Mild,Normal,t)    \n",
    "\n",
    "            P(Rainy,Mild,Normal,t|Yes) * P(Yes)\n",
    "       = ___________________________________\n",
    "                P(Rainy,Mild,Normal,t)        \n",
    "                \n",
    "                P(Rainy|Yes)*P(Mild|Yes)*P(Normal|Yes)*P(t|Yes)*P(Yes)\n",
    "       = ________________________________________________________________\n",
    "                    P(Rainy)*P(Mild)*P(Normal)*P(t)\n",
    "\n",
    "\n",
    "\n",
    "Since Conditional independence of two random variables, A and B gave C holds just in case<br>   P(A, B | C) = P(A | C) * P(B | C)\n",
    "\n",
    "         (2/9) * (4/9) * (6/9) * (3/9) * (9/14)\n",
    "       = _______________________________________\n",
    "            (5/14) * (6/14) * (7/14) * (6/14)\n",
    "       \n",
    "       = 0.43 P(y=No|x) = P(No|Rainy,Mild,Normal,t)         \n",
    "       \n",
    "           P(Rainy,Mild,Normal,t|No) * P(No)\n",
    "       = ___________________________________\n",
    "                P(Rainy,Mild,Normal,t)      \n",
    "                \n",
    "                \n",
    "             P(Rainy|No)*P(Mild|No)*P(Normal|No)*P(t|No)*P(No)\n",
    "       = ______________________________________________________\n",
    "                    P(Rainy)*P(Mild)*P(Normal)*P(t)          \n",
    "                    \n",
    "                    \n",
    "           (3/5) * (2/5) * (1/5) * (3/5) * (5/14)\n",
    "       = _______________________________________\n",
    "            (5/14) * (6/14) * (7/14) * (6/14)\n",
    "       \n",
    "       = 0.31\n",
    "       \n",
    "       \n",
    "       \n",
    "Now, P(Play=Yes|Rainy,Mild,Normal,t) has the highest Posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a01aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "\n",
    "    self.features = list(X.columns)\n",
    "    self.X_train = X\n",
    "    self.y_train = y\n",
    "    self.train_size = X.shape[0]\n",
    "    self.num_feats = X.shape[1]\n",
    "\n",
    "    for feature in self.features:\n",
    "        self.likelihoods[feature] = {}\n",
    "        self.pred_priors[feature] = {}\n",
    "\n",
    "        for feat_val in np.unique(self.X_train[feature]):\n",
    "            self.pred_priors[feature].update({feat_val: 0})\n",
    "\n",
    "            for outcome in np.unique(self.y_train):\n",
    "                self.likelihoods[feature].update({feat_val+'_'+outcome:0})\n",
    "                self.class_priors.update({outcome: 0})\n",
    "\n",
    "    self._calc_class_prior()\n",
    "    self._calc_likelihoods()\n",
    "    self._calc_predictor_prior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "\n",
    "    \"\"\" Calculates Posterior probability P(c|x) \"\"\"\n",
    "\n",
    "    results = []\n",
    "    X = np.array(X)\n",
    "\n",
    "    for query in X:\n",
    "        probs_outcome = {}\n",
    "        for outcome in np.unique(self.y_train):\n",
    "            prior = self.class_priors[outcome]\n",
    "            likelihood = 1\n",
    "            evidence = 1\n",
    "\n",
    "            for feat, feat_val in zip(self.features, query):\n",
    "                likelihood *= self.likelihoods[feat][feat_val + '_' + outcome]\n",
    "                evidence *= self.pred_priors[feat][feat_val]\n",
    "\n",
    "            posterior = (likelihood * prior) / (evidence)\n",
    "\n",
    "            probs_outcome[outcome] = posterior\n",
    "\n",
    "        result = max(probs_outcome, key = lambda x: probs_outcome[x])\n",
    "        results.append(result)\n",
    "\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518b9f3",
   "metadata": {},
   "source": [
    "Weather Dataset:Train Accuracy: 92.86\n",
    "\n",
    "Query 1:- [['Rainy' 'Mild' 'Normal' 't']] ---> ['yes']\n",
    "\n",
    "Query 2:- [['Overcast' 'Cool' 'Normal' 't']] ---> ['yes']\n",
    "\n",
    "Query 3:- [['Sunny' 'Hot' 'High' 't']] ---> ['no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  NaiveBayes:\n",
    "\n",
    "    \"\"\"\n",
    "        Bayes Theorem:\n",
    "                                        Likelihood * Class prior probability\n",
    "                Posterior Probability = -------------------------------------\n",
    "                                            Predictor prior probability\n",
    "\n",
    "                                         P(x|c) * p(c)\n",
    "                               P(c|x) = ------------------ \n",
    "                                              P(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "            Attributes:\n",
    "                likelihoods: Likelihood of each feature per class\n",
    "                class_priors: Prior probabilities of classes \n",
    "                pred_priors: Prior probabilities of features \n",
    "                features: All features of dataset\n",
    "        \"\"\"\n",
    "        self.features = list\n",
    "        self.likelihoods = {}\n",
    "        self.class_priors = {}\n",
    "        self.pred_priors = {}\n",
    "\n",
    "        self.X_train = np.array\n",
    "        self.y_train = np.array\n",
    "        self.train_size = int\n",
    "        self.num_feats = int\n",
    "\n",
    "    def fit(self, X, y): #code\n",
    "\n",
    "    def _calc_class_prior(self): #code\n",
    "\n",
    "    def _calc_likelihoods(self): #code\n",
    "\n",
    "    def _calc_predictor_prior(self): # code\n",
    "\n",
    "    def predict(self, X): #code\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Weather Dataset\n",
    "    print(\"\\nWeather Dataset:\")\n",
    "\n",
    "    df = pd.read_table(\"../Data/weather.txt\")\n",
    "    #print(df)\n",
    "\n",
    "    #Split fearures and target\n",
    "    X,y  = pre_processing(df)\n",
    "\n",
    "    nb_clf = NaiveBayes()\n",
    "    nb_clf.fit(X, y)\n",
    "\n",
    "    print(\"Train Accuracy: {}\".format(accuracy_score(y, nb_clf.predict(X))))\n",
    "\n",
    "    #Query 1:\n",
    "    query = np.array([['Rainy','Mild', 'Normal', 't']])\n",
    "    print(\"Query 1:- {} ---> {}\".format(query, nb_clf.predict(query)))\n",
    "\n",
    "    #Query 2:\n",
    "    query = np.array([['Overcast','Cool', 'Normal', 't']])\n",
    "    print(\"Query 2:- {} ---> {}\".format(query, nb_clf.predict(query)))\n",
    "\n",
    "    #Query 3:\n",
    "    query = np.array([['Sunny','Hot', 'High', 't']])\n",
    "    print(\"Query 3:- {} ---> {}\".format(query, nb_clf.predict(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d9e51",
   "metadata": {},
   "source": [
    "# Naive Bays - GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_likelihoods(self):\n",
    "\n",
    "        \"\"\" P(x|c) - Likelihood \"\"\"\n",
    "\n",
    "        for feature in self.features:\n",
    "\n",
    "            for outcome in np.unique(self.y_train):\n",
    "                self.likelihoods[feature][outcome]['mean'] = self.X_train[feature][self.y_train[self.y_train == outcome].index.values.tolist()].mean()\n",
    "                self.likelihoods[feature][outcome]['variance'] = self.X_train[feature][self.y_train[self.y_train == outcome].index.values.tolist()].var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "\n",
    "        self.features = list(X.columns)\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        self.train_size = X.shape[0]\n",
    "        self.num_feats = X.shape[1]\n",
    "\n",
    "        for feature in self.features:\n",
    "            self.likelihoods[feature] = {}\n",
    "\n",
    "            for outcome in np.unique(self.y_train):\n",
    "                self.likelihoods[feature].update({outcome:{}})\n",
    "                self.class_priors.update({outcome: 0})\n",
    "\n",
    "\n",
    "        self._calc_class_prior()\n",
    "        self._calc_likelihoods()\n",
    "\n",
    "        # print(self.likelihoods)\n",
    "        # print(self.class_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1905e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "\n",
    "    \"\"\" Calculates Posterior probability P(c|x) \"\"\"\n",
    "\n",
    "    results = []\n",
    "    X = np.array(X)\n",
    "\n",
    "    for query in X:\n",
    "        probs_outcome = {}\n",
    "\n",
    "        \"\"\"\n",
    "            Note: No Need to calculate evidence i.e P(x) since it is constant fot the given sample.\n",
    "                  Therfore, it does not affect classification and can be ignored\n",
    "        \"\"\"\n",
    "        for outcome in np.unique(self.y_train):\n",
    "            prior = self.class_priors[outcome]\n",
    "            likelihood = 1\n",
    "            evidence_temp = 1\n",
    "\n",
    "            for feat, feat_val in zip(self.features, query):\n",
    "                mean = self.likelihoods[feat][outcome]['mean']\n",
    "                var = self.likelihoods[feat][outcome]['variance']\n",
    "                likelihood *= (1/math.sqrt(2*math.pi*var)) * np.exp(-(feat_val - mean)**2 / (2*var))\n",
    "\n",
    "            posterior_numerator = (likelihood * prior)\n",
    "            probs_outcome[outcome] = posterior_numerator\n",
    "\n",
    "\n",
    "        result = max(probs_outcome, key = lambda x: probs_outcome[x])\n",
    "        results.append(result)\n",
    "\n",
    "    return np.array(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb773f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  GaussianNB:\n",
    "\n",
    "    \"\"\"\n",
    "        Bayes Theorem:\n",
    "                                        Likelihood * Class prior probability\n",
    "                Posterior Probability = -------------------------------------\n",
    "                                            Predictor prior probability\n",
    "\n",
    "                                         P(x|c) * p(c)\n",
    "                               P(c|x) = ------------------ \n",
    "                                              P(x)\n",
    "        Gaussian Naive Bayes:\n",
    "                                     1\n",
    "                P(x|c) = --------------------------- * exp(- (x - mean)^2 / 2*(var(x)^2)))\n",
    "                           sqrt(2 * pi * var(x)^2)\n",
    "                           \n",
    "                           \n",
    "                Here var(x) is actually std(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "            Attributes:\n",
    "                likelihoods: Likelihood of each feature per class\n",
    "                class_priors: Prior probabilities of classes  \n",
    "                features: All features of dataset\n",
    "        \"\"\"\n",
    "        self.features = list\n",
    "        self.likelihoods = {}\n",
    "        self.class_priors = {}\n",
    "\n",
    "        self.X_train = np.array\n",
    "        self.y_train = np.array\n",
    "        self.train_size = int\n",
    "        self.num_feats = int\n",
    "\n",
    "    def fit(self, X, y): #code\n",
    "\n",
    "    def _calc_class_prior(self): #code\n",
    "\n",
    "    def _calc_likelihoods(self): #code\n",
    "        \n",
    "    def predict(self, X):  #code\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Iris Dataset\n",
    "    print(\"\\nIris Dataset:\")\n",
    "\n",
    "    df = pd.read_csv(\"../Data/iris.csv\")\n",
    "    #print(df)\n",
    "\n",
    "    #Split fearures and target\n",
    "    X,y  = pre_processing(df)\n",
    "\n",
    "    #Split data into Training and Testing Sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "    #print(X_train, y_train)\n",
    "    gnb_clf = GaussianNB()\n",
    "    gnb_clf.fit(X_train, y_train)\n",
    "    #print(X_train, y_train)\n",
    "\n",
    "    print(\"Train Accuracy: {}\".format(accuracy_score(y_train, gnb_clf.predict(X_train))))\n",
    "    print(\"Test Accuracy: {}\".format(accuracy_score(y_test, gnb_clf.predict(X_test))))\n",
    "\n",
    "    #Query 1:\n",
    "    query = np.array([[5.7, 2.9, 4.2, 1.3]])\n",
    "    print(\"Query 1:- {} ---> {}\".format(query, gnb_clf.predict(query)))\n",
    "\n",
    "\n",
    "    #############################################################################################################\n",
    "\n",
    "    #Gender Classification Dataset\n",
    "    print(\"\\nGender Dataset:\")\n",
    "\n",
    "    df = pd.read_csv(\"../Data/gender.csv\")\n",
    "    #print(df)\n",
    "\n",
    "    #Split fearures and target\n",
    "    X,y  = df.drop([df.columns[0]], axis = 1), df[df.columns[0]]\n",
    "\n",
    "    X_train, y_train = X, y\n",
    "\n",
    "    gnb_clf = GaussianNB()\n",
    "    gnb_clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Train Accuracy: {}\".format(accuracy_score(y_train, gnb_clf.predict(X_train))))\n",
    "\n",
    "    #Query 1:\n",
    "    query = np.array([[6, 130, 8]])\n",
    "    print(\"Query 1:- {} ---> {}\".format(query, gnb_clf.predict(query)))\n",
    "\n",
    "    #Query 2:\n",
    "    query = np.array([[5, 80, 6]])\n",
    "    print(\"Query 2:- {} ---> {}\".format(query, gnb_clf.predict(query)))\n",
    "\n",
    "    #Query 3:\n",
    "    query = np.array([[7, 140, 14]])\n",
    "    print(\"Query 3:- {} ---> {}\".format(query, gnb_clf.predict(query)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765fdfb",
   "metadata": {},
   "source": [
    "naive_bayes.BernoulliNB(*[, alpha, ...]) : **Naive Bayes classifier for multivariate Bernoulli models.**\n",
    "\n",
    "naive_bayes.CategoricalNB(*[, alpha, ...]) : **Naive Bayes classifier for categorical features.**\n",
    "\n",
    "naive_bayes.ComplementNB(*[, alpha, ...]) : **The Complement Naive Bayes classifier described in Rennie et al. (2003).**\n",
    "\n",
    "naive_bayes.GaussianNB(*[, priors, ...]) : **Gaussian Naive Bayes (GaussianNB).**\n",
    "\n",
    "naive_bayes.MultinomialNB(*[, alpha, ...]) : **Naive Bayes classifier for multinomial models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from .base import BaseEstimator, ClassifierMixin\n",
    "from .preprocessing import binarize\n",
    "from .preprocessing import LabelBinarizer\n",
    "from .preprocessing import label_binarize\n",
    "from .utils import deprecated\n",
    "from .utils.extmath import safe_sparse_dot\n",
    "from .utils.multiclass import _check_partial_fit_first_call\n",
    "from .utils.validation import check_is_fitted, check_non_negative\n",
    "from .utils.validation import _check_sample_weight\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"BernoulliNB\",\n",
    "    \"GaussianNB\",\n",
    "    \"MultinomialNB\",\n",
    "    \"ComplementNB\",\n",
    "    \"CategoricalNB\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370f3b7",
   "metadata": {},
   "source": [
    "X : array-like of shape (n_samples, n_features) The input samples.\n",
    "\n",
    "C : ndarray of shape (n_samples,) Predicted target values for X.\n",
    "\n",
    "_joint_log_likelihood :\n",
    "\n",
    "    I.e. ``log P(c) + log P(x|c)`` for all rows x of X\n",
    "    \n",
    "    predict, predict_proba, and predict_log_proba pass the input\n",
    "    through _check_X and handle it over to _joint_log_likelihood.\n",
    "            \n",
    "predict_log_proba :\n",
    "\n",
    "    C : array-like of shape (n_samples, n_classes)\n",
    "         Returns the log-probability of the samples for each class \n",
    "         in the model. The columns correspond to the classes in \n",
    "         sorted order, as they appear in the attribute \n",
    "         :term:`classes_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    \"\"\"Abstract base class for naive Bayes estimators\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Compute the unnormalized posterior log probability of X\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _check_X(self, X):\n",
    "        \"\"\"To be overridden in subclasses with the actual checks.\n",
    "        Only used in predict* methods.\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform classification on an array of test vectors X.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = self._check_X(X)\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return log-probability estimates for the test vector X.\n",
    "        \"\"\"\n",
    "        1. calculate jll just like predict method\n",
    "        # normalize by P(x) = P(f_1, ..., f_n)\n",
    "        2. calculte log of the sum of exponentials of jll using logsumexp() \n",
    "            for each column (axis=1)\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for the test vector X.\n",
    "        \"\"\"\n",
    "        return np.exp(self.predict_log_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fefefc",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "    priors : array-like of shape (n_classes,)\n",
    "        Prior probabilities of the classes. If specified, the priors are not\n",
    "        adjusted according to the data.\n",
    "    var_smoothing : float, default=1e-9\n",
    "        Portion of the largest variance of all features that is added to\n",
    "        variances for calculation stability.\n",
    "        \n",
    "### Attributes\n",
    " \n",
    "    class_count_ : ndarray of shape (n_classes,) \n",
    "                number of training samples observed in each class.\n",
    "    class_prior_ : ndarray of shape (n_classes,)  \n",
    "              probability of each class.\n",
    "    classes_ : ndarray of shape (n_classes,)  \n",
    "              class labels known to the classifier.\n",
    "    epsilon_ : float \n",
    "             absolute additive value to variances.\n",
    "    n_features_in_ : int \n",
    "            Number of features seen during :term:`fit`.\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,) \n",
    "            Names of features seen during :term:`fit`. Defined only when `X` has feature\n",
    "            names that are all trings.\n",
    "    var_ : ndarray of shape (n_classes, n_features) \n",
    "            Variance of each feature perclass.\n",
    "    theta_ : ndarray of shape (n_classes, n_features) \n",
    "            mean of each feature per class.\n",
    "        \n",
    "        \n",
    "**fit()**\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features) : Training vectors, where \n",
    "         `n_samples` is the number of samples and `n_features` is the number of \n",
    "         features.\n",
    "    y : array-like of shape (n_samples,)\n",
    "                Target values.\n",
    "     sample_weight : array-like of shape (n_samples,), default=None\n",
    "                Weights applied to individual samples (1. for unweighted).\n",
    "                   Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
    "\n",
    "**_update_mean_variance()**\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "            n_past : int\n",
    "                Number of samples represented in old mean and variance. If sample\n",
    "                weights were given, this should contain the sum of sample\n",
    "                weights represented in old mean and variance.\n",
    "            mu : array-like of shape (number of Gaussians,)\n",
    "                Means for Gaussians in original set.\n",
    "            var : array-like of shape (number of Gaussians,)\n",
    "                Variances for Gaussians in original set.\n",
    "            sample_weight : array-like of shape (n_samples,), default=None\n",
    "                Weights applied to individual samples (1. for unweighted).\n",
    "     Returns\n",
    "     -------\n",
    "            total_mu : array-like of shape (number of Gaussians,)\n",
    "                Updated mean for each Gaussian over the combined set.\n",
    "            total_var : array-like of shape (number of Gaussians,)\n",
    "                Updated variance for each Gaussian over the combined set.\n",
    "                \n",
    "                \n",
    "**_partial_fit()**\n",
    "\n",
    "        classes : array-like of shape (n_classes,), default=None\n",
    "            List of all the classes that can possibly appear in the y vector.\n",
    "            Must be provided at the first call to partial_fit, can be omitted\n",
    "            in subsequent calls.\n",
    "        _refit : bool, default=False\n",
    "            If true, act as though this were the first time we called\n",
    "            _partial_fit (ie, throw away any past fitting and start over).\n",
    "            \n",
    "        This method is expected to be called several times consecutively\n",
    "        on different chunks of a dataset so as to implement out-of-core\n",
    "        or online learning.\n",
    "        This is especially useful when the whole dataset is too big to fit in\n",
    "        memory at once.\n",
    "        This method has some performance and numerical stability overhead,\n",
    "        hence it is better to call partial_fit on chunks of data that are\n",
    "        as large as possible (as long as fitting in the memory budget) to\n",
    "        hide the overhead. \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ad640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(_BaseNB):\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes (GaussianNB).\n",
    "    Can perform online updates to model parameters via :meth:`partial_fit`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, priors=None, var_smoothing=1e-9):\n",
    "        self.priors = priors\n",
    "        self.var_smoothing = var_smoothing\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit Gaussian Naive Bayes according to X, y.\n",
    "        \"\"\"\n",
    "        y = self._validate_data(y=y)\n",
    "        return self._partial_fit(\n",
    "            X, y, np.unique(y), _refit=True, sample_weight=sample_weight\n",
    "        )\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        \"\"\"Validate X, used only in predict* methods.\"\"\"\n",
    "        return self._validate_data(X, reset=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_mean_variance(n_past, mu, var, X, sample_weight=None):\n",
    "        \"\"\"Compute online update of Gaussian mean and variance.\n",
    "        return the updated mean and variance. (NB - each dimension (column) \n",
    "        in X is treated as independent-- you get variance, not covariance).\n",
    "        \"\"\"\n",
    "        if X.shape[0] == 0:\n",
    "            return mu, var\n",
    "\n",
    "        1. if sample_weight is given use it to calculate n_new,new_mu,new_var\n",
    "           else use only X , row-wise(axis=0)   \n",
    "        2. if n_past == 0 return new_mu, new_var\n",
    "        3. n_total = float(n_past + n_new)\n",
    "        4. calculate (weighted) total_mu using n_new,new_mu,n_past,mu\n",
    "        5. calculate total_var using old & new_var  \n",
    "\n",
    "        return total_mu, total_var\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\"Incremental fit on a batch of samples.\n",
    "        \"\"\"\n",
    "        return self._partial_fit(\n",
    "            X, y, classes, _refit=False, sample_weight=sample_weight\n",
    "        )\n",
    "\n",
    "    def _partial_fit(self, X, y, classes=None, _refit=False, sample_weight=None):\n",
    "        \"\"\"Actual implementation of Gaussian NB fitting.\"\"\"\n",
    "        if _refit:\n",
    "            self.classes_ = None\n",
    "\n",
    "        first_call = _check_partial_fit_first_call(self, classes)\n",
    "        X, y = self._validate_data(X, y, reset=first_call)\n",
    "        \n",
    "        1. validate data + check sample_weight\n",
    "        2. variance smoothing to avoid numerical error due to ratio of data \n",
    "            variance between dimensions is too small\n",
    "        3.if first_call:\n",
    "            1. initalize parameters\n",
    "            2. initalize priors then some condition checking\n",
    "                2.1 check provided prior matches no of classes\n",
    "                2.2 sum of priors should be 1\n",
    "                2.3 priors should be non-neg\n",
    "                else : initalize prior to zeros rather than self.priors\n",
    "        else:\n",
    "            1. if X.shape[1] != self.theta_.shape[1] then no of feature doesn''t match\n",
    "            2. put epsilon back \n",
    "\n",
    "\n",
    "        4. check for unique classes in y , if all are not unique then labels don''t exist\n",
    "        5. for each unique class calculate - new_theta,new_sigma & update var with epsilon\n",
    "           parameter\n",
    "        6. update in case of no prior provided\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \n",
    "        1. for each classes calculate joint log likelihood\n",
    "        \n",
    "        return joint_log_likelihood\n",
    "\n",
    "    @deprecated(  # type: ignore\n",
    "        \"Attribute `sigma_` was deprecated in 1.0 and will be removed in\"\n",
    "        \"1.2. Use `var_` instead.\"\n",
    "    )\n",
    "    @property\n",
    "    def sigma_(self):\n",
    "        return self.var_\n",
    "\n",
    "\n",
    "_ALPHA_MIN = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseDiscreteNB(_BaseNB):\n",
    "    \"\"\"Abstract base class for naive Bayes on discrete/categorical data\n",
    "    Any estimator based on this class should provide:\n",
    "    __init__\n",
    "    _joint_log_likelihood(X) as per _BaseNB\n",
    "    _update_feature_log_prob(alpha)\n",
    "    _count(X, Y)\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _count(self, X, Y):\n",
    "        \"\"\"Update counts that are used to calculate probabilities.\n",
    "        The counts make up a sufficient statistic extracted from the data.\n",
    "        Accordingly, this method is called each time `fit` or `partial_fit`\n",
    "        update the model. `class_count_` and `feature_count_` must be updated\n",
    "        here along with any model specific counts.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "        \"\"\"Update feature log probabilities based on counts.\n",
    "        This method is called each time `fit` or `partial_fit` update the\n",
    "        model.\n",
    "        \"\"\"\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        \"\"\"Validate X, used only in predict* methods.\"\"\"\n",
    "        return self._validate_data(X, accept_sparse=\"csr\", reset=False)\n",
    "\n",
    "    def _check_X_y(self, X, y, reset=True):\n",
    "        \"\"\"Validate X and y in fit methods.\"\"\"\n",
    "        return self._validate_data(X, y, accept_sparse=\"csr\", reset=reset)\n",
    "\n",
    "    def _update_class_log_prior(self, class_prior=None):\n",
    "        \"\"\"Update class log priors.\n",
    "        The class log priors are based on `class_prior`, class count or the\n",
    "        number of classes. This method is called each time `fit` or\n",
    "        `partial_fit` update the model.\n",
    "        \"\"\"\n",
    "        1. if dimensions of classes match : class_log_prior_ is np.log(class_prior)\n",
    "        2. elif sample weight is taken into account during self.fit_prior we need to \n",
    "           calculate class_log_prior_ differently\n",
    "        3. else class_log_prior_ is calculate on whole classes\n",
    "        \n",
    "    def _check_alpha(self):\n",
    "        \n",
    "        1. alpha should be > 0 , alpha is the smoothing parameter\n",
    "        2. alpha should be a scalar or a numpy array with shape [n_features]\n",
    "        3. alpha too small will result in numeric errors\n",
    "        \n",
    "        return self.alpha\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\"Incremental fit on a batch of samples.\"\"\"\n",
    "        1. check input data X,y,classes\n",
    "        2. for _check_partial_fit_first_call check if dimension matches\n",
    "        3. extend Y for multiclass using label_binarize()\n",
    "            1. binary classifier then 1-Y & Y\n",
    "            2. single class,degenerate case only Y\n",
    "        4. check class shape of Y\n",
    "        5. incorporate weight in calculation of Y\n",
    "        6. update class_prior + _update_class_log_prior() , \n",
    "            you can include alpha in claculation of _update_feature_log_prob()\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit Naive Bayes classifier according to X, y.\n",
    "        \"\"\"\n",
    "        1. check data, get parameters ready\n",
    "        2. use LabelBinarizer() & fit on Y \n",
    "        3. check class dimensions for Binary & single class\n",
    "        4. if sample_weight is present use that in calulation of Y\n",
    "        5. Count raw events from data before updating the class log prior and feature \n",
    "            log probas\n",
    "        6. calculate _update_feature_log_prob(),_update_class_log_prior(), you can use alpha\n",
    "           here for smoothing\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def _init_counters(self, n_classes, n_features):\n",
    "        self.class_count_ = np.zeros(n_classes, dtype=np.float64)\n",
    "        self.feature_count_ = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"poor_score\": True}\n",
    "\n",
    "    # TODO: Remove in 1.2\n",
    "    # mypy error: Decorated property not supported\n",
    "    @deprecated(  # type: ignore\n",
    "        \"Attribute `n_features_` was deprecated in version 1.0 and will be \"\n",
    "        \"removed in 1.2. Use `n_features_in_` instead.\"\n",
    "    )\n",
    "    @property\n",
    "    def n_features_(self):\n",
    "        return self."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd891e4c",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "    alpha : float, default=1.0\n",
    "        Additive (Laplace/Lidstone) smoothing parameter\n",
    "        (0 for no smoothing).\n",
    "\n",
    "    fit_prior : bool, default=True\n",
    "        Whether to learn class prior probabilities or not.\n",
    "        If false, a uniform prior will be used.\n",
    "\n",
    "### Attributes\n",
    "    class_log_prior_ : ndarray of shape (n_classes,)\n",
    "        Smoothed empirical log probability for each class.\n",
    "\n",
    "    feature_count_ : ndarray of shape (n_classes, n_features)\n",
    "        Number of samples encountered for each (class, feature)\n",
    "        during fitting. This value is weighted by the sample weight when\n",
    "        provided.\n",
    "    feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
    "        Empirical log probability of features\n",
    "        given a class, ``P(x_i|y)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB(_BaseDiscreteNB):\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for multinomial models.\n",
    "    The multinomial Naive Bayes classifier is suitable for classification with\n",
    "    discrete features (e.g., word counts for text classification). The\n",
    "    multinomial distribution normally requires integer feature counts. However,\n",
    "    in practice, fractional counts such as tf-idf may also work.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"requires_positive_X\": True}\n",
    "\n",
    "    def _count(self, X, Y):\n",
    "        \"\"\"Count and smooth feature occurrences.\"\"\"\n",
    "        check_non_negative(X, \"MultinomialNB (input X)\")\n",
    "        self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
    "        self.class_count_ += Y.sum(axis=0)\n",
    "\n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "        \"\"\"Apply smoothing to raw counts and recompute log probabilities\"\"\"\n",
    "        smoothed_fc = self.feature_count_ + alpha\n",
    "        smoothed_cc = smoothed_fc.sum(axis=1)\n",
    "\n",
    "        self.feature_log_prob_ = np.log(smoothed_fc) - np.log(smoothed_cc.reshape(-1, 1))\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Calculate the posterior log probability of the samples X\"\"\"\n",
    "        return safe_sparse_dot(X, self.feature_log_prob_.T) + self.class_log_prior_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2904a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplementNB(_BaseDiscreteNB):\n",
    "    \"\"\"The Complement Naive Bayes classifier described in Rennie et al. (2003).\n",
    "    The Complement Naive Bayes classifier was designed to correct the \"severe\n",
    "    assumptions\" made by the standard Multinomial Naive Bayes classifier. It is\n",
    "    particularly suited for imbalanced data sets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None, norm=False):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "        self.norm = norm\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"requires_positive_X\": True}\n",
    "\n",
    "    def _count(self, X, Y):\n",
    "        \"\"\"Count feature occurrences.\"\"\"\n",
    "        check_non_negative(X, \"ComplementNB (input X)\")\n",
    "        self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
    "        self.class_count_ += Y.sum(axis=0)\n",
    "        self.feature_all_ = self.feature_count_.sum(axis=0)\n",
    "\n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "        \"\"\"Apply smoothing to raw counts and compute the weights.\"\"\"\n",
    "        comp_count = self.feature_all_ + alpha - self.feature_count_\n",
    "        logged = np.log(comp_count / comp_count.sum(axis=1, keepdims=True))\n",
    "        # _BaseNB.predict uses argmax, but ComplementNB operates with argmin.\n",
    "        if self.norm:\n",
    "            summed = logged.sum(axis=1, keepdims=True)\n",
    "            feature_log_prob = logged / summed\n",
    "        else:\n",
    "            feature_log_prob = -logged\n",
    "        self.feature_log_prob_ = feature_log_prob\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Calculate the class scores for the samples in X.\"\"\"\n",
    "        jll = safe_sparse_dot(X, self.feature_log_prob_.T)\n",
    "        if len(self.classes_) == 1:\n",
    "            jll += self.class_log_prior_\n",
    "        return jll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB(_BaseDiscreteNB):\n",
    "    \"\"\"Naive Bayes classifier for multivariate Bernoulli models.\n",
    "    Like MultinomialNB, this classifier is suitable for discrete data. The\n",
    "    difference is that while MultinomialNB works with occurrence counts,\n",
    "    BernoulliNB is designed for binary/boolean features.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None):\n",
    "        self.alpha = alpha\n",
    "        self.binarize = binarize\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        \"\"\"Validate X, used only in predict* methods.\"\"\"\n",
    "        X = super()._check_X(X)\n",
    "        if self.binarize is not None:\n",
    "            X = binarize(X, threshold=self.binarize)\n",
    "        return X\n",
    "\n",
    "    def _check_X_y(self, X, y, reset=True):\n",
    "        X, y = super()._check_X_y(X, y, reset=reset)\n",
    "        if self.binarize is not None:\n",
    "            X = binarize(X, threshold=self.binarize)\n",
    "        return X, y\n",
    "\n",
    "    def _count(self, X, Y):\n",
    "        \"\"\"Count and smooth feature occurrences.\"\"\"\n",
    "        self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
    "        self.class_count_ += Y.sum(axis=0)\n",
    "\n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "        \"\"\"Apply smoothing to raw counts and recompute log probabilities\"\"\"\n",
    "        smoothed_fc = self.feature_count_ + alpha\n",
    "        smoothed_cc = self.class_count_ + alpha * 2\n",
    "\n",
    "        self.feature_log_prob_ = np.log(smoothed_fc) - np.log(smoothed_cc.reshape(-1, 1))\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Calculate the posterior log probability of the samples X\"\"\"\n",
    "        n_features = self.feature_log_prob_.shape[1]\n",
    "        n_features_X = X.shape[1]\n",
    "\n",
    "        if n_features_X != n_features:\n",
    "            raise ValueError(\n",
    "                \"Expected input with %d features, got %d instead\"\n",
    "                % (n_features, n_features_X)\n",
    "            )\n",
    "\n",
    "        neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
    "        # Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob\n",
    "        jll = safe_sparse_dot(X, (self.feature_log_prob_ - neg_prob).T)\n",
    "        jll += self.class_log_prior_ + neg_prob.sum(axis=1)\n",
    "\n",
    "        return jll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalNB(_BaseDiscreteNB):\n",
    "    \"\"\"Naive Bayes classifier for categorical features.\n",
    "    The categorical Naive Bayes classifier is suitable for classification with\n",
    "    discrete features that are categorically distributed. The categories of\n",
    "    each feature are drawn from a categorical distribution.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "        self.min_categories = min_categories\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit Naive Bayes classifier according to X, y.\n",
    "        \"\"\"\n",
    "        return super().fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\"Incremental fit on a batch of samples.\n",
    "        \"\"\"\n",
    "        return super().partial_fit(X, y, classes, sample_weight=sample_weight)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"requires_positive_X\": True}\n",
    "\n",
    "    def _check_X(self, X):\n",
    "        \"\"\"Validate X, used only in predict* methods.\"\"\"\n",
    "        X = self._validate_data(\n",
    "            X, dtype=\"int\", accept_sparse=False, force_all_finite=True, reset=False\n",
    "        )\n",
    "        check_non_negative(X, \"CategoricalNB (input X)\")\n",
    "        return X\n",
    "\n",
    "    def _check_X_y(self, X, y, reset=True):\n",
    "        X, y = self._validate_data(\n",
    "            X, y, dtype=\"int\", accept_sparse=False, force_all_finite=True, reset=reset\n",
    "        )\n",
    "        check_non_negative(X, \"CategoricalNB (input X)\")\n",
    "        return X, y\n",
    "\n",
    "    def _init_counters(self, n_classes, n_features):\n",
    "        self.class_count_ = np.zeros(n_classes, dtype=np.float64)\n",
    "        self.category_count_ = [np.zeros((n_classes, 0)) for _ in range(n_features)]\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_n_categories(X, min_categories):\n",
    "        1. rely on max for n_categories categories are encoded between 0...n-1\n",
    "        2. error check for min_categories\n",
    "            2.1. integral type\n",
    "            2.2. shape & array-like object\n",
    "        3. return n_categories_ or n_categories_X\n",
    "        \n",
    "\n",
    "    def _count(self, X, Y):\n",
    "        def _update_cat_count_dims(cat_count, highest_feature):\n",
    "            1. if difference between highest_feature & cat_count non-zero ppend a \n",
    "                    column full of zeros for each new category\n",
    "            return cat_count\n",
    "\n",
    "        def _update_cat_count(X_feature, Y, cat_count, n_classes):\n",
    "            1. for each class create a mask=Y[:,j] then use this mask to get number of \n",
    "                bincount()\n",
    "            2. cat_count[j, indices] += counts[indices]\n",
    "            3. calculate class_count\n",
    "            4. validate n_categories_\n",
    "            5. for each feature update _update_cat_count_dims & _update_cat_count\n",
    "            \n",
    "\n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "        feature_log_prob = []\n",
    "        1. for each feature \n",
    "            1.1 smooth category_count_ using alpha + get the sum\n",
    "            1.2 store difference between log of smoothed_cat_count & smoothed_class_count\n",
    "        2. update self.feature_log_prob_ = feature_log_prob\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        1. do feature check\n",
    "        2. for each feature\n",
    "            2.1 calculate sum of feature_log_prob_ for each indices = X[:, i]\n",
    "        3. total_ll = jll + self.class_log_prior_\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3076aace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
