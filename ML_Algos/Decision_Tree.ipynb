{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb74d6bf",
      "metadata": {
        "id": "cb74d6bf"
      },
      "source": [
        "# This notebook contains easy to follow Decision Trees documentaion including <br>\n",
        "     1. psudocode form sklearn implementation(taken from github)\n",
        "     2. quck links to different section of sklearn Decision tree documentation \n",
        "     3. implementation from scratch\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245d3b81",
      "metadata": {
        "id": "245d3b81"
      },
      "source": [
        "## Decision Trees \n",
        "\n",
        "https://scikit-learn.org/stable/modules/tree.html#tree\n",
        "  \n",
        "\n",
        "\n",
        "## sklearn.tree: Decision Trees\n",
        "\n",
        "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "422f1bc9",
      "metadata": {
        "id": "422f1bc9"
      },
      "source": [
        "### Mathematical formulation\n",
        "\n",
        "https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb46be2f",
      "metadata": {
        "id": "bb46be2f"
      },
      "source": [
        "##### class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
        "\n",
        "https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/tree/_classes.py#L532"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b46b0c0",
      "metadata": {
        "id": "2b46b0c0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "This module gathers tree-based methods, including decision, regression and\n",
        "randomized trees. Single and multi-output problems are both handled.\n",
        "\n",
        "\n",
        "# Authors: Gilles Louppe <g.louppe@gmail.com>\n",
        "#          Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
        "#          Brian Holt <bdholt1@gmail.com>\n",
        "#          Noel Dawe <noel@dawe.me>\n",
        "#          Satrajit Gosh <satrajit.ghosh@gmail.com>\n",
        "#          Joly Arnaud <arnaud.v.joly@gmail.com>\n",
        "#          Fares Hedayati <fares.hedayati@gmail.com>\n",
        "#          Nelson Liu <nelson@nelsonliu.me>\n",
        "#\n",
        "# License: BSD 3 clause\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import from ..base,..utils,._criterion,._splitter,._tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "297d847b",
      "metadata": {
        "id": "297d847b"
      },
      "source": [
        "\n",
        "### Types and constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c9670bd",
      "metadata": {
        "id": "4c9670bd"
      },
      "outputs": [],
      "source": [
        "DTYPE = _tree.DTYPE\n",
        "DOUBLE = _tree.DOUBLE\n",
        "\n",
        "CRITERIA_CLF = {\n",
        "    \"gini\": _criterion.Gini,\n",
        "    \"log_loss\": _criterion.Entropy,\n",
        "    \"entropy\": _criterion.Entropy,\n",
        "}\n",
        "# TODO(1.2): Remove \"mse\" and \"mae\".\n",
        "CRITERIA_REG = {\n",
        "    \"squared_error\": _criterion.MSE,\n",
        "    \"mse\": _criterion.MSE,\n",
        "    \"friedman_mse\": _criterion.FriedmanMSE,\n",
        "    \"absolute_error\": _criterion.MAE,\n",
        "    \"mae\": _criterion.MAE,\n",
        "    \"poisson\": _criterion.Poisson,\n",
        "}\n",
        "\n",
        "DENSE_SPLITTERS = {\"best\": _splitter.BestSplitter, \"random\": _splitter.RandomSplitter}\n",
        "\n",
        "SPARSE_SPLITTERS = {\n",
        "    \"best\": _splitter.BestSparseSplitter,\n",
        "    \"random\": _splitter.RandomSparseSplitter,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c01f965",
      "metadata": {
        "id": "1c01f965"
      },
      "source": [
        "\n",
        "### Base decision tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503f95e1",
      "metadata": {
        "id": "503f95e1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            The input samples. Internally, it will be converted to\n",
        "            ``dtype=np.float32`` and if a sparse matrix is provided\n",
        "            to a sparse \"csr_matrix\"\n",
        "            \n",
        "check_input : bool, default=True\n",
        "            Allow to bypass several input checking.\n",
        "            Don''t use this parameter unless you know what you do.\n",
        "\n",
        "y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
        "            The predicted classes, or the predict values.\n",
        "\n",
        "sample_weight : array-like of shape (n_samples,), default=None\n",
        "            Sample weights. If None, then samples are equally weighted. Splits\n",
        "            that would create child nodes with net zero or negative weight are\n",
        "            ignored while searching for a split in each node. Splits are also\n",
        "            ignored if they would result in any single class carrying a\n",
        "            negative weight in either child node.\n",
        "\n",
        "The importance of a feature is computed as the (normalized) total\n",
        "        reduction of the criterion brought by that feature.\n",
        "        It is also known as the Gini importance.\n",
        "        Warning: impurity-based feature importances can be misleading for\n",
        "        high cardinality features (many unique values). See\n",
        "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
        "        \n",
        "feature_importances_ : ndarray of shape (n_features,)\n",
        "            Normalized total reduction of criteria by feature\n",
        "            (Gini importance). \n",
        "            \n",
        "Predict class or regression value for X.\n",
        "        For a classification model, the predicted class for each sample in X is\n",
        "        returned. For a regression model, the predicted value based on X is\n",
        "        returned.\n",
        "        \n",
        "        \n",
        "Predict class probabilities of the input samples X.\n",
        "        The predicted class probability is the fraction of samples of the same\n",
        "        class in a leaf.\n",
        "\n",
        "ccp_path : :class:`~sklearn.utils.Bunch`\n",
        "            Dictionary-like object, with the following attributes.\n",
        "            ccp_alphas : ndarray\n",
        "                Effective alphas of subtree during pruning.\n",
        "            impurities : ndarray\n",
        "                Sum of the impurities of the subtree leaves for the\n",
        "                corresponding alpha value in ``ccp_alphas``.\n",
        "                \n",
        "X_leaves : array-like of shape (n_samples,)\n",
        "            For each datapoint x in X, return the index of the leaf x\n",
        "            ends up in. Leaves are numbered within\n",
        "            ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
        "            numbering.\n",
        "            \n",
        "indicator : sparse matrix of shape (n_samples, n_nodes)\n",
        "            Return a node indicator CSR matrix where non zero elements\n",
        "            indicates that the samples goes through the nodes.\n",
        "\"\"\"\n",
        "            \n",
        "# for most of the methods which returns a parameter of tree input check  is performed \n",
        "# before returning value\n",
        "\n",
        "func : self._validate_X_predict(X, check_input) \n",
        "    \n",
        "        1. predict(self, X, check_input=True)\n",
        "        2. apply(self, X, check_input=True)\n",
        "        3. decision_path(self, X, check_input=True)\n",
        "        \n",
        "        \n",
        "func : check_is_fitted(self)\n",
        "    \n",
        "    1. get_depth(self)\n",
        "    2. get_n_leaves(self)\n",
        "    3. _prune_tree(self)\n",
        "    4. feature_importances_(self)\n",
        "    5. predict(self, X, check_input=True)\n",
        "    6. apply(self, X, check_input=True)\n",
        "    7. decision_path(self, X, check_input=True)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d42c3c",
      "metadata": {
        "id": "52d42c3c"
      },
      "outputs": [],
      "source": [
        "class BaseDecisionTree(MultiOutputMixin, BaseEstimator, metaclass=ABCMeta):\n",
        "    \n",
        "    @abstractmethod\n",
        "    def __init__(self,*args):\n",
        "        initalize the constructor with Parameters\n",
        "\n",
        "    def get_depth(self): return self.tree_.max_depth\n",
        "          \n",
        "    def get_n_leaves(self):  return self.tree_.n_leaves   \n",
        "\n",
        "    def fit(self, X, y, sample_weight=None, check_input=True):\n",
        "\n",
        "        1. random_state = check_random_state(self.random_state)\n",
        "\n",
        "        2. check_scalar(\"ccp_alpha\") # check parameter for ccp_alpha + initalize\n",
        "\n",
        "        3. if check_input: # can't pass multi_output=True as means y can be csr\n",
        "            \n",
        "            1. validate X and y data type # is it's sparse\n",
        "            \n",
        "            2. if (X is parse):\n",
        "                1. sort the indices and \n",
        "                \"No support for np.int64 index based sparse matrices\"\n",
        "            \n",
        "            3 check criterion == \"poisson\":\n",
        "                # criterion include y > 0 & sum(y) > 0\n",
        "                \n",
        "            4. determine output settings for X & y which include\n",
        "                1 what is X.shape i.e n_samples, self.n_features_in_\n",
        "                \n",
        "                2. is it a classificaton problem\n",
        "                    1. declare self.classes_ , self.n_classes_\n",
        "                    2. copy class weights (if any)\n",
        "                    3. initalize y_encoded with 0's & store unique \n",
        "                    classes_ and it's shape n_classes_ i.e \n",
        "                    4. compute calls weights (if any) using the funcion \n",
        "                    compute_sample_weight\n",
        "                    \n",
        "                3. does y has atleast 1-dimension otherwise need to \n",
        "                reshape to aX1 using(-1,1) and store it as self.n_outputs_\n",
        "                \n",
        "                4. check_scaler(parameters for max_depth) if max_depth provided\n",
        "                store that in otherwise set a limit value for None\n",
        "                \n",
        "                5. later on the rest of the parameters are calculated - format\n",
        "                \n",
        "                    if isinstance(self.parameter_name, data_type):\n",
        "                        check_scaler(parameter_name)\n",
        "                        self.parameter_name=calculated value\n",
        "                    elif self.parameter_name==None:\n",
        "                         self.parameter_name=fixed value\n",
        "                    else: # parameter is not mentioned data_type\n",
        "                        check_scaler(parameter_name)\n",
        "                        self.parameter_name=calculated value\n",
        "                    \n",
        "                    # here multiple parameter condition is checked as one parameter may\n",
        "                    # depend on rest of the parameters \n",
        "                \n",
        "                6. len(y)!=n_samples: \"Number of labels does not match number of samples\"\n",
        "             \n",
        "        4. if we have provided certain parameters during fit like expanded_class_weight\n",
        "            1. _check_sample_weight(...) \n",
        "            2. if provided sample_weight = sample_weight * expanded_class_weight\n",
        "                else: otherwise sample_weight = expanded_class_weight\n",
        "                \n",
        "                \n",
        "        5. Build Tree\n",
        "                1. set criterion = self.criterion\n",
        "                    1. if not isinstance(criterion, Criterion)\n",
        "                        1. is it classification problem or regression set criterion accordingly\n",
        "                    2. make a deep copy of criterion as it is mutable & subjected to change\n",
        "                    during parallel fiiting\n",
        "                    \n",
        "                2. splitter = self.splitter # different treatment for sparse & dense\n",
        "                    1. if not isinstance(self.splitter, Splitter)\n",
        "               \n",
        "                3. is it classifier tree or not\n",
        "                    if is_classifier(self):\n",
        "                        self.tree_ = Tree(self.n_features_in_, self.n_classes_, self.n_outputs_)\n",
        "                    else:\n",
        "                        self.tree_ = Tree(\n",
        "                            self.n_features_in_,\n",
        "                            # TODO: tree shouldn't need this in this case\n",
        "                            np.array([1] * self.n_outputs_, dtype=np.intp),\n",
        "                            self.n_outputs_,\n",
        "                        )\n",
        "                \n",
        "                4. use bestfirst if max_leaf_nodes given i.e we have width of tree otherwise \n",
        "                use depthfirst\n",
        "                \n",
        "                5. call builder.build(self.tree_, X, y, sample_weight)\n",
        "                \n",
        "        6. if self.n_outputs_ == 1 and is_classifier(self)               \n",
        "              self.n_classes_ = self.n_classes_[0]\n",
        "              self.classes_ = self.classes_[0]\n",
        "\n",
        "        7. self._prune_tree()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _validate_X_predict(self, X, check_input):\n",
        "        \"\"\"Validate the training data on predict (probabilities).\"\"\"\n",
        "        if check_input:\n",
        "             X=self._validate_data(...,accept_sparse=\"csr\") but \n",
        "            \"No support for np.int64 index based sparse matrices  \n",
        "        else:\n",
        "            # The number of features is checked regardless of `check_input`\n",
        "            self._check_n_features(X, reset=False)\n",
        "        return X\n",
        "\n",
        "    def predict(self, X, check_input=True):\n",
        "        # The predicted classes, or the predict values.\n",
        "        proba = self.tree_.predict(X) \n",
        "        # Classification - use self.classes_.take(np.argmax(prba[:k],axis=1),axis=0)\n",
        "        if classification problem: \n",
        "            if single output : #(n_samples,)\n",
        "                return maximum proba value\n",
        "            else:\n",
        "                initalize predictions with zeros (remember to add datatype)\n",
        "                for k in self.n_outputs_:\n",
        "                    predictions[:, k] = maximum of proba[:k] value\n",
        "        else: # Regression\n",
        "            # pattern proba[all row,all col,0]\n",
        "            return according (n_samples,) or (n_samples, n_outputs)  \n",
        "\n",
        "    def apply(self, X, check_input=True):  return self.tree_.apply(X)\n",
        "        \"\"\"X_leaves : Return the index of the leaf that each sample is predicted as\"\"\"      \n",
        "\n",
        "    def decision_path(self, X, check_input=True): return self.tree_.decision_path(X)\n",
        "        \"\"\"indicator : Return the decision path in the tree..\"\"\"\n",
        "        \n",
        "    def _prune_tree(self):\n",
        "        \"\"\"Prune tree using Minimal Cost-Complexity Pruning.\"\"\"\n",
        "        1. don''t prune if self.ccp_alpha == 0.0\n",
        "        2. build pruned tree\n",
        "            1. model is classifier:\n",
        "                n_classes have to be atleast 1D\n",
        "                pruned_tree = Tree(self.n_features_in_, n_classes, self.n_outputs_)\n",
        "            2. model is regressor\n",
        "                1. n_classes = np.array([1] * self.n_outputs_, dtype=np.intp) then\n",
        "                call pruned_tree=Tree(...)\n",
        "\n",
        "        3. _build_pruned_tree_ccp(pruned_tree, self.tree_, self.ccp_alpha) # imported\n",
        "        4. self.tree_ = pruned_tree\n",
        "\n",
        "    def cost_complexity_pruning_path(self, X, y, sample_weight=None):\n",
        "        \"\"\"Compute the pruning path during Minimal Cost-Complexity Pruning. \"\"\"\n",
        "        est = clone(self).set_params(ccp_alpha=0.0)\n",
        "        est.fit(X, y, sample_weight=sample_weight)\n",
        "        return Bunch(**ccp_pruning_path(est.tree_))\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self): return self.tree_.compute_feature_importances()\n",
        "        \"\"\"Return the feature importances.\"\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a46908c",
      "metadata": {
        "id": "2a46908c"
      },
      "source": [
        "\n",
        "### Public estimators\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bff0fb",
      "metadata": {
        "id": "36bff0fb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "proba : ndarray of shape (n_samples, n_classes) or list of n_outputs \n",
        "            such arrays if n_outputs > 1\n",
        "            The class probabilities of the input samples. The order of the\n",
        "            classes corresponds to that in the attribute :term:`classes_`.\n",
        "\"\"\"\n",
        "# for most of the methods which returns a parameter of tree input check  is performed \n",
        "# before returning value\n",
        "\n",
        "func : check_is_fitted(self)\n",
        "    1. predict_proba(self, X, check_input=True)\n",
        "func : self._validate_X_predict(X, check_input)\n",
        "    1. predict_proba(self, X, check_input=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51177558",
      "metadata": {
        "id": "51177558"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeClassifier(ClassifierMixin, BaseDecisionTree):\n",
        "    \n",
        "    def __init__(self,*args):\n",
        "        super().__init__(initalize the constructor with Parameters)\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None, check_input=True):\n",
        "        super().fit(initalize the constructor with Parameters)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X, check_input=True):\n",
        "        \"\"\" proba : Predict class probabilities of the input samples X\"\"\"\n",
        "        proba = self.tree_.predict(X)\n",
        "        # for self.n_outputs_ == 1 : remove all k\n",
        "        all_proba = []\n",
        "            for k in range(self.n_outputs_):\n",
        "                proba_k = proba[:, k, : self.n_classes_[k]]\n",
        "                normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n",
        "                normalizer[normalizer == 0.0] = 1.0\n",
        "                proba_k /= normalizer\n",
        "                all_proba.append(proba_k)\n",
        "            return all_proba\n",
        "\n",
        "    def predict_log_proba(self, X):\n",
        "        \"\"\" proba : Predict class log-probabilities of the input samples X.\"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        if self.n_outputs_ == 1:return np.log(proba)\n",
        "        else:\n",
        "            for k in range(self.n_outputs_):\n",
        "                proba[k] = np.log(proba[k])\n",
        "            return proba\n",
        "\n",
        "    @deprecated(  # type: ignore\n",
        "        \"The attribute `n_features_` is deprecated in 1.0 and will be removed \"\n",
        "        \"in 1.2. Use `n_features_in_` instead.\"\n",
        "    )\n",
        "    @property\n",
        "    def n_features_(self):\n",
        "        return self.n_features_in_\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {\"multilabel\": True}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faa8df03",
      "metadata": {
        "id": "faa8df03"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "grid : ndarray of shape (n_samples, n_target_features)\n",
        "            The grid points on which the partial dependence should be\n",
        "            evaluated.\n",
        "target_features : ndarray of shape (n_target_features)\n",
        "            The set of target features for which the partial dependence\n",
        "            should be evaluated.\n",
        "\n",
        "averaged_predictions : ndarray of shape (n_samples,)\n",
        "            The value of the partial dependence function on each grid point.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3738d4d",
      "metadata": {
        "id": "c3738d4d"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeRegressor(RegressorMixin, BaseDecisionTree):\n",
        "    \n",
        "    def __init__(self,*args):\n",
        "        super().__init__(initalize the constructor with Parameters)\n",
        "    \n",
        "    def fit(self, X, y, sample_weight=None, check_input=True):\n",
        "        super().fit(initalize the constructor with Parameters)\n",
        "        return self\n",
        "\n",
        "    def _compute_partial_dependence_recursion(self, grid, target_features):\n",
        "        \"\"\"Fast partial dependence computation. \"\"\"\n",
        "        grid = np.asarray(grid, dtype=DTYPE, order=\"C\")\n",
        "        averaged_predictions = np.zeros(\n",
        "            shape=grid.shape[0], dtype=np.float64, order=\"C\"\n",
        "        )\n",
        "\n",
        "        self.tree_.compute_partial_dependence(\n",
        "            grid, target_features, averaged_predictions\n",
        "        )\n",
        "        return averaged_predictions\n",
        "\n",
        "    @deprecated(  # type: ignore\n",
        "        \"The attribute `n_features_` is deprecated in 1.0 and will be removed \"\n",
        "        \"in 1.2. Use `n_features_in_` instead.\"\n",
        "    )\n",
        "    @property\n",
        "    def n_features_(self):\n",
        "        return self.n_features_in_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1024a8f",
      "metadata": {
        "id": "d1024a8f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ExtraTreeClassifier(DecisionTreeClassifier) : \n",
        "\n",
        "An extremely randomized tree classifier.\n",
        "    Extra-trees differ from classic decision trees in the way they are built.\n",
        "    When looking for the best split to separate the samples of a node into two\n",
        "    groups, random splits are drawn for each of the `max_features` randomly\n",
        "    selected features and the best split among those is chosen. When\n",
        "    `max_features` is set 1, this amounts to building a totally random\n",
        "    decision tree.\n",
        "\n",
        "ExtraTreeRegressor(DecisionTreeRegressor):\n",
        "\n",
        "An extremely randomized tree regressor.\n",
        "    Extra-trees differ from classic decision trees in the way they are built.\n",
        "    When looking for the best split to separate the samples of a node into two\n",
        "    groups, random splits are drawn for each of the `max_features` randomly\n",
        "    selected features and the best split among those is chosen. When\n",
        "    `max_features` is set 1, this amounts to building a totally random\n",
        "    decision tree.   \n",
        "    \n",
        "    \n",
        "Warning: Extra-trees should only be used within ensemble methods.\n",
        "Read more in the :ref:`User Guide <tree>`.\n",
        "    \n",
        "    \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a70b3f40",
      "metadata": {
        "id": "a70b3f40"
      },
      "outputs": [],
      "source": [
        "class ExtraTreeClassifier(DecisionTreeClassifier):\n",
        "    \n",
        "    def __init__(self,*args):\n",
        "        super().__init__(initalize the constructor with Parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43629c5",
      "metadata": {
        "id": "b43629c5"
      },
      "outputs": [],
      "source": [
        "class ExtraTreeRegressor(DecisionTreeRegressor):\n",
        "\n",
        "    def __init__(self,*args):\n",
        "        super().__init__(initalize the constructor with Parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eHtS8wyTUlaE",
      "metadata": {
        "id": "eHtS8wyTUlaE"
      },
      "source": [
        "# Implementation from scratch \n",
        "\n",
        "Source - https://towardsdatascience.com/ml-from-scratch-decision-tree-c6444102436a\n",
        "\n",
        "Source - https://www.analyticsvidhya.com/blog/2020/10/all-about-decision-tree-from-scratch-with-python-implementation/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W8t35YW9ZHzF",
      "metadata": {
        "id": "W8t35YW9ZHzF"
      },
      "source": [
        "![summary.png](attachment:summary.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e792988",
      "metadata": {
        "id": "4e792988"
      },
      "source": [
        "1. Gini impurity\n",
        "\n",
        "Gini says, if we select two items from a population at random then they must be of the same class and the probability for this is 1 if the population is pure.\n",
        "\n",
        "    1. It works with the categorical target variable “Success” or “Failure”.\n",
        "    2. It performs only Binary splits\n",
        "    3. Higher the value of Gini higher the homogeneity.\n",
        "    4. CART (Classification and Regression Tree) uses the Gini method to create binary splits.\n",
        "    \n",
        "Steps to Calculate Gini impurity for a split\n",
        "\n",
        "    1. Calculate Gini impurity for sub-nodes, using the formula subtracting the sum of the square of probability for success and failure from one.\n",
        "    1-(p²+q²)\n",
        "    where p =P(Success) & q=P(Failure)\n",
        "    2. Calculate Gini for split using the weighted Gini score of each node of that split\n",
        "    3. Select the feature with the least Gini impurity for the split.\n",
        "    \n",
        "    \n",
        "#### Gini Index = 1 - sum ( prob[i]^2) \n",
        "\n",
        "\n",
        "After calculating the Gini index for left and right nodes we take a weighted average of left and right node and that becomes the Gini index of the parent node which will be used to split the node.\n",
        "\n",
        "\n",
        "\n",
        "2. Chi-Square\n",
        "\n",
        "It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node. We measure it by the sum of squares of standardized differences between observed and expected frequencies of the target variable.\n",
        "\n",
        "    1. It works with the categorical target variable “Success” or “Failure”.\n",
        "    2. It can perform two or more splits.\n",
        "    3. Higher the value of Chi-Square higher the statistical significance of differences between sub-node and Parent node.\n",
        "    4. Chi-Square of each node is calculated using the formula,\n",
        "    5. Chi-square = ((Actual — Expected)² / Expected)¹/2\n",
        "    6. It generates a tree called CHAID (Chi-square Automatic Interaction Detector)\n",
        "\n",
        "Steps to Calculate Chi-square for a split:\n",
        "\n",
        "    1. Calculate Chi-square for an individual node by calculating the deviation for Success and Failure both\n",
        "    2. Calculated Chi-square of Split using Sum of all Chi-square of success and Failure of each node of the split\n",
        "    3. Select the split where Chi-Square is maximum.\n",
        "\n",
        "\n",
        "3. Information Gain\n",
        "\n",
        "A less impure node requires less information to describe it and, a more impure node requires more information. Information theory is a measure to define this degree of disorganization in a system known as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is equally divided (50% — 50%), it has an entropy of one. Entropy is calculated as follows.\n",
        "\n",
        "#### Decision tree - Entropy = -plog2(p) - qlog2(q)\n",
        "\n",
        "Steps to calculate entropy for a split:\n",
        "\n",
        "    1. Calculate the entropy of the parent node\n",
        "    2. Calculate entropy of each individual node of split and calculate the weighted average of all sub-nodes available in the split. The lesser the entropy, the better it is.\n",
        "    3. calculate information gain as follows and chose the node with the highest information gain for splitting\n",
        "\n",
        "#### Information Gain  = 1 - Entropy\n",
        "\n",
        "\n",
        "4. Reduction in Variance\n",
        "\n",
        "Till now, we have discussed the algorithms for the categorical target variable. Reduction in variance is an algorithm used for continuous target variables (regression problems).\n",
        "\n",
        "    1. Used for continuous variables\n",
        "    2. This algorithm uses the standard formula of variance to choose the best split.\n",
        "    3. The split with lower variance is selected as the criteria to split the population\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d6f957",
      "metadata": {
        "id": "a0d6f957"
      },
      "outputs": [],
      "source": [
        "#### calculating gini index \n",
        "    def gini_index(data):\n",
        "        size = len(data)\n",
        "        instances = [0] * len(classes)\n",
        "        for row in data:\n",
        "            instances[int(row[-1])] += 1\n",
        "        return 1 - np.sum([(val/size)**2 for val in instances]) if size > 0 else 1\n",
        "\n",
        "\n",
        "### calculating gini index for the parent node\n",
        "def calculate_gini_index(value,child):\n",
        "        group_size = [len(child['l']),len(child['r'])] \n",
        "        left_gini = self.gini_index(child['l'])\n",
        "        right_gini =  self.gini_index(child['r'])\n",
        "        return (group_size[0]/np.sum(group_size) * left_gini) + (group_size[1]/np.sum(group_size) * right_gini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GBb_3mBgbEDW",
      "metadata": {
        "id": "GBb_3mBgbEDW"
      },
      "outputs": [],
      "source": [
        "numpy.bincount(x, /, weights=None, minlength=0)\n",
        "\n",
        "# Count number of occurrences of each value in array of non-negative ints.\n",
        "\n",
        "# The number of bins (of size 1) is one larger than the largest value in x. If minlength is specified, \n",
        "# there will be at least this number of bins in the output array (though it will be longer if necessary, \n",
        "# depending on the contents of x). Each bin gives the number of occurrences of its index value in x. If weights is \n",
        "# specified the input array is weighted by it, i.e. if a value n is found at position i, out[n] += weight[i] instead of out[n] += 1.\n",
        "\n",
        "\n",
        "np.bincount(np.array([0, 1, 1, 3, 2, 1, 7]))\n",
        "array([1, 3, 1, 1, 0, 0, 0, 1])\n",
        "\n",
        "x = np.array([0, 1, 1, 3, 2, 1, 7, 23])\n",
        "\n",
        "np.bincount(x).size == np.amax(x)+1 # 24\n",
        "True\n",
        "\n",
        "\n",
        "w = np.array([0.3, 0.5, 0.2, 0.7, 1., -0.6]) # weights\n",
        "\n",
        "x = np.array([0, 1, 1, 2, 2, 2])\n",
        "\n",
        "np.bincount(x,  weights=w)\n",
        "array([ 0.3,  0.7,  1.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O0IvZKKDVt1G",
      "metadata": {
        "id": "O0IvZKKDVt1G"
      },
      "source": [
        "Condition for leaf node (min_samples) : if the total data points present is less than the minimum samples\n",
        "\n",
        "\n",
        "Continue split or consider as leaf node (minimum Gini value) : node Gini value should be at least greater then minimum Gini value set by the user to continue split if not then the node is considered as a leaf node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZZj7iccdeqa5",
      "metadata": {
        "id": "ZZj7iccdeqa5"
      },
      "outputs": [],
      "source": [
        "### split into child nodes\n",
        "def get_split(self,data,col_index,value):\n",
        "        left_child, right_child = [], []\n",
        "        for index in range(len(data)):\n",
        "            if data[index][col_index] < value:\n",
        "                left_child.append(data[index])\n",
        "            if data[index][col_index] > value:\n",
        "                right_child.append(data[index])\n",
        "        return {'l':left_child, 'r':right_child}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7555296",
      "metadata": {
        "id": "d7555296"
      },
      "outputs": [],
      "source": [
        "### function which will create a new node will following attributes\n",
        "# value - the value of the node\n",
        "# col index - feature index for future use when predicting on sample\n",
        "# gini value - gini value for this node\n",
        "# left - pointing to left child array during tree build process or pointing to left node after tree is built\n",
        "# right - pointing to right child array during tree build process or pointing to right node after tree is built\n",
        "### it also checks for some other constraints\n",
        "def create_node(data):\n",
        "        node = Node(None)\n",
        "        \n",
        "        ### checking if node can be further split using minimum gini as criterion\n",
        "        gini_score = gini_index(data)\n",
        "        if gini_score <= min_gini:\n",
        "            node.is_leaf = True\n",
        "            node.value = np.bincount([row[-1] for row in data]).argmax()\n",
        "            node.gini_value = gini_score\n",
        "            return node\n",
        "        \n",
        "        ### checking if node has enough samples to be split again \n",
        "        if len(data) <= min_samples:\n",
        "            node.is_leaf = True\n",
        "            node.value = np.bincount([row[-1] for row in data]).argmax()\n",
        "            node.gini_value = gini_score\n",
        "            return node\n",
        "        ### finding minimum gini impurity split \n",
        "        gini_index = 1.0\n",
        "        for col_index in range(len(data[0])-1):\n",
        "            for row_index in range(len(data)):\n",
        "                value = data[row_index][col_index]\n",
        "                child = get_split(data,col_index,value)\n",
        "                node_gini_index = self.calculate_gini_index(value,child)\n",
        "                if node_gini_index < gini_index:       # parent gini index < gini index of current node\n",
        "                    gini_index = node_gini_index\n",
        "                    node.value = value\n",
        "                    node.gini_value = node_gini_index\n",
        "                    node.col_index = col_index\n",
        "                    node.left = child['l']\n",
        "                    node.right = child['r']\n",
        "        return node"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "805PvI5vYKQt",
      "metadata": {
        "id": "805PvI5vYKQt"
      },
      "source": [
        "max depth - (set by the user to make sure we only built tree up to a certain depth\n",
        "\n",
        "\n",
        "traverse tree - inorder traversal of tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2lTf2dHTYuP-",
      "metadata": {
        "id": "2lTf2dHTYuP-"
      },
      "outputs": [],
      "source": [
        "def build_tree(node,current_depth):\n",
        "        ## create left subtree for the node if possible under constraints\n",
        "        if  current_depth < max_depth:\n",
        "            ## creating left node \n",
        "            if node.left is not None and isinstance(node.left,list):\n",
        "                left_node = create_node(node.left)\n",
        "                node.left = left_node\n",
        "                if node.left.is_leaf is not True:\n",
        "                    build_tree(node.left,current_depth+1)\n",
        "            if node.right is not None and isinstance(node.right,list):\n",
        "            ## creating right node\n",
        "                right_node = create_node(node.right)\n",
        "                node.right = right_node\n",
        "\n",
        "                if node.right.is_leaf is not True:\n",
        "                    build_tree(node.right,current_depth+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PVXIDEGkfGGt",
      "metadata": {
        "id": "PVXIDEGkfGGt"
      },
      "outputs": [],
      "source": [
        "def fit(self,data):\n",
        "        self.classes = list(set(int(row[-1]) for row in data))\n",
        "        self.root = self.create_node(data) \n",
        "        self.build_tree(self.root,current_depth = 0)\n",
        "        return self.root\n",
        "\n",
        "### prediction\n",
        "def predict(self,sample):\n",
        "        predictions = []\n",
        "        for row in sample:\n",
        "            node = self.root\n",
        "            while node.is_leaf is not True:\n",
        "                if row[node.col_index] < node.value:\n",
        "                    node = node.left\n",
        "                    continue\n",
        "                if row[node.col_index] >= node.value:\n",
        "                    node = node.right\n",
        "            predictions.append(node.value)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iI_UorFvYuwh",
      "metadata": {
        "id": "iI_UorFvYuwh"
      },
      "outputs": [],
      "source": [
        "def traverse(self):\n",
        "        tree = {'root':str(self.root.value)}\n",
        "        stack = [self.root]\n",
        "        node = self.root\n",
        "        while len(stack) > 0:\n",
        "            while node is not None:\n",
        "                stack.append(node)\n",
        "                node = node.left            \n",
        "            node = stack.pop(-1)\n",
        "            if node.is_leaf is not True:\n",
        "                tree[str(node.value)] = {'left':node.left.value,'right':node.right.value,'feature':node.col_index,'gini value':node.gini_value}\n",
        "            else:\n",
        "                tree[str(node.value)] = {'left':'None','right':'None','class label':node.value,'gini value':node.gini_value,'leaf':True}\n",
        "            node = node.right\n",
        "        return tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TjRagO9QY2bY",
      "metadata": {
        "id": "TjRagO9QY2bY"
      },
      "source": [
        "Now using all components togethwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SxV9TtKSW5VW",
      "metadata": {
        "id": "SxV9TtKSW5VW"
      },
      "outputs": [],
      "source": [
        "class Node(object):\n",
        "    def __init__(self,value,is_leaf=False):\n",
        "        self.value = value\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.gini_value = None\n",
        "        self.col_index = None\n",
        "        self.is_leaf = is_leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YmPMi5QwW5Zg",
      "metadata": {
        "id": "YmPMi5QwW5Zg"
      },
      "outputs": [],
      "source": [
        "class decision_tree_classifier(object):\n",
        "    def __init__(self,min_samples=10,min_gini=0.2,max_depth=5):\n",
        "        self.root = None\n",
        "        self.min_samples = min_samples\n",
        "        self.min_gini = min_gini\n",
        "        self.max_depth = max_depth\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self,data):\n",
        "    \n",
        "    def create_node(self,data):\n",
        "    \n",
        "    def gini_index(self,data):\n",
        "\n",
        "    def calculate_gini_index(self,value,child):\n",
        "\n",
        "    def get_split(self,data,col_index,value):\n",
        "\n",
        "    def build_tree(self,node,current_depth):\n",
        "                    \n",
        "    def traverse(self):\n",
        "\n",
        "    def predict(self,sample):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gU5mAT3gW5dL",
      "metadata": {
        "id": "gU5mAT3gW5dL"
      },
      "outputs": [],
      "source": [
        "def accuracy(data):\n",
        "    dt = decision_tree_classifier()\n",
        "    tree = dt.fit(data)\n",
        "    print('<========= decision tree ===========>')\n",
        "    print(dt.traverse())\n",
        "    predictions = dt.predict(data[0:-1][0:-1])\n",
        "    true_values = [row[-1] for row in data]\n",
        "    return '{:1f}'.format(sum([t==p for t,p in zip(true_values, predictions)])/len(true_values) *100 ) + '% accuracy' "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S2bWeUtNdwSu",
      "metadata": {
        "id": "S2bWeUtNdwSu"
      },
      "source": [
        "Using the above algorithm on Iris flower dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n407uISQW5gW",
      "metadata": {
        "id": "n407uISQW5gW"
      },
      "outputs": [],
      "source": [
        "def flower_to_id(value):\n",
        "    if value == 'Iris-virginica':\n",
        "        return 2\n",
        "    if value == 'Iris-versicolor':\n",
        "        return 0\n",
        "    if value == 'Iris-setosa':\n",
        "        return 1\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "    values = requests.get(url).content.decode('utf-8').split('\\n')\n",
        "    data_set = [val.split(',') for val in values]\n",
        "    data_df = pd.DataFrame(data=data_set,columns=['sepal_length','sepal_width','petal_length','petal_width','flower'])\n",
        "    data_df = data_df.dropna()\n",
        "    data_df[['sepal_length','sepal_width','petal_length','petal_width']] = data_df[['sepal_length','sepal_width','petal_length','petal_width']].astype('float')\n",
        "    data_df['flower'] = data_df['flower'].map(flower_to_id)\n",
        "    return data_df\n",
        "    \n",
        "data = get_data()\n",
        "print(accuracy(data.to_numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56213847",
      "metadata": {
        "id": "56213847"
      },
      "source": [
        "## Avoid Overfitting\n",
        "\n",
        "Overfitting is one of the key challenges in a tree-based algorithm. If no limit is set, it will give 100% fitting, because, in the worst-case scenario, it will end up making a leaf node for each observation. Hence we need to take some precautions to avoid overfitting. It is mostly done in two ways:\n",
        "\n",
        "    Setting constraints on tree size\n",
        "    Tree pruning\n",
        "\n",
        "Let’s discuss both of these briefly.\n",
        "Setting Constraints on tree size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e9a1d52",
      "metadata": {
        "id": "3e9a1d52"
      },
      "source": [
        "Setting Constraints on tree size\n",
        "\n",
        "Parameters play an important role in tree modeling. Overfitting can be avoided by using various parameters that are used to define a tree.\n",
        "\n",
        "##### Minimum samples for a node split\n",
        "    1. Defines the minimum number of observations that are required in a node to be considered for splitting. (this ensures above mentioned worst-case scenario).\n",
        "    2. A higher value of this parameter prevents a model from learning relations that might be highly specific to the particular sample selected for a tree.\n",
        "    3. Too high values can lead to under-fitting hence, it should be tuned properly using cross-validation.\n",
        "##### Minimum samples for a leaf node\n",
        "    1. Defines the minimum observations required in a leaf. (Again this also prevents worst-case scenarios)\n",
        "    2. Generally, lower values should be chosen for imbalanced class problems as the regions in which the minority class will be in majority will be of small size.\n",
        "##### Maximum depth of the tree (vertical depth)\n",
        "    1. Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample.\n",
        "    2. Should be tuned properly using Cross-validation as too little height can cause underfitting.\n",
        "##### Maximum number of leaf nodes\n",
        "    1. The maximum number of leaf nodes or leaves in a tree.\n",
        "    2. Can be defined in place of max_depth. Since binary trees are created, a depth of n would produce a maximum of 2^n leaves.\n",
        "##### Maximum features to consider for a split\n",
        "    1. The number of features to consider while searching for the best split. These will be randomly selected.\n",
        "    2. As a thumb-rule, the square root of the total number of features works great but we should check up to 30–40% of the total number of features.\n",
        "    3. Higher values can lead to over-fitting but depend on case to case.\n",
        "\n",
        "![constraints_tree_size.png](attachment:constraints_tree_size.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ae65da",
      "metadata": {
        "id": "e7ae65da"
      },
      "source": [
        "## Pruning\n",
        "\n",
        "\n",
        "![p1.png](attachment:p1.png)\n",
        "\n",
        "\n",
        "\n",
        "Recall from terminologies, pruning is something opposite to splitting. Now we see how exactly that is the case. Let’s see the following example where drug effectiveness is plotted against drug doses. All the red points are the training dataset. We can see that for very small and very large quantities of doses, the drug effectiveness is almost negligible. Between 10–20mg, almost 100% and gradually decreasing between 20 to 30.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "336c841d",
      "metadata": {
        "id": "336c841d"
      },
      "source": [
        "\n",
        "![p2.png](attachment:p2.png)\n",
        "\n",
        "We modeled a tree and we got the following results. we did splitting at three places and got 4 leaf nodes which will give output as 0(y): 0–10(X), 100:10–20, 70:20–30, 0:30–40 respectively as we increase the doses. It is a great fit for the training dataset, the black horizontal lines are output given for the node.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37891568",
      "metadata": {
        "id": "37891568"
      },
      "source": [
        "\n",
        "![p3.png](attachment:p3.png)\n",
        "\n",
        "Let’s test it on the training dataset (blue dots in the following image are of the testing dataset). It is a bad fit for testing data. A good fit for training data and a bad fit for testing data means overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7633375e",
      "metadata": {
        "id": "7633375e"
      },
      "source": [
        "\n",
        "![p4.png](attachment:p4.png)\n",
        "\n",
        "To overcome the overfitting issue of our tree, we decide to merge two segments in the middle which means removing two nodes from the tree as you can see in the image below(the nodes with the red cross on them are removed). now again we fit this tree on the training dataset. Now, the tree is not that great fit for training data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48752fa9",
      "metadata": {
        "id": "48752fa9"
      },
      "source": [
        "\n",
        "![p5.png](attachment:p5.png)\n",
        "\n",
        "But, when we introduce testing data, it performs better than before.\n",
        "\n",
        "This is basically pruning. Using pruning we can avoid overfitting to the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12d86198",
      "metadata": {
        "id": "12d86198"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}