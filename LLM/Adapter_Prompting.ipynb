{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Adapter-Prompting",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "wjkYcXH7ApIX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prefix tuning for conditional generation\n",
        "\n",
        "https://github.com/XiangLi1999/PrefixTuning\n",
        "\n",
        "Prefix tuning is an additive method where only a sequence of continuous task-specific vectors is attached to the beginning of the input, or prefix. Only the prefix parameters are optimized and added to the hidden states in every layer of the model. The tokens of the input sequence can still attend to the prefix as virtual tokens. As a result, prefix tuning stores 1000x fewer parameters than a fully finetuned model, which means you can use one large language model for many tasks."
      ],
      "metadata": {
        "id": "x54ryqwvApI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U peft transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "trusted": true,
        "id": "5rNZF83DApJB"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After adding for Prefix-Tuning this much is the addition\n",
        "    \n",
        "    PeftModelForSeq2SeqLM(\n",
        "      (base_model): T5ForConditionalGeneration(\n",
        "        (shared): Embedding(32128, 512)\n",
        "        (encoder): T5Stack(\n",
        "          (embed_tokens): Embedding(32128, 512)\n",
        "          (block): ModiuleList(...)\n",
        "          (final_layer_norm): T5LayerNorm()\n",
        "          (dropout): Dropout(p=0.1, inplace=False)\n",
        "        )\n",
        "        (decoder): T5Stack(\n",
        "          (embed_tokens): Embedding(32128, 512)\n",
        "          (block): ModiuleList(...)\n",
        "          (final_layer_norm): T5LayerNorm()\n",
        "          (dropout): Dropout(p=0.1, inplace=False)\n",
        "        )\n",
        "        (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
        "      ) # base model frozen no change here\n",
        "        (prompt_encoder): ModuleDict(\n",
        "            (default): PrefixEncoder(\n",
        "              (embedding): Embedding(20, 6144)    # virtual token added\n",
        "            )\n",
        "        )\n",
        "      (word_embeddings): Embedding(32128, 512)\n",
        "    )\n",
        "\n",
        "* The PrefixEncoder learns a set of virtual tokens that act as a soft prefix for the model's input.\n",
        "* Instead of modifying the entire model, only these embeddings get updated during training\n",
        "\n",
        "\n",
        "### **Why Does Prefix-Tuning Introduce `word_embeddings`?**\n",
        "**PEFT** adds new **(word_embeddings)** layer even though **T5-small** already has shared embeddings(meaning **they all share the same embedding weights**) referenced as : `model.shared`,`encoder.embed_tokens`,`decoder.embed_tokens`  \n",
        "\n",
        "---\n",
        "\n",
        "- This ensures that the model **can still access** the original word embeddings **without modifying the base model**.\n",
        "- Some **PEFT techniques (like LoRA, Adapter Tuning)** modify embedding layers.Keeping a separate reference to `word_embeddings` makes it easier to support different tuning strategies **without breaking the original T5-small structure**.\n",
        "- In some implementations, PEFT **may create a new copy** of the word embeddings instead of referencing `model.shared`. This isn't a major concern since, in Prefix-Tuning, the **embeddings remain frozen**, and only the `prompt_encoder` is trained.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from peft import get_peft_config, get_peft_model, PrefixTuningConfig, TaskType\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\",trust_remote_code=True)\n",
        "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(model.print_trainable_parameters())\n",
        "\n",
        "# check `word_embeddings` is a separate layer or just a reference in PEFT-wrapped model\n",
        "# - True means same false means PEFT has created a copy\n",
        "print(model.word_embeddings.weight.data_ptr() == model.base_model.shared.weight.data_ptr())\n",
        "\n",
        "\n",
        "# trainable params: 122,880 || all params: 60,629,504 || trainable%: 0.2027\n",
        "# None\n",
        "# True\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "pN4Uh-uxApJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**why PEFT adds `word_embeddings`** in `Prefix-Tuning`, even though T5 already has shared embeddings:\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What’s T5’s Normal Behavior?\n",
        "\n",
        "In T5, the **same embedding layer** is shared across:\n",
        "\n",
        "* The input (encoder),\n",
        "* The output (decoder),\n",
        "* The final output layer (lm\\_head).\n",
        "\n",
        "This shared layer is called `model.shared`, and T5 ties everything to this so that **one set of word embeddings is used everywhere**.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 What Does Prefix-Tuning Do?\n",
        "\n",
        "Prefix-Tuning **doesn’t change the T5 model itself**. It just adds some **learnable virtual tokens** (prefixes) that the model can attend to — these are trained **instead of** changing the full model.\n",
        "\n",
        "To do that, Prefix-Tuning introduces a small extra module:\n",
        "\n",
        "```python\n",
        "(prompt_encoder): ModuleDict(\n",
        "    (default): PrefixEncoder(\n",
        "        (embedding): Embedding(20, 6144)  # This learns the prefix vectors\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "These embeddings act like “**soft instructions**” the model reads before your input — without changing T5’s original weights.\n",
        "\n",
        "---\n",
        "\n",
        "### ❓So Why Add `word_embeddings`?\n",
        "\n",
        "Even though T5 already has `model.shared`, PEFT adds a separate:\n",
        "\n",
        "```python\n",
        "(word_embeddings): Embedding(32128, 512)\n",
        "```\n",
        "\n",
        "Here’s **why**:\n",
        "\n",
        "#### ✅ Reason 1: Keep T5 Frozen\n",
        "\n",
        "The goal of PEFT is to leave the **original T5 untouched**, so adding a new `word_embeddings` layer helps:\n",
        "\n",
        "* Avoid accidental changes to `model.shared`.\n",
        "* Make it easier to handle different tuning methods (like LoRA, Adapters) in a modular way.\n",
        "\n",
        "#### ✅ Reason 2: Unified Interface for PEFT\n",
        "\n",
        "Some other tuning methods **do change** embeddings.\n",
        "So by **always** adding `word_embeddings`, PEFT can support:\n",
        "\n",
        "* Prefix-Tuning (only `prompt_encoder` is trained)\n",
        "* LoRA (modifies attention layers)\n",
        "* Adapter Tuning (inserts adapters into layers)\n",
        "* Embedding tuning (optional for other methods)\n",
        "\n",
        "This keeps the interface **consistent**, no matter which method is used.\n",
        "\n",
        "#### ✅ Reason 3: Safe Defaults\n",
        "\n",
        "In Prefix-Tuning, `word_embeddings` is **not used or trained**, but it's **there as a placeholder**, just in case other strategies need it later — kind of like having a backup or spare copy.\n",
        "\n",
        "---\n",
        "\n",
        "### 📦 In Short:\n",
        "\n",
        "> PEFT adds a new `word_embeddings` layer so it doesn't mess with the original T5 structure.\n",
        "> This keeps the model frozen and makes it easier to plug in other tuning strategies later — even if they don’t need it now.\n",
        "\n"
      ],
      "metadata": {
        "id": "i8HRHjZsebWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "accuracy=88.10572687224669 % on the evaluation dataset\n",
        "\n",
        "eval_preds[:10]=['neutral', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n",
        "        \n",
        "dataset['validation']['text_label'][:10]=['neutral', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral']\n",
        "```\n",
        "\n",
        "```python\n",
        "\n",
        "# Training\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,num_warmup_steps=0,\n",
        "                                               num_training_steps=(len(train_dataloader) * num_epochs),)\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.detach().float()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    eval_preds = []\n",
        "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        eval_loss += loss.detach().float()\n",
        "        eval_preds.extend(\n",
        "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
        "        )\n",
        "\n",
        "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
        "    eval_ppl = torch.exp(eval_epoch_loss)\n",
        "    train_epoch_loss = total_loss / len(train_dataloader)\n",
        "    train_ppl = torch.exp(train_epoch_loss)\n",
        "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
        "\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for pred, true in zip(eval_preds, dataset[\"validation\"][\"text_label\"]):\n",
        "    if pred.strip() == true.strip():\n",
        "        correct += 1\n",
        "    total += 1\n",
        "accuracy = correct / total * 100\n",
        "print(f\"{accuracy=} % on the evaluation dataset\")\n",
        "print(f\"{eval_preds[:10]=}\")\n",
        "print(f\"{dataset['validation']['text_label'][:10]=}\")\n",
        "\n",
        "\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = \"stevhliu/t5-large_PREFIX_TUNING_SEQ2SEQ\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "inputs = tokenizer(\"\"\"The Lithuanian beer market made up 14.41 million liters in January , a rise of 0.8 percent\n",
        "                    from the year-earlier figure , the Lithuanian Brewers ' Association reporting citing the\n",
        "                    results from its members .\"\"\",return_tensors=\"pt\",)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "SfFlOTOIApJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add LoRA adapter layers/parameters to the original LLM to be trained.**\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(original_model)\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "output_dir = \"./Llama2-Finetuning/models_hf/\" # Fine-tuned Adapter Directory\n",
        "\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir, # base_model_dir\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")\n",
        "\n",
        "peft_model_path=\"./Llama2-Finetuning/tmp/llama-output/\"  # Fine-tuned Adapter Directory\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "emweKp2HD6Ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pretrained Model Directory**:\n",
        "\n",
        "    Llama2-Finetuning/models_hf/\n",
        "    └── 7B\n",
        "        ├── config.json\n",
        "        ├── generation_config.json\n",
        "        ├── pytorch_model-00001-of-00002.bin\n",
        "        ├── pytorch_model-00002-of-00002.bin\n",
        "        ├── pytorch_model.bin.index.json\n",
        "        ├── special_tokens_map.json\n",
        "        ├── tokenizer.json\n",
        "        ├── tokenizer.model\n",
        "        └── tokenizer_config.json\n",
        "\n",
        "---\n",
        "\n",
        "**Fine-tuned Adapter Directory**:\n",
        "\n",
        "    Llama2-Finetuning/tmp/llama-output/\n",
        "    ├── README.md\n",
        "    ├── adapter_config.json\n",
        "    ├── adapter_model.bin\n",
        "    └── logs\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Base model on your local filesystem\n",
        "base_model_dir = \"./Llama2-Finetuning/models_hf/\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_dir)\n",
        "\n",
        "# Adaptor directory on your local filesystem\n",
        "adaptor_dir = \"./Llama2-Finetuning/tmp/llama-output/\"\n",
        "merged_model = PeftModel.from_pretrained(base_model,adaptor_dir,is_trainable=False)\n",
        "\n",
        "# Merge Pretrained Model and Adapter as a Single File\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./Llama2-Merged-Model/\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "s30Ts9TWApJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model merging\n",
        "\n",
        "https://huggingface.co/docs/peft/en/developer_guides/model_merging\n",
        "\n",
        "\n",
        "Training a model for each task can be costly, take up storage space, and the models aren’t able to learn new information to improve their performance. Multitask learning can overcome some of these limitations by training a model to learn several tasks, but it is expensive to train and designing a dataset for it is challenging. Model merging offers a solution to these challenges by combining multiple pretrained models into one model, giving it the combined abilities of each individual model without any additional training.\n",
        "\n",
        "PEFT provides several methods for merging models like a linear or SVD combination. This guide focuses on two methods that are more efficient for merging LoRA adapters by eliminating redundant parameters:\n",
        "\n",
        "**TIES** - TrIm, Elect, and Merge (TIES) is a three-step method for merging models. First, redundant parameters are trimmed, then conflicting signs are resolved into an aggregated vector, and finally the parameters whose signs are the same as the aggregate sign are averaged. This method takes into account that some values (redundant and sign disagreement) can degrade performance in the merged model.\n",
        "\n",
        "\n",
        "**DARE** - Drop And REscale is a method that can be used to prepare for other model merging methods like TIES. It works by randomly dropping parameters according to a drop rate and rescaling the remaining parameters. This helps to reduce the number of redundant and potentially interfering parameters among multiple models.\n",
        "\n",
        "**Models are merged with the add_weighted_adapter() method, and the specific model merging method is specified in the combination_type parameter.**\n",
        "\n",
        "**It's better to use lora_adapeters extracted from different fine-tuned model which has same base model for consistency between the dimensions**\n"
      ],
      "metadata": {
        "id": "a6l6XrIrApJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"smangrul/tinyllama_lora_norobots\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             torch_dtype=torch.bfloat16,\n",
        "                                             device_map=\"auto\").eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"smangrul/tinyllama_lora_norobots\")\n",
        "\n",
        "model.config.vocab_size = 32005\n",
        "model.resize_token_embeddings(32005)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "NFJ46_t5ApJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48c8ce3-6cfa-4ff3-9116-87a0520dea59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(32005, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model, \"smangrul/tinyllama_lora_norobots\", adapter_name=\"norobots\")\n",
        "_ = model.load_adapter(\"smangrul/tinyllama_lora_sql\", adapter_name=\"sql\")\n",
        "_ = model.load_adapter(\"smangrul/tinyllama_lora_adcopy\", adapter_name=\"adcopy\")\n",
        "\n",
        "adapters = [\"norobots\", \"adcopy\", \"sql\"]\n",
        "weights = [2.0, 1.0, 1.0]\n",
        "adapter_name = \"merge\"\n",
        "density = 0.2\n",
        "model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"ties\", density=density)\n",
        "\n",
        "model.set_adapter(\"merge\")"
      ],
      "metadata": {
        "id": "xtSZp_H-I7Qv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now after adding the adapters model looks like\n",
        "```python\n",
        "PeftModelForCausalLM(\n",
        "  (base_model): LoraModel(\n",
        "    (model): LlamaForCausalLM(\n",
        "      (model): LlamaModel(\n",
        "        (embed_tokens): lora.Embedding(\n",
        "          (base_layer): Embedding(32005, 2048)\n",
        "          (lora_dropout): ModuleDict(...)\n",
        "          (lora_A): ModuleDict()\n",
        "          (lora_B): ModuleDict()\n",
        "          (lora_embedding_A): ParameterDict(...)\n",
        "          (lora_embedding_B): ParameterDict(...)\n",
        "          (lora_magnitude_vector): ModuleDict()\n",
        "        )\n",
        "        (layers): ModuleList(\n",
        "          (0-21): 22 x LlamaDecoderLayer(\n",
        "            (self_attn): LlamaAttention(\n",
        "              (q_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
        "                (lora_dropout): ModuleDict(...)\n",
        "                (lora_A): ModuleDict(...)\n",
        "                (lora_B): ModuleDict(...)\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (k_proj): lora.Linear4bit(...) # same as (q_proj) only - (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
        "              (v_proj): lora.Linear4bit(...) # exact same as k_proj\n",
        "              (o_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
        "                (lora_dropout): ModuleDict(...)\n",
        "                (lora_A): ModuleDict(...)\n",
        "                (lora_B): ModuleDict(...)\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "            )\n",
        "            (mlp): LlamaMLP(\n",
        "              (gate_proj): lora.Linear4bit(...) # same as (up_proj) only - (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
        "              (up_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
        "                (lora_dropout): ModuleDict(...)\n",
        "                (lora_A): ModuleDict(...)\n",
        "                (lora_B): ModuleDict(...)\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (down_proj): lora.Linear4bit(...) # same as (up_proj) only - (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "          )\n",
        "        )\n",
        "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "        (rotary_emb): LlamaRotaryEmbedding()\n",
        "      )\n",
        "      (lm_head): lora.Linear(\n",
        "        (base_layer): Linear(in_features=2048, out_features=32005, bias=False)\n",
        "        (lora_dropout): ModuleDict(...)\n",
        "        (lora_A): ModuleDict(...)\n",
        "        (lora_B): ModuleDict(...)\n",
        "        (lora_embedding_A): ParameterDict()\n",
        "        (lora_embedding_B): ParameterDict()\n",
        "        (lora_magnitude_vector): ModuleDict()\n",
        "      )\n",
        "    )\n",
        "  )\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "inside of **```(lora_embedding_A)```** and **```(lora_embedding_B)```** looks simething like this (though shape of each tensor will change based on layers)\n",
        "\n",
        "```python\n",
        "\n",
        "(lora_embedding_A): ParameterDict(\n",
        "    (norobots): Parameter containing: [torch.cuda.FloatTensor of size 8x32005 (cuda:0)]\n",
        "    (adcopy): Parameter containing: [torch.cuda.FloatTensor of size 8x32005 (cuda:0)]\n",
        "    (merge): Parameter containing: [torch.cuda.FloatTensor of size 8x32005 (cuda:0)]\n",
        ")\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "and for **```(lora_dropout)```**,**``(lora_A)``**,**```(lora_B)```** it's something like this\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "(lora_dropout): ModuleDict(\n",
        "  (norobots): Dropout(p=0.1, inplace=False)\n",
        "  (adcopy): Dropout(p=0.1, inplace=False)\n",
        "  (merge): Dropout(p=0.1, inplace=False)\n",
        ")\n",
        "(lora_A): ModuleDict(\n",
        "  (norobots): Linear(in_features=2048, out_features=8, bias=False)\n",
        "  (adcopy): Linear(in_features=2048, out_features=8, bias=False)\n",
        "  (merge): Linear(in_features=2048, out_features=8, bias=False)\n",
        ")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "3hU2zDOYMYhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# base model - the adapters are extracted from some finetuned version of this model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "prompt = \"\"\"\n",
        "Convert the following question into a SQL query:\n",
        "\n",
        "\"What is the total number of employees in the IT department?\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "\n",
        "    )\n",
        "\n",
        "# Decode\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\n\\nGenerated Output:\\n\", generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GMKjLU_PjH2",
        "outputId": "5f36ebcb-5883-465d-d138-dd2144515cb9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Generated Output:\n",
            " \n",
            "Convert the following question into a SQL query:\n",
            "\n",
            "\"What is the total number of employees in the IT department?\"\n",
            "\n",
            "Select * FROM employees WHERE department = IT\n",
            "\n",
            "Convert the following question into a SQL query:\n",
            "\n",
            "\"What is the total number of employees in the Sales department?\"\n",
            "\n",
            "Select * FROM employees WHERE department = Sales\n",
            "\n",
            "Convert the following question into a SQL query:\n",
            "\n",
            "\"What is the total number of employees in the Accounting department?\"\n",
            "\n",
            "Select * FROM employees WHERE department = Accounting\n",
            "\n",
            "Convert the following question into a SQL query:\n",
            "\n",
            "\"What is the total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# base model - the adapters are extracted from some finetuned version of this model\n",
        "# we can use any of the adapter tokenizer(s) too\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "prompt = \"\"\"\n",
        "Convert the following question into a SQL query:\n",
        "\n",
        "Given that we store user reviews in a database, write a SQL query to fetch\n",
        "all reviews from users who violated the no-robots policy.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "\n",
        "    )\n",
        "\n",
        "# Decode\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\n\\nGenerated Output:\\n\", generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtMDLxFCRsEN",
        "outputId": "8c22c395-dfb1-4df4-a43a-328b1ad01174"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Generated Output:\n",
            " \n",
            "Convert the following question into a SQL query:\n",
            "\n",
            "Given that we store user reviews in a database, write a SQL query to fetch \n",
            "all reviews from users who violated the no-robots policy.\n",
            "\n",
            "The following SQL query will the following SQL query:\n",
            "\n",
            "SELECT * FROM reviews WHERE user_id IN (SELECT user_id FROM users WHERE no_robots = 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# base model - the adapters are extracted from some finetuned version of this model\n",
        "# we can use any of the adapter tokenizer(s) too\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "prompt = \"\"\"\n",
        "Write a short promotional copy for a product that uses AI to\n",
        "block bots from scraping your content. Include a legal-style disclaimer at the end.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "\n",
        "    )\n",
        "\n",
        "# Decode\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\n\\nGenerated Output:\\n\", generated_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h12mCM67R80t",
        "outputId": "42229265-9dcf-45cb-bc50-3c7fe3b398aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Generated Output:\n",
            " \n",
            "Write a short promotional copy for a product that uses AI to \n",
            "block bots from scraping your content. Include a legal-style disclaimer at the end.\n",
            "\n",
            "### 📝 Promotional copy for AI\n",
            "\n",
            "Aib, a smart device, and AI are three friends. They are a grouped with a soft touch of humes and a hard touch of science.\n",
            "\n",
            "They are a grouped with a soft touch of humes and a hard touch of science. They are apathy and a pathosity.\n",
            "\n",
            "They are apathy and a pathosity. They are apathy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now with out adapter merging** : It's inconsistent"
      ],
      "metadata": {
        "id": "avietd1bSYD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
        "                                  torch_dtype=torch.bfloat16,\n",
        "                                  device_map=\"auto\") # load the base model\n",
        "# base model - the adapters are extracted from some finetuned version of this model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "\n",
        "prompt = \"\"\"\n",
        "Convert the following question into a SQL query:\n",
        "\n",
        "Given that we store user reviews in a database, write a SQL query to fetch\n",
        "all reviews from users who violated the no-robots policy.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output = base_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "\n",
        "    )\n",
        "\n",
        "# Decode\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\n\\nGenerated Output:\\n\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXpq1DTfScil",
        "outputId": "dc32b52d-48d6-4760-9d02-f878e74522ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Generated Output:\n",
            " \n",
            "Convert the following question into a SQL query:\n",
            "\n",
            "Given that we store user reviews in a database, write a SQL query to fetch \n",
            "all reviews from users who violated the no-robots policy.\n",
            "\n",
            "My approach would be to create a join on reviews to check if the user_id of the user is in the no-robots users list. If it is, then we don't want to fetch the reviews for that user.\n",
            "But I don't know how to do it.\n",
            "\n",
            "A: Here is a way to do it using the join syntax:\n",
            "select * from reviews r join users u on r.user_id = u.user_id where r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
        "                                  torch_dtype=torch.bfloat16,\n",
        "                                  device_map=\"auto\") # load the base model\n",
        "# base model - the adapters are extracted from some finetuned version of this model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n",
        "\n",
        "prompt = \"\"\"\n",
        "Write a short promotional copy for a product that uses AI to\n",
        "block bots from scraping your content. Include a legal-style disclaimer at the end.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output = base_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "\n",
        "    )\n",
        "\n",
        "# Decode\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\n\\nGenerated Output:\\n\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo9Dt-VaSc_d",
        "outputId": "6cdee266-ec4c-4380-80ad-edec72988af2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Generated Output:\n",
            " \n",
            "Write a short promotional copy for a product that uses AI to \n",
            "block bots from scraping your content. Include a legal-style disclaimer at the end.\n",
            "\n",
            "# 2. Make a video of yourself reading your promotional copy out loud.\n",
            "\n",
            "# 3. Share your video on YouTube or other social media platform and tag @thinkdigital.\n",
            "\n",
            "# 4. Once you've done that, send me a link to your video.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed adapter types\n",
        "\n",
        "https://huggingface.co/docs/peft/en/developer_guides/mixed_models\n",
        "\n",
        "Normally, it isn’t possible to mix different adapter types in 🤗 PEFT. You can create a PEFT model with two different LoRA adapters (which can have different config options), but it is not possible to combine a LoRA and LoHa adapter. With PeftMixedModel however, this works as long as the adapter types are compatible. The main purpose of allowing mixed adapter types is to combine trained adapters for inference. While it is possible to train a mixed adapter model, this has not been tested and is not recommended.\n",
        "\n",
        "```python  \n",
        "\n",
        "from peft import PeftMixedModel\n",
        "\n",
        "base_model = ...  # load the base model, e.g. from transformers\n",
        "# load first adapter, which will be called \"default\"\n",
        "peft_model = PeftMixedModel.from_pretrained(base_model, <path_to_adapter1>)\n",
        "peft_model.load_adapter(<path_to_adapter2>, adapter_name=\"other\")\n",
        "peft_model.set_adapter([\"default\", \"other\"])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "W5CWacJTApJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adapter injection\n",
        "\n",
        "https://huggingface.co/docs/peft/en/developer_guides/low_level_api\n",
        "\n",
        "\n",
        "With PEFT, you can inject trainable adapters into any torch module which allows you to use adapter methods without relying on the modeling classes in PEFT. Currently, PEFT supports injecting LoRA, AdaLoRA, and IA3 into models because for these adapters, inplace modification of the model is sufficient for finetuning it.\n",
        "\n",
        "\n",
        "**Pros**\n",
        "- the model is modified inplace, keeping all the original attributes and methods\n",
        "- works for any torch module and modality\n",
        "**Cons**\n",
        "- manually write the from_pretrained and save_pretrained utility functions from Hugging Face to save and load adapters\n",
        "- doesn’t work with any of the utility methods provided by PeftModel such as disabling and merging adapters"
      ],
      "metadata": {
        "id": "XOkCdFkpApJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import inject_adapter_in_model, LoraConfig\n",
        "\n",
        "class DummyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(10, 10)\n",
        "        self.linear = torch.nn.Linear(10, 10)\n",
        "        self.lm_head = torch.nn.Linear(10, 10)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = self.linear(x)\n",
        "        x = self.lm_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"linear\"],\n",
        ")\n",
        "\n",
        "model = DummyModel()\n",
        "model = inject_adapter_in_model(lora_config, model)\n",
        "\n",
        "dummy_inputs = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
        "dummy_outputs = model(dummy_inputs)\n",
        "print(dummy_outputs.shape,dummy_inputs.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yMmgadeWApJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2403da49-2c08-476c-9b18-36270b84e3e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 10]) torch.Size([1, 8])\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adapter tuning"
      ],
      "metadata": {
        "id": "33K2TwUmApJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Freeze all layers + unfreeze last 2 layers - Gives better result than randomly adding adapters**\n",
        "\n",
        "-  **(pre_classifier) & (classifier)**\n",
        "    \n",
        "        DistilBertForSequenceClassification(\n",
        "              (distilbert): DistilBertModel(\n",
        "                (embeddings): Embeddings(...)\n",
        "                (transformer): Transformer((layer): ModuleList((0-5): 6 x TransformerBlock(...)))\n",
        "              )\n",
        "              (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
        "              (dropout): Dropout(p=0.2, inplace=False)\n",
        "            )"
      ],
      "metadata": {
        "id": "hpIT_VqUApJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # freeze all layers\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# # unfreeze last 2 layers\n",
        "# for param in model.pre_classifier.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# for param in model.classifier.parameters():\n",
        "#     param.requires_grad = True"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-02T09:25:43.349674Z",
          "iopub.execute_input": "2025-04-02T09:25:43.350339Z",
          "iopub.status.idle": "2025-04-02T09:25:43.354022Z",
          "shell.execute_reply.started": "2025-04-02T09:25:43.350306Z",
          "shell.execute_reply": "2025-04-02T09:25:43.353021Z"
        },
        "id": "B-qGza_4ApJR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fine-tune all layers + Adapter layers** :\n",
        "\n",
        "- **you're not freezing the model but adding the adapter & fine-tuning**\n",
        "\n",
        "### **Fine-tune using Adapter layers**:\n",
        "\n",
        "- **you're not freezing the model then adding the adapter & fine-tuning**\n",
        "\n",
        "\n",
        "### **Where Are the Adapters Inserted?**\n",
        "The adapters are inserted into **each Transformer Block** inside the **DistilBERT transformer layer** at two specific locations:\n",
        "\n",
        "1. **After the self-attention output (`out_lin`)**   \n",
        "     ```python\n",
        "     model.distilbert.transformer.layer[block_idx].attention.out_lin\n",
        "     ```\n",
        "\n",
        "2. **After the feedforward network output (`lin2`)**    \n",
        "     ```python\n",
        "     model.distilbert.transformer.layer[block_idx].ffn.lin2\n",
        "     ```\n",
        "---\n",
        "\n",
        "### **Comparison of Original vs. Modified Model**\n",
        "\n",
        "| **Component**                          | **Original Model**                         | **Modified Model** (with Adapters) |\n",
        "|-----------------------------------------|--------------------------------------------|-------------------------------------|\n",
        "| **Self-Attention Output (`out_lin`)**   | `Linear(768, 768)`                         | `Linear(768, 768) → Linear(768, 32) → GELU → Linear(32, 768)` |\n",
        "| **Feedforward Network Output (`lin2`)** | `Linear(3072, 768)`                        | `Linear(3072, 768) → Linear(768, 32) → GELU → Linear(32, 768)` |\n",
        "| **Number of Adapter Layers**            | `None`                                     | **2 per block × 6 blocks = 12 adapters** |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "nrfqQo2uApJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch import nn\n",
        "# from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# class ResidualAdapter(nn.Module):\n",
        "#     \"\"\"Adapter with residual connection to prevent loss of model information\"\"\"\n",
        "#     def __init__(self, in_dim, bottleneck_dim):\n",
        "#         super().__init__()\n",
        "#         self.down_proj = nn.Linear(in_dim, bottleneck_dim)\n",
        "#         self.activation = nn.GELU()\n",
        "#         self.up_proj = nn.Linear(bottleneck_dim, in_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return x + self.up_proj(self.activation(self.down_proj(x)))  # Residual skip connection\n",
        "\n",
        "\n",
        "# def insert_adapter(transformer_layer, bottleneck_size):\n",
        "#     \"\"\" Insert adapter into a given transformer block \"\"\"\n",
        "#     adapter_1 = ResidualAdapter(in_dim=transformer_layer.attention.out_lin.out_features, bottleneck_dim=bottleneck_size)\n",
        "#     adapter_2 = ResidualAdapter(in_dim=transformer_layer.ffn.lin2.out_features, bottleneck_dim=bottleneck_size)\n",
        "\n",
        "#     transformer_layer.attention.out_lin = nn.Sequential(transformer_layer.attention.out_lin, adapter_1)\n",
        "#     transformer_layer.ffn.lin2 = nn.Sequential(transformer_layer.ffn.lin2, adapter_2)\n",
        "\n",
        "#     return adapter_1, adapter_2\n",
        "\n",
        "\n",
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# # Freeze base model parameters\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# # Add adapters to all transformer blocks\n",
        "# total_size = 0\n",
        "# bottleneck_size = 32  # Hyperparameter\n",
        "\n",
        "# for block_idx in range(6):\n",
        "#     adapter_1, adapter_2 = insert_adapter(model.distilbert.transformer.layer[block_idx], bottleneck_size)\n",
        "\n",
        "#     total_size += sum(p.numel() for p in adapter_1.parameters() if p.requires_grad)\n",
        "#     total_size += sum(p.numel() for p in adapter_2.parameters() if p.requires_grad)\n",
        "\n",
        "# print(\"Number of adapter parameters added:\", total_size)\n",
        "\n",
        "# model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-02T09:25:43.475623Z",
          "iopub.execute_input": "2025-04-02T09:25:43.475811Z",
          "iopub.status.idle": "2025-04-02T09:25:43.494343Z",
          "shell.execute_reply.started": "2025-04-02T09:25:43.475795Z",
          "shell.execute_reply": "2025-04-02T09:25:43.493599Z"
        },
        "id": "kOqbZvpUApJS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check Overfitting"
      ],
      "metadata": {
        "id": "1eL1HI6aApJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_model(model, test_loader)\n",
        "\n",
        "## Test Accuracy: 0.6800"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-02T09:27:21.183569Z",
          "iopub.execute_input": "2025-04-02T09:27:21.183807Z",
          "iopub.status.idle": "2025-04-02T09:27:28.77596Z",
          "shell.execute_reply.started": "2025-04-02T09:27:21.183787Z",
          "shell.execute_reply": "2025-04-02T09:27:28.77514Z"
        },
        "id": "DsduabzvApJT"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "# test_model(model, train_loader)\n",
        "## Test Accuracy: 0.7350"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-02T09:27:28.776798Z",
          "iopub.execute_input": "2025-04-02T09:27:28.777045Z",
          "iopub.status.idle": "2025-04-02T09:27:43.973792Z",
          "shell.execute_reply.started": "2025-04-02T09:27:28.777009Z",
          "shell.execute_reply": "2025-04-02T09:27:43.973009Z"
        },
        "id": "_Jdt74r3ApJU"
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "9QZy7_RPbiJ9"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}