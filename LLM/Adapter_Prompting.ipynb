{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Adapter-Prompting",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "wjkYcXH7ApIX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prefix tuning for conditional generation\n",
        "\n",
        "https://github.com/XiangLi1999/PrefixTuning\n",
        "\n",
        "Prefix tuning is an additive method where only a sequence of continuous task-specific vectors is attached to the beginning of the input, or prefix. Only the prefix parameters are optimized and added to the hidden states in every layer of the model. The tokens of the input sequence can still attend to the prefix as virtual tokens. As a result, prefix tuning stores 1000x fewer parameters than a fully finetuned model, which means you can use one large language model for many tasks."
      ],
      "metadata": {
        "id": "x54ryqwvApI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U peft transformers"
      ],
      "metadata": {
        "trusted": true,
        "id": "5rNZF83DApJB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After adding for Prefix-Tuning this much is the addition\n",
        "    \n",
        "    PeftModelForSeq2SeqLM(\n",
        "      (base_model): T5ForConditionalGeneration(...) # frozen no change here\n",
        "        (prompt_encoder): ModuleDict(\n",
        "            (default): PrefixEncoder(\n",
        "              (embedding): Embedding(20, 6144)    # virtual token added\n",
        "            )\n",
        "        )\n",
        "      (word_embeddings): Embedding(32128, 512)\n",
        "    )\n",
        "\n",
        "* The PrefixEncoder learns a set of virtual tokens that act as a soft prefix for the model's input.\n",
        "* Instead of modifying the entire model, only these embeddings get updated during training\n",
        "\n",
        "\n",
        "### **Why Does Prefix-Tuning Introduce `word_embeddings`?**\n",
        "**PEFT** adds new **(word_embeddings)** layer even though **T5-small** already has shared embeddings(meaning **they all share the same embedding weights**) referenced as : `model.shared`,`encoder.embed_tokens`,`decoder.embed_tokens`  \n",
        "\n",
        "---\n",
        "\n",
        "- This ensures that the model **can still access** the original word embeddings **without modifying the base model**.\n",
        "- Some **PEFT techniques (like LoRA, Adapter Tuning)** modify embedding layers.Keeping a separate reference to `word_embeddings` makes it easier to support different tuning strategies **without breaking the original T5-small structure**.\n",
        "- In some implementations, PEFT **may create a new copy** of the word embeddings instead of referencing `model.shared`. This isn't a major concern since, in Prefix-Tuning, the **embeddings remain frozen**, and only the `prompt_encoder` is trained.\n"
      ],
      "metadata": {
        "id": "pN4Uh-uxApJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import get_peft_config, get_peft_model, PrefixTuningConfig, TaskType\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\",trust_remote_code=True)\n",
        "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(model.print_trainable_parameters())\n",
        "\n",
        "# check `word_embeddings` is a separate layer or just a reference in PEFT-wrapped model\n",
        "# - True means same false means PEFT has created a copy\n",
        "model.word_embeddings.weight.data_ptr() == model.base_model.shared.weight.data_ptr()"
      ],
      "metadata": {
        "trusted": true,
        "id": "x7QMpLDPApJG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy=88.10572687224669 % on the evaluation dataset\n",
        "\n",
        "- eval_preds[:10]=['neutral', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n",
        "        \n",
        "- dataset['validation']['text_label'][:10]=['neutral', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral']"
      ],
      "metadata": {
        "id": "SfFlOTOIApJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Training\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "# lr_scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,num_warmup_steps=0,\n",
        "#                                                num_training_steps=(len(train_dataloader) * num_epochs),)\n",
        "# model = model.to(device)\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
        "#         outputs = model(**batch)\n",
        "#         loss = outputs.loss\n",
        "#         total_loss += loss.detach().float()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         lr_scheduler.step()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#     model.eval()\n",
        "#     eval_loss = 0\n",
        "#     eval_preds = []\n",
        "#     for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(**batch)\n",
        "#         loss = outputs.loss\n",
        "#         eval_loss += loss.detach().float()\n",
        "#         eval_preds.extend(\n",
        "#             tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
        "#         )\n",
        "\n",
        "#     eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
        "#     eval_ppl = torch.exp(eval_epoch_loss)\n",
        "#     train_epoch_loss = total_loss / len(train_dataloader)\n",
        "#     train_ppl = torch.exp(train_epoch_loss)\n",
        "#     print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
        "\n",
        "\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# for pred, true in zip(eval_preds, dataset[\"validation\"][\"text_label\"]):\n",
        "#     if pred.strip() == true.strip():\n",
        "#         correct += 1\n",
        "#     total += 1\n",
        "# accuracy = correct / total * 100\n",
        "# print(f\"{accuracy=} % on the evaluation dataset\")\n",
        "# print(f\"{eval_preds[:10]=}\")\n",
        "# print(f\"{dataset['validation']['text_label'][:10]=}\")\n",
        "\n",
        "\n",
        "# from peft import PeftModel, PeftConfig\n",
        "\n",
        "# peft_model_id = \"stevhliu/t5-large_PREFIX_TUNING_SEQ2SEQ\"\n",
        "\n",
        "# config = PeftConfig.from_pretrained(peft_model_id)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "# model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "# inputs = tokenizer(\"\"\"The Lithuanian beer market made up 14.41 million liters in January , a rise of 0.8 percent\n",
        "#                     from the year-earlier figure , the Lithuanian Brewers ' Association reporting citing the\n",
        "#                     results from its members .\"\"\",return_tensors=\"pt\",)\n",
        "\n",
        "# model.to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "#     outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
        "#     print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Qsrvw_kAApJK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "trusted": true,
        "id": "XQZ4osX4ApJL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add LoRA adapter layers/parameters to the original LLM to be trained."
      ],
      "metadata": {
        "id": "S9K5zvaYApJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(original_model)\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "output_dir = \"./Llama2-Finetuning/models_hf/\" # Fine-tuned Adapter Directory\n",
        "\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir, # base_model_dir\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")\n",
        "\n",
        "peft_model_path=\"./Llama2-Finetuning/tmp/llama-output/\"  # Fine-tuned Adapter Directory\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bkyPSAP6ApJM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained Model Directory:\n",
        "\n",
        "    Llama2-Finetuning/models_hf/\n",
        "    └── 7B\n",
        "        ├── config.json\n",
        "        ├── generation_config.json\n",
        "        ├── pytorch_model-00001-of-00002.bin\n",
        "        ├── pytorch_model-00002-of-00002.bin\n",
        "        ├── pytorch_model.bin.index.json\n",
        "        ├── special_tokens_map.json\n",
        "        ├── tokenizer.json\n",
        "        ├── tokenizer.model\n",
        "        └── tokenizer_config.json\n",
        "\n",
        "---\n",
        "\n",
        "Fine-tuned Adapter Directory:\n",
        "\n",
        "    Llama2-Finetuning/tmp/llama-output/\n",
        "    ├── README.md\n",
        "    ├── adapter_config.json\n",
        "    ├── adapter_model.bin\n",
        "    └── logs"
      ],
      "metadata": {
        "id": "s30Ts9TWApJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Base model on your local filesystem\n",
        "base_model_dir = \"./Llama2-Finetuning/models_hf/\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_dir)\n",
        "\n",
        "# Adaptor directory on your local filesystem\n",
        "adaptor_dir = \"./Llama2-Finetuning/tmp/llama-output/\"\n",
        "merged_model = PeftModel.from_pretrained(base_model,adaptor_dir,is_trainable=False)\n",
        "\n",
        "# Merge Pretrained Model and Adapter as a Single File\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./Llama2-Merged-Model/\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "foSAorxZApJO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model merging\n",
        "\n",
        "https://huggingface.co/docs/peft/en/developer_guides/model_merging\n",
        "\n",
        "\n",
        "Training a model for each task can be costly, take up storage space, and the models aren’t able to learn new information to improve their performance. Multitask learning can overcome some of these limitations by training a model to learn several tasks, but it is expensive to train and designing a dataset for it is challenging. Model merging offers a solution to these challenges by combining multiple pretrained models into one model, giving it the combined abilities of each individual model without any additional training.\n",
        "\n",
        "PEFT provides several methods for merging models like a linear or SVD combination. This guide focuses on two methods that are more efficient for merging LoRA adapters by eliminating redundant parameters:\n",
        "\n",
        "**TIES** - TrIm, Elect, and Merge (TIES) is a three-step method for merging models. First, redundant parameters are trimmed, then conflicting signs are resolved into an aggregated vector, and finally the parameters whose signs are the same as the aggregate sign are averaged. This method takes into account that some values (redundant and sign disagreement) can degrade performance in the merged model.\n",
        "**DARE** - Drop And REscale is a method that can be used to prepare for other model merging methods like TIES. It works by randomly dropping parameters according to a drop rate and rescaling the remaining parameters. This helps to reduce the number of redundant and potentially interfering parameters among multiple models.\n",
        "\n",
        "Models are merged with the add_weighted_adapter() method, and the specific model merging method is specified in the combination_type parameter.\n",
        "\n"
      ],
      "metadata": {
        "id": "a6l6XrIrApJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"smangrul/tinyllama_lora_norobots\")\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, load_in_4bit=True, device_map=\"auto\").eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"smangrul/tinyllama_lora_norobots\")\n",
        "\n",
        "model.config.vocab_size = 32005\n",
        "model.resize_token_embeddings(32005)\n",
        "\n",
        "model = PeftModel.from_pretrained(model, \"smangrul/tinyllama_lora_norobots\", adapter_name=\"norobots\")\n",
        "_ = model.load_adapter(\"smangrul/tinyllama_lora_sql\", adapter_name=\"sql\")\n",
        "_ = model.load_adapter(\"smangrul/tinyllama_lora_adcopy\", adapter_name=\"adcopy\")\n",
        "\n",
        "adapters = [\"norobots\", \"adcopy\", \"sql\"]\n",
        "weights = [2.0, 1.0, 1.0]\n",
        "adapter_name = \"merge\"\n",
        "density = 0.2\n",
        "model.add_weighted_adapter(adapters, weights, adapter_name, combination_type=\"ties\", density=density)\n",
        "\n",
        "model.set_adapter(\"merge\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "NFJ46_t5ApJO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed adapter types\n",
        "\n",
        "https://huggingface.co/docs/peft/en/developer_guides/mixed_models\n",
        "\n",
        "Normally, it isn’t possible to mix different adapter types in 🤗 PEFT. You can create a PEFT model with two different LoRA adapters (which can have different config options), but it is not possible to combine a LoRA and LoHa adapter. With PeftMixedModel however, this works as long as the adapter types are compatible. The main purpose of allowing mixed adapter types is to combine trained adapters for inference. While it is possible to train a mixed adapter model, this has not been tested and is not recommended."
      ],
      "metadata": {
        "id": "W5CWacJTApJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftMixedModel\n",
        "\n",
        "base_model = ...  # load the base model, e.g. from transformers\n",
        "# load first adapter, which will be called \"default\"\n",
        "peft_model = PeftMixedModel.from_pretrained(base_model, <path_to_adapter1>)\n",
        "peft_model.load_adapter(<path_to_adapter2>, adapter_name=\"other\")\n",
        "peft_model.set_adapter([\"default\", \"other\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "FIKbxReVApJP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adapter injection\n",
        "\n",
        "https://huggingface.co/docs/peft/en/developer_guides/low_level_api\n",
        "\n",
        "\n",
        "With PEFT, you can inject trainable adapters into any torch module which allows you to use adapter methods without relying on the modeling classes in PEFT. Currently, PEFT supports injecting LoRA, AdaLoRA, and IA3 into models because for these adapters, inplace modification of the model is sufficient for finetuning it.\n",
        "\n",
        "\n",
        "**Pros**\n",
        "- the model is modified inplace, keeping all the original attributes and methods\n",
        "- works for any torch module and modality\n",
        "**Cons**\n",
        "- manually write the from_pretrained and save_pretrained utility functions from Hugging Face to save and load adapters\n",
        "- doesn’t work with any of the utility methods provided by PeftModel such as disabling and merging adapters"
      ],
      "metadata": {
        "id": "XOkCdFkpApJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import inject_adapter_in_model, LoraConfig\n",
        "\n",
        "class DummyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(10, 10)\n",
        "        self.linear = torch.nn.Linear(10, 10)\n",
        "        self.lm_head = torch.nn.Linear(10, 10)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = self.linear(x)\n",
        "        x = self.lm_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"linear\"],\n",
        ")\n",
        "\n",
        "model = DummyModel()\n",
        "model = inject_adapter_in_model(lora_config, model)\n",
        "\n",
        "dummy_inputs = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
        "dummy_outputs = model(dummy_inputs)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yMmgadeWApJQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adapter tuning"
      ],
      "metadata": {
        "id": "33K2TwUmApJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Freeze all layers + unfreeze last 2 layers - Gives better result than randomly adding adapters**\n",
        "\n",
        "-  **(pre_classifier) & (classifier)**\n",
        "    \n",
        "        DistilBertForSequenceClassification(\n",
        "              (distilbert): DistilBertModel(\n",
        "                (embeddings): Embeddings(...)\n",
        "                (transformer): Transformer((layer): ModuleList((0-5): 6 x TransformerBlock(...)))\n",
        "              )\n",
        "              (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
        "              (dropout): Dropout(p=0.2, inplace=False)\n",
        "            )"
      ],
      "metadata": {
        "id": "hpIT_VqUApJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # freeze all layers\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# # unfreeze last 2 layers\n",
        "# for param in model.pre_classifier.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "# for param in model.classifier.parameters():\n",
        "#     param.requires_grad = True"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-02T09:25:43.349674Z",
          "iopub.execute_input": "2025-04-02T09:25:43.350339Z",
          "iopub.status.idle": "2025-04-02T09:25:43.354022Z",
          "shell.execute_reply.started": "2025-04-02T09:25:43.350306Z",
          "shell.execute_reply": "2025-04-02T09:25:43.353021Z"
        },
        "id": "B-qGza_4ApJR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fine-tune all layers + Adapter layers** :\n",
        "\n",
        "- **you're not freezing the model but adding the adapter & fine-tuning**\n",
        "\n",
        "### **Fine-tune using Adapter layers**:\n",
        "\n",
        "- **you're not freezing the model then adding the adapter & fine-tuning**\n",
        "\n",
        "\n",
        "### **Where Are the Adapters Inserted?**\n",
        "The adapters are inserted into **each Transformer Block** inside the **DistilBERT transformer layer** at two specific locations:\n",
        "\n",
        "1. **After the self-attention output (`out_lin`)**   \n",
        "     ```python\n",
        "     model.distilbert.transformer.layer[block_idx].attention.out_lin\n",
        "     ```\n",
        "\n",
        "2. **After the feedforward network output (`lin2`)**    \n",
        "     ```python\n",
        "     model.distilbert.transformer.layer[block_idx].ffn.lin2\n",
        "     ```\n",
        "---\n",
        "\n",
        "### **Comparison of Original vs. Modified Model**\n",
        "\n",
        "| **Component**                          | **Original Model**                         | **Modified Model** (with Adapters) |\n",
        "|-----------------------------------------|--------------------------------------------|-------------------------------------|\n",
        "| **Self-Attention Output (`out_lin`)**   | `Linear(768, 768)`                         | `Linear(768, 768) → Linear(768, 32) → GELU → Linear(32, 768)` |\n",
        "| **Feedforward Network Output (`lin2`)** | `Linear(3072, 768)`                        | `Linear(3072, 768) → Linear(768, 32) → GELU → Linear(32, 768)` |\n",
        "| **Number of Adapter Layers**            | `None`                                     | **2 per block × 6 blocks = 12 adapters** |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "nrfqQo2uApJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch import nn\n",
        "# from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# class ResidualAdapter(nn.Module):\n",
        "#     \"\"\"Adapter with residual connection to prevent loss of model information\"\"\"\n",
        "#     def __init__(self, in_dim, bottleneck_dim):\n",
        "#         super().__init__()\n",
        "#         self.down_proj = nn.Linear(in_dim, bottleneck_dim)\n",
        "#         self.activation = nn.GELU()\n",
        "#         self.up_proj = nn.Linear(bottleneck_dim, in_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return x + self.up_proj(self.activation(self.down_proj(x)))  # Residual skip connection\n",
        "\n",
        "\n",
        "# def insert_adapter(transformer_layer, bottleneck_size):\n",
        "#     \"\"\" Insert adapter into a given transformer block \"\"\"\n",
        "#     adapter_1 = ResidualAdapter(in_dim=transformer_layer.attention.out_lin.out_features, bottleneck_dim=bottleneck_size)\n",
        "#     adapter_2 = ResidualAdapter(in_dim=transformer_layer.ffn.lin2.out_features, bottleneck_dim=bottleneck_size)\n",
        "\n",
        "#     transformer_layer.attention.out_lin = nn.Sequential(transformer_layer.attention.out_lin, adapter_1)\n",
        "#     transformer_layer.ffn.lin2 = nn.Sequential(transformer_layer.ffn.lin2, adapter_2)\n",
        "\n",
        "#     return adapter_1, adapter_2\n",
        "\n",
        "\n",
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# # Freeze base model parameters\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# # Add adapters to all transformer blocks\n",
        "# total_size = 0\n",
        "# bottleneck_size = 32  # Hyperparameter\n",
        "\n",
        "# for block_idx in range(6):\n",
        "#     adapter_1, adapter_2 = insert_adapter(model.distilbert.transformer.layer[block_idx], bottleneck_size)\n",
        "\n",
        "#     total_size += sum(p.numel() for p in adapter_1.parameters() if p.requires_grad)\n",
        "#     total_size += sum(p.numel() for p in adapter_2.parameters() if p.requires_grad)\n",
        "\n",
        "# print(\"Number of adapter parameters added:\", total_size)\n",
        "\n",
        "# model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-02T09:25:43.475623Z",
          "iopub.execute_input": "2025-04-02T09:25:43.475811Z",
          "iopub.status.idle": "2025-04-02T09:25:43.494343Z",
          "shell.execute_reply.started": "2025-04-02T09:25:43.475795Z",
          "shell.execute_reply": "2025-04-02T09:25:43.493599Z"
        },
        "id": "kOqbZvpUApJS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check Overfitting"
      ],
      "metadata": {
        "id": "1eL1HI6aApJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model, test_loader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-02T09:27:21.183569Z",
          "iopub.execute_input": "2025-04-02T09:27:21.183807Z",
          "iopub.status.idle": "2025-04-02T09:27:28.77596Z",
          "shell.execute_reply.started": "2025-04-02T09:27:21.183787Z",
          "shell.execute_reply": "2025-04-02T09:27:28.77514Z"
        },
        "id": "DsduabzvApJT",
        "outputId": "355a4922-0af1-4238-b707-2769ce5bc73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Test Accuracy: 0.6800\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model, train_loader)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-02T09:27:28.776798Z",
          "iopub.execute_input": "2025-04-02T09:27:28.777045Z",
          "iopub.status.idle": "2025-04-02T09:27:43.973792Z",
          "shell.execute_reply.started": "2025-04-02T09:27:28.777009Z",
          "shell.execute_reply": "2025-04-02T09:27:43.973009Z"
        },
        "id": "_Jdt74r3ApJU",
        "outputId": "f920fadd-efce-406d-cd17-65a01671709b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Test Accuracy: 0.7350\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}