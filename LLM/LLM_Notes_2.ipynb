{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "LLM-Notes-1",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "y_1A4DU9wG0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Input Embedding**\n",
        "\n",
        "```tokens``` : sentences are broken down to words or sub-words\n",
        "\n",
        "```input_ids``` : each token has an index in the ```vocabulary``` we identify any new token by these ids\n",
        "\n",
        "```embed_tokens``` : This refers to the embedding layer of the model. Each ```input_ids``` is converted into an n-dimensional dense vector (of size ```embed_dim or hidden_sz```). These vectors capture semantic relationships between tokens\n",
        "\n",
        "During training or inference, multiple sequences (sentences) are grouped into a ```batch```. Each token in a sequence is converted to its corresponding embedding vector, and the model processes these batches in parallel for efficiency.\n",
        "\n",
        "## **Positional Encoding**\n",
        "\n",
        "These encodings are vectors that keep the location of a word in the sentence & helps the model understand the order and context of words in the sentence (since it lacks the recurrence feature found in RNNs to feed the input one at a time) Currently in models like Mistral ```RotaryEmbedding``` (a type of Dynamic positional encoding) is used inside ```self_attn``` block\n",
        "\n",
        "## **Self-Attention Mechanism**\n",
        "\n",
        "It calculates a weighted sum of the embeddings of all words in a sentence for each word. These weights are determined based on some learned “attention” scores between words. The terms with higher relevance to one another will receive higher “attention” weights.\n",
        "\n",
        "**Query Vector**:\n",
        "\n",
        "  - It represents the word or token for which the attention weights are being calculated.\n",
        "  - The Query vector determines which parts of the input sequence should receive more attention.\n",
        "  - Multiplying word embeddings with the Query vector is like asking, **\"What should I pay attention to?\"**\n",
        "\n",
        "**Key Vector**:\n",
        "\n",
        "  - It represents the set of words or tokens in the input sequence that are compared with the Query.\n",
        "  - The Key vector helps identify the relevant or essential information in the input sequence.\n",
        "  - Multiplying word embeddings with the Key vector is like asking, **\"What is important to consider?\"**\n",
        "\n",
        "**Value Vector**:\n",
        "\n",
        "- It contains the input sequence's associated information or features for each word or token.\n",
        "- The Value vector provides the actual data that will be weighted and combined based on the attention weights calculated between the Query and Key.\n",
        "- The Value vector answers the question, **\"What information do we have?\"**\n",
        "\n",
        "**Attention Weights/Scores** : Calculated for each token and head.Each element represents the attention a particular head pays to a specific token when processing the input.\n",
        "\n",
        "**Attention Output** : Combine the weights with the original input. It incorporates information from relevant tokens based on the attention scores, enriching each token's representation."
      ],
      "metadata": {
        "id": "tLkgsScXwG1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparseAttention\n",
        "https://github.com/kyegomez/SparseAttention\n",
        "\n",
        "\n",
        "# FlashAttention\n",
        "\n",
        "https://github.com/kyegomez/FlashAttention20\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9K0MmYLswG0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replacing existing Attention Layer with Custom Attention Layer"
      ],
      "metadata": {
        "id": "Tx97pM8HWoJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import OPTForCausalLM, OPTConfig\n",
        "\n",
        "class SparseAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_dim, \"Embedding dimension must be divisible by num_heads\"\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        assert embed_dim == self.embed_dim, \"Input embedding dimension must match the module's embedding dimension\"\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.q_linear(x)  # (batch_size, seq_length, embed_dim)\n",
        "        k = self.k_linear(x)  # (batch_size, seq_length, embed_dim)\n",
        "        v = self.v_linear(x)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        k = k.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        v = v.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1))  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        # Apply local (sparse) attention mask\n",
        "        mask = self._create_local_attention_mask(seq_length)\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        # Compute attention output\n",
        "        attn_output = torch.matmul(attn_probs, v)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.out_linear(attn_output)  # (batch_size, seq_length, embed_dim)\n",
        "        return output\n",
        "\n",
        "    def _create_local_attention_mask(self, seq_length):\n",
        "        mask = torch.zeros(seq_length, seq_length)\n",
        "        for i in range(seq_length):\n",
        "            start = max(0, i - self.window_size // 2)\n",
        "            end = min(seq_length, i + self.window_size // 2 + 1)\n",
        "            mask[start:end, i] = 1\n",
        "        return mask.unsqueeze(0).unsqueeze(0).to(torch.bool)  # (1, 1, seq_length, seq_length)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T03:50:29.336942Z",
          "iopub.execute_input": "2024-07-25T03:50:29.337442Z",
          "iopub.status.idle": "2024-07-25T03:50:29.356851Z",
          "shell.execute_reply.started": "2024-07-25T03:50:29.337404Z",
          "shell.execute_reply": "2024-07-25T03:50:29.355193Z"
        },
        "trusted": true,
        "id": "3-qBlzzgwG1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomOPTModel(OPTForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super(CustomOPTModel, self).__init__(config)\n",
        "        self._replace_attention_layers()\n",
        "\n",
        "    def _replace_attention_layers(self):\n",
        "        for layer_name, layer_module in self.named_modules():\n",
        "            if isinstance(layer_module, nn.MultiheadAttention):\n",
        "                # Replace the self-attention layer with SparseAttention\n",
        "                setattr(self, layer_name, SparseAttention(\n",
        "                    embed_dim=layer_module.embed_dim,\n",
        "                    num_heads=layer_module.num_heads,\n",
        "                    window_size=3  # Example window size\n",
        "                ))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T03:54:14.30697Z",
          "iopub.execute_input": "2024-07-25T03:54:14.307386Z",
          "iopub.status.idle": "2024-07-25T03:54:14.316091Z",
          "shell.execute_reply.started": "2024-07-25T03:54:14.307356Z",
          "shell.execute_reply": "2024-07-25T03:54:14.314488Z"
        },
        "trusted": true,
        "id": "4c8oaskPwG1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "# Load tokenizer appropriate for your model\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
        "\n",
        "# Example input\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors='pt')\n",
        "\n",
        "# Load pre-trained model configuration and model\n",
        "config = AutoConfig.from_pretrained('facebook/opt-350m')\n",
        "model = AutoModelForCausalLM.from_pretrained('facebook/opt-350m')\n",
        "\n",
        "# Example usage\n",
        "outputs = model(**inputs)\n",
        "print(inputs.items())\n",
        "print(inputs.tokens())\n",
        "print(outputs.logits.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:08:29.385503Z",
          "iopub.execute_input": "2024-07-25T04:08:29.38595Z",
          "iopub.status.idle": "2024-07-25T04:08:30.755549Z",
          "shell.execute_reply.started": "2024-07-25T04:08:29.385916Z",
          "shell.execute_reply": "2024-07-25T04:08:30.754194Z"
        },
        "trusted": true,
        "id": "gcWgWe00wG1R",
        "outputId": "5023b038-f665-4de2-c29a-a172b2a37f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "dict_items([('input_ids', tensor([[    2, 31414,     6,   127,  2335,    16, 11962]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1]]))])\n['</s>', 'Hello', ',', 'Ġmy', 'Ġdog', 'Ġis', 'Ġcute']\ntorch.Size([1, 7, 50272])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Model Structure**: The code assumes the model has a modular structure, typically found in transformer models.\n",
        "\n",
        "**Layer Access**: Accessing and replacing layers depends on the model’s architecture. For instance, in BERT, you access encoder layers through model.encoder.layer, while in GPT or OPT, you might need to adjust according to the specific layer structure.\n",
        "\n",
        "**Modification Scope**: This method can be adapted to replace other components like feed-forward layers, normalization layers, etc.\n",
        "\n",
        "**Model-Specific Adjustments**: The exact implementation may vary based on the model. For instance, GPT models use transformer.h instead of encoder.layer"
      ],
      "metadata": {
        "id": "qRYIKRonwG1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General pattern for Layer replacement"
      ],
      "metadata": {
        "id": "Z-KKNRdiwG1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from transformers import BertModel\n",
        "\n",
        "# class CustomAttention(nn.Module):\n",
        "#     def __init__(self, embed_dim, num_heads):\n",
        "#         super(CustomAttention, self).__init__()\n",
        "#         self.multihead_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "#     def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None,\n",
        "#                 encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "#         # Use the multihead_attention module to perform the attention operation\n",
        "#         return self.multihead_attention(hidden_states, hidden_states, hidden_states,\n",
        "#                                         attn_mask=attention_mask, key_padding_mask=attention_mask)[0]\n"
      ],
      "metadata": {
        "id": "QEFTmQ2mwG1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom LayerNorm + Residual connection + Attention"
      ],
      "metadata": {
        "id": "j9dBXAF3wG1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class CustomLayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, normalized_shape, eps=1e-12):\n",
        "        super(CustomLayerNorm, self).__init__(normalized_shape, eps=eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Custom behavior, if needed, can be added here\n",
        "        return super(CustomLayerNorm, self).forward(x)\n",
        "\n",
        "\n",
        "class CustomResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout_prob=0.1):\n",
        "        super(CustomResidualConnection, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        # Custom residual connection implementation\n",
        "        return x + self.dropout(residual)\n",
        "\n",
        "\n",
        "class CustomAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(CustomAttention, self).__init__()\n",
        "        self.multihead_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.layer_norm = CustomLayerNorm(normalized_shape=embed_dim)\n",
        "        self.residual_connection = CustomResidualConnection()\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None,\n",
        "                encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        # Perform attention operation\n",
        "        attention_output, _ = self.multihead_attention(\n",
        "            hidden_states, hidden_states, hidden_states,\n",
        "            attn_mask=attention_mask, key_padding_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Apply layer normalization and residual connection\n",
        "        normalized_output = self.layer_norm(attention_output)\n",
        "        residual_output = self.residual_connection(normalized_output, hidden_states)\n",
        "\n",
        "        # Return as a tuple\n",
        "        return (residual_output,)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:37:44.586948Z",
          "iopub.execute_input": "2024-07-25T04:37:44.587425Z",
          "iopub.status.idle": "2024-07-25T04:37:44.601617Z",
          "shell.execute_reply.started": "2024-07-25T04:37:44.58739Z",
          "shell.execute_reply": "2024-07-25T04:37:44.600357Z"
        },
        "trusted": true,
        "id": "26nH-p2awG1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integration of Custom Layers"
      ],
      "metadata": {
        "id": "a9pu5X3TwG1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self._replace_attention_layers()\n",
        "\n",
        "    def _replace_attention_layers(self):\n",
        "        for layer_name, layer_module in self.named_modules():\n",
        "            if isinstance(layer_module, nn.MultiheadAttention):\n",
        "                # Replace the self-attention layer\n",
        "                embed_dim = layer_module.embed_dim\n",
        "                num_heads = layer_module.num_heads\n",
        "                setattr(self, layer_name, CustomAttention(embed_dim, num_heads))\n",
        "                print(f\"Replaced attention layer {layer_name}\")\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,\n",
        "                head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None,\n",
        "                return_dict=None):\n",
        "        # Ensure the forward method matches the input signature of BERT\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict\n",
        "        )\n",
        "        return outputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:37:51.735478Z",
          "iopub.execute_input": "2024-07-25T04:37:51.735907Z",
          "iopub.status.idle": "2024-07-25T04:37:51.749752Z",
          "shell.execute_reply.started": "2024-07-25T04:37:51.735875Z",
          "shell.execute_reply": "2024-07-25T04:37:51.748212Z"
        },
        "trusted": true,
        "id": "xQ5Xi38ywG1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model_name = 'bert-base-uncased'\n",
        "custom_model = CustomModel(model_name)\n",
        "\n",
        "# Example input (adjust according to the model's tokenizer)\n",
        "input_ids = torch.tensor([[101, 1045, 2064, 1005, 1055, 1037, 1000, 102]], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "outputs = custom_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "print(outputs.keys())\n",
        "print(outputs.last_hidden_state.shape)  # Example output shape\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:41:24.578345Z",
          "iopub.execute_input": "2024-07-25T04:41:24.578826Z",
          "iopub.status.idle": "2024-07-25T04:41:24.936887Z",
          "shell.execute_reply.started": "2024-07-25T04:41:24.578791Z",
          "shell.execute_reply": "2024-07-25T04:41:24.93544Z"
        },
        "trusted": true,
        "id": "BBxjI1XMwG1a",
        "outputId": "b4e3522b-b954-4d0a-8a59-12c964b3026b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "odict_keys(['last_hidden_state', 'pooler_output'])\ntorch.Size([1, 8, 768])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder only Architecture - BERT\n",
        "\n",
        "The model is comprised of both encoder and decoder components, with each component consisting of 12 layers. Additionally, The decoder component, in particular, contains an additional encoder_attn layer, referred to as cross-attention. The cross-attention component will condition the decoder’s output based on the encoder representations.\n",
        "\n",
        "```python\n",
        "BertModel(\n",
        "  (embeddings): BertEmbeddings(\n",
        "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
        "    (position_embeddings): Embedding(512, 768)\n",
        "    (token_type_embeddings): Embedding(2, 768)\n",
        "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "    (dropout): Dropout(p=0.1, inplace=False)\n",
        "  )\n",
        "  (encoder): BertEncoder(\n",
        "    (layer): ModuleList(\n",
        "      (0-11): 12 x BertLayer(\n",
        "        (attention): BertAttention(\n",
        "          (self): BertSelfAttention(\n",
        "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "          (output): BertSelfOutput(\n",
        "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "        )\n",
        "        (intermediate): BertIntermediate(\n",
        "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "          (intermediate_act_fn): GELUActivation()\n",
        "        )\n",
        "        (output): BertOutput(\n",
        "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "          (dropout): Dropout(p=0.1, inplace=False)\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "  )\n",
        "  (pooler): BertPooler(\n",
        "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "    (activation): Tanh()\n",
        "  )\n",
        ")\n",
        "```\n"
      ],
      "metadata": {
        "id": "7ZSwV9FHVWfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, GPT2Model\n",
        "\n",
        "# def print_trainable_parameters(model):\n",
        "#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "#     print(f\"Total Trainable Parameters: {total_params}\")\n",
        "\n",
        "gpt2 = AutoModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "def prepare_for_peft(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False  # freeze the model - train adapters later\n",
        "        if param.dim() == 1:\n",
        "            # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "            param.data = param.data.to(torch.float32)\n",
        "\n",
        "    # Commented this line because it's not a valid function for GPT2Model\n",
        "    # model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "\n",
        "    model.config.gradient_checkpointing = True  # enable gradient checkpointing\n",
        "    model.config.use_cache = False  # disable cache for memory efficiency\n",
        "    model.config.output_hidden_states = False  # set to True if you want hidden states\n",
        "    model.config.output_attentions = False  # set to True if you want attention weights\n",
        "\n",
        "    # No need to define a separate class, we can use nn.Sequential directly\n",
        "    model.lm_head = nn.Sequential(nn.Linear(model.config.hidden_size, model.config.vocab_size))\n",
        "    return model\n",
        "\n",
        "print(\"-\"*250)\n",
        "gpt2 = prepare_for_peft(gpt2)\n",
        "print(gpt2)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y5aWuZjMWBkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "GPT2Model(\n",
        "  (wte): Embedding(50257, 768)\n",
        "  (wpe): Embedding(1024, 768)\n",
        "  (drop): Dropout(p=0.1, inplace=False)\n",
        "  (h): ModuleList(\n",
        "    (0-11): 12 x GPT2Block(\n",
        "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "      (attn): GPT2Attention(\n",
        "        (c_attn): Conv1D()\n",
        "        (c_proj): Conv1D()\n",
        "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
        "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
        "      )\n",
        "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "      (mlp): GPT2MLP(\n",
        "        (c_fc): Conv1D()\n",
        "        (c_proj): Conv1D()\n",
        "        (act): NewGELUActivation()\n",
        "        (dropout): Dropout(p=0.1, inplace=False)\n",
        "      )\n",
        "    )\n",
        "  )\n",
        "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        ")\n",
        "trainable params: 124439808 || all params: 124439808 || trainable%: 100.0\n",
        "\n",
        "now after preparing for peft a lm_head is added with above model & only part of it is trained\n",
        "\n",
        "GPT2Model(\n",
        "  (lm_head): Sequential(\n",
        "    (0): Linear(in_features=768, out_features=50257, bias=True)\n",
        "  )\n",
        ")\n",
        "trainable params: 38647633 || all params: 163087441 || trainable%: 23.697491825872724\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "dHf2BwkDXaT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local Attention & Global Attention\n",
        "\n",
        "https://github.com/lucidrains/local-attention\n",
        "\n",
        "\n",
        "### Local vs Global Attention\n",
        "\n",
        "**Local Attention** focuses on a subset of the input sequence within a fixed-size window, offering computational efficiency by reducing the scope of attention. It’s useful in tasks like language modeling and image processing, where nearby context is more relevant.\n",
        "\n",
        "**Global Attention** considers all positions of the input, providing a comprehensive view of the entire sequence. It’s more resource-intensive but excels in capturing long-range dependencies, such as in machine translation or image classification.\n",
        "\n",
        "### Comparison:\n",
        "- **Scope**: Local attention focuses on a neighborhood; global attends to all positions.\n",
        "- **Efficiency**: Local is computationally efficient, global is more costly but captures broader context.\n",
        "- **Use**: Local is for tasks with local dependencies, while global is for tasks needing long-range context.\n",
        "\n",
        "### Example:\n",
        "- **LocalAttention Class**: Implements a custom attention layer using a local window mask.\n",
        "- **CustomTransformer Class**: Combines local and global attention mechanisms with a feedforward network.\n",
        "\n",
        "This framework demonstrates how local and global attention can be integrated into a transformer model for various applications.\n"
      ],
      "metadata": {
        "id": "xCrvB5pIwG1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Custom Local Attention Layer\n",
        "class LocalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, window_size=5):\n",
        "        super(LocalAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.window_size = window_size\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "#         attention_scores = torch.matmul(q, k.transpose(-1, -2)) / (self.embed_dim ** 0.5)\n",
        "        print(attention_scores.shape)\n",
        "        # Apply local window mask\n",
        "        mask = torch.zeros_like(attention_scores)\n",
        "        mask[:, :, :self.window_size] = -float('inf')\n",
        "        print(mask.shape)\n",
        "        attention_scores = attention_scores + mask\n",
        "\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        context = torch.matmul(attention_weights, v)\n",
        "\n",
        "        return context\n",
        "\n",
        "# Custom Transformer Model using both Local and Global Attention\n",
        "class CustomTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size=5):\n",
        "        super(CustomTransformer, self).__init__()\n",
        "        self.local_attention = LocalAttention(embed_dim, window_size)\n",
        "        self.global_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Local Attention\n",
        "        local_context = self.local_attention(x)\n",
        "\n",
        "        # Global Attention\n",
        "        global_context, _ = self.global_attention(x, x, x)\n",
        "\n",
        "        # Combine local and global contexts\n",
        "        combined_context = local_context + global_context\n",
        "\n",
        "        # Feedforward layer\n",
        "        output = self.feedforward(combined_context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    embed_dim = 16\n",
        "    num_heads = 4\n",
        "    window_size = 5\n",
        "    batch_size = 8\n",
        "    seq_len = 10\n",
        "\n",
        "    # Create an instance of the custom transformer model\n",
        "    model = CustomTransformer(embed_dim, num_heads, window_size)\n",
        "\n",
        "    # Generate a dummy input tensor\n",
        "    input_tensor = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output_tensor = model(input_tensor)\n",
        "\n",
        "    print(\"Input Tensor Shape:\", input_tensor.shape)\n",
        "    print(\"Output Tensor Shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T05:50:11.53156Z",
          "iopub.execute_input": "2024-07-25T05:50:11.532724Z",
          "iopub.status.idle": "2024-07-25T05:50:11.551228Z",
          "shell.execute_reply.started": "2024-07-25T05:50:11.532683Z",
          "shell.execute_reply": "2024-07-25T05:50:11.549956Z"
        },
        "trusted": true,
        "id": "Z2egUv_9wG1d",
        "outputId": "70dd343c-7c34-4a14-9ce1-2651bcc58420"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([8, 10, 10])\ntorch.Size([8, 10, 10])\nInput Tensor Shape: torch.Size([8, 10, 16])\nOutput Tensor Shape: torch.Size([8, 10, 16])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "aZ8YcFiwwG1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# class SparseAttention(nn.Module):\n",
        "#     def __init__(self, input_dim, embed_dim, num_heads, window):\n",
        "#         super(SparseAttention, self).__init__()\n",
        "#         self.head_dim = embed_dim // num_heads\n",
        "#         self.num_heads = num_heads\n",
        "#         self.window = window\n",
        "#         self.q_proj = nn.Linear(input_dim, embed_dim)\n",
        "#         self.k_proj = nn.Linear(input_dim, embed_dim)\n",
        "#         self.v_proj = nn.Linear(input_dim, embed_dim)\n",
        "#         self.output_proj = nn.Linear(embed_dim, input_dim)\n",
        "\n",
        "#     def _create_local_attention_mask(self, seq):\n",
        "#         mask = torch.zeros(seq, seq)  # [seq, seq]\n",
        "#         for i in range(seq):\n",
        "#             start = max(0, i - self.window // 2)\n",
        "#             end = min(seq, i + self.window // 2 + 1)\n",
        "#             mask[start:end, i] = 1\n",
        "#         mask = mask.unsqueeze(0).unsqueeze(0).to(torch.bool)  # [1, 1, seq, seq]\n",
        "#         return mask\n",
        "\n",
        "#     def forward(self, x, local_attn=False):\n",
        "#         batch, seq, embed_dim = x.size()\n",
        "#         q = self.q_proj(x)\n",
        "#         k = self.k_proj(x)\n",
        "#         v = self.v_proj(x)\n",
        "\n",
        "#         q = q.view(batch, seq, self.num_heads, self.head_dim).permute(0, 2, 1, 3) # [batch,n_heads,seq,head_dim]\n",
        "#         k = k.view(batch, seq, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "#         v = v.view(batch, seq, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "#         attn_scores = torch.matmul(q, k.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5) # [batch,n_heads,q_seq,k_seq]\n",
        "\n",
        "#         if local_attn:\n",
        "#             mask = self._create_local_attention_mask(seq)\n",
        "#             attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "#         attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "#         attn_output = torch.matmul(attn_weights, v) # [batch,n_heads,q_seq,head_dim]\n",
        "\n",
        "#         attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch, seq, -1) # [batch,q_seq,input_dim]\n",
        "#         attn_output = self.output_proj(attn_output)\n",
        "\n",
        "#         return attn_output, attn_weights\n",
        "\n",
        "class SparseAttention(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads, window):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.window = window\n",
        "        self.q_proj = nn.Linear(input_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(input_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(input_dim, embed_dim)\n",
        "        self.output_proj = nn.Linear(embed_dim, input_dim)\n",
        "\n",
        "    def _create_local_attention_mask(self, seq):\n",
        "        mask = torch.zeros(seq, seq, dtype=torch.bool)  # [seq, seq]\n",
        "        for i in range(seq):\n",
        "            start = max(0, i - self.window // 2)\n",
        "            end = min(seq, i + self.window // 2 + 1)\n",
        "            mask[start:end, i] = 1\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, seq]\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x, local_attn=False):\n",
        "        batch_size, seq_len, embed_dim = x.size() # [batch,seq_len,head_dim * num_heads]\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # Reshape and transpose for multi-head attention\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
        "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
        "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [batch, num_heads, seq_len, seq_len]\n",
        "\n",
        "        if local_attn:\n",
        "            mask = self._create_local_attention_mask(seq_len)\n",
        "            attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, v)  # [batch, num_heads, seq_len, head_dim]\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)  # [batch, seq_len, embed_dim]\n",
        "        attn_output = self.output_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = AutoModelForCausalLM.from_pretrained('facebook/opt-350m')\n",
        "        self._replace_attention_layer() # can add more custom layers like this\n",
        "\n",
        "    def _replace_attention_layer(self):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.MultiheadAttention):\n",
        "                # Extract parameters from the existing module\n",
        "                input_dim = module.embed_dim\n",
        "                num_heads = module.num_heads\n",
        "                # Create and replace the attention layer\n",
        "                new_attention_layer = SparseAttention(\n",
        "                    input_dim=input_dim,\n",
        "                    embed_dim=input_dim,\n",
        "                    num_heads=num_heads,\n",
        "                    window=4,  # Example window size\n",
        "\n",
        "                )\n",
        "                setattr(self.model, name, new_attention_layer)\n",
        "\n",
        "    def forward(self,*args, local_attn=False, **kwargs):\n",
        "        return self.model(*args,**kwargs)\n",
        "\n",
        "\n",
        "# Tokenization and Model Execution\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
        "inputs = tokenizer(\"If you modify the model\",return_tensors='pt')\n",
        "inputs.items()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:35:01.232525Z",
          "iopub.execute_input": "2024-08-29T04:35:01.233036Z",
          "iopub.status.idle": "2024-08-29T04:35:01.599872Z",
          "shell.execute_reply.started": "2024-08-29T04:35:01.233Z",
          "shell.execute_reply": "2024-08-29T04:35:01.598434Z"
        },
        "trusted": true,
        "id": "eR4QYf7PwG1f",
        "outputId": "1d9c0cad-6a8f-4175-e412-2a52084e194c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "dict_items([('input_ids', tensor([[    2,  1106,    47, 23209,     5,  1421]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1]]))])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input_ids\"][0].shape[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:22:37.692491Z",
          "iopub.execute_input": "2024-08-29T04:22:37.692939Z",
          "iopub.status.idle": "2024-08-29T04:22:37.701772Z",
          "shell.execute_reply.started": "2024-08-29T04:22:37.692905Z",
          "shell.execute_reply": "2024-08-29T04:22:37.700202Z"
        },
        "trusted": true,
        "id": "s87rOyMvwG1g",
        "outputId": "531cff9e-f10a-4880-b511-c60039dde016"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "6"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_local_attention_mask(seq,window = 3):\n",
        "        mask = torch.zeros(seq, seq, dtype=torch.bool)  # [seq, seq]\n",
        "        for i in range(seq):\n",
        "            start = max(0, i - window // 2)\n",
        "            end = min(seq, i + window // 2 + 1)\n",
        "            mask[start:end, i] = 1\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, seq]\n",
        "        return mask\n",
        "\n",
        "window = 3\n",
        "seq = inputs[\"input_ids\"][0].shape[0]\n",
        "mask = create_local_attention_mask(seq)\n",
        "print(seq)\n",
        "print(mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:35:08.151978Z",
          "iopub.execute_input": "2024-08-29T04:35:08.152575Z",
          "iopub.status.idle": "2024-08-29T04:35:08.16488Z",
          "shell.execute_reply.started": "2024-08-29T04:35:08.152533Z",
          "shell.execute_reply": "2024-08-29T04:35:08.163512Z"
        },
        "trusted": true,
        "id": "dl_k11EtwG1h",
        "outputId": "6909c135-1ac6-4d61-8c77-ce8bd5dd9393"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "6\ntensor([[[[ True,  True, False, False, False, False],\n          [ True,  True,  True, False, False, False],\n          [False,  True,  True,  True, False, False],\n          [False, False,  True,  True,  True, False],\n          [False, False, False,  True,  True,  True],\n          [False, False, False, False,  True,  True]]]])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Word Prediction"
      ],
      "metadata": {
        "id": "B5OyU-F5wG1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel()\n",
        "outputs = model(**inputs, local_attn=True)\n",
        "\n",
        "\n",
        "logits = outputs.logits  # Raw logits (scores for each token in the vocabulary)\n",
        "hidden_states = outputs.hidden_states  # Optional, if return_dict=True and output_hidden_states=True\n",
        "attention_weights = outputs.attentions  # Optional, if return_dict=True and output_attentions=True\n",
        "print(\"Logits shape:\", logits.shape) # [batch_size, seq_length, vocab_size]\n",
        "predicted_token_ids = torch.argmax(logits, dim=-1)  # [batch, seq]\n",
        "decoded_output = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Decoded output:\", decoded_output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:35:17.961667Z",
          "iopub.execute_input": "2024-08-29T04:35:17.962099Z",
          "iopub.status.idle": "2024-08-29T04:35:19.229974Z",
          "shell.execute_reply.started": "2024-08-29T04:35:17.962065Z",
          "shell.execute_reply": "2024-08-29T04:35:19.228617Z"
        },
        "trusted": true,
        "id": "nuMNXwq8wG1i",
        "outputId": "15770b4a-8a45-42da-9f52-5bb37d2d32d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Logits shape: torch.Size([1, 6, 50272])\nDecoded output: [\"\\n you're the game,\"]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained('facebook/opt-350m')\n",
        "\n",
        "# Example usage\n",
        "base_outputs = base_model(**inputs)\n",
        "logits_base_model = base_outputs.logits\n",
        "pred_token_id_base_model = torch.argmax(logits_base_model ,dim=-1)\n",
        "decoded_output_base_model = tokenizer.batch_decode(pred_token_id_base_model, skip_special_tokens=True)\n",
        "print(\"Logits shape(Base model):\", logits_base_model .shape)\n",
        "print(\"Decoded output(Base model):\", decoded_output_base_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:35:24.079375Z",
          "iopub.execute_input": "2024-08-29T04:35:24.080339Z",
          "iopub.status.idle": "2024-08-29T04:35:25.812781Z",
          "shell.execute_reply.started": "2024-08-29T04:35:24.080299Z",
          "shell.execute_reply": "2024-08-29T04:35:25.811028Z"
        },
        "trusted": true,
        "id": "zoHu-GT2wG1i",
        "outputId": "7c27794d-b7e3-4c34-c2ca-80c2ce24ba1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Logits shape(Base model): torch.Size([1, 6, 50272])\nDecoded output(Base model): [\"\\n you're the game,\"]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-16T08:33:15.437491Z",
          "iopub.execute_input": "2024-08-16T08:33:15.438827Z",
          "iopub.status.idle": "2024-08-16T08:33:15.443546Z",
          "shell.execute_reply.started": "2024-08-16T08:33:15.438789Z",
          "shell.execute_reply": "2024-08-16T08:33:15.442393Z"
        },
        "trusted": true,
        "id": "9jP_kD8ywG1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}