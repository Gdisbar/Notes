{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YiWGrmLoYBL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🚀 Custom Transformer Components"
      ],
      "metadata": {
        "id": "cA9s1n5moYdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention Variants\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Symbols & Definitions\n",
        "\n",
        "* **`d_model`** → embedding size of the model (here 256).\n",
        "\n",
        "* **`num_heads`** → number of **query heads** (here 8).\n",
        "\n",
        "* **`d_head`** → dimension of each query head:\n",
        "\n",
        "  $$\n",
        "  d_{head} = \\frac{d_{model}}{num\\_heads}\n",
        "  $$\n",
        "\n",
        "  → In your case: $256 / 8 = 32$.\n",
        "\n",
        "* **`num_kv_heads`** → number of **key/value heads**.\n",
        "\n",
        "  * Vanilla MHA: $num\\_kv\\_heads = num\\_heads$.\n",
        "  * GQA: $num\\_kv\\_heads < num\\_heads$.\n",
        "  * MQA: $num\\_kv\\_heads = 1$.\n",
        "\n",
        "* **`d_kv`** → the **dimension of all K/V projections combined**.\n",
        "\n",
        "  $$\n",
        "  d_{kv} = d_{head} \\times num\\_kv\\_heads\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. What each part means\n",
        "\n",
        "* **`d_head` (32 in your case):**\n",
        "  The “width” of a single attention head. Each query head looks at tokens through a subspace of size 32.\n",
        "\n",
        "* **`num_kv_heads` (e.g. 1, 2, or 8):**\n",
        "  How many *distinct* sets of K/V vectors are learned.\n",
        "\n",
        "  * If 8 → every query head has its own K/V (MHA).\n",
        "  * If 2 → groups of query heads share K/V (GQA).\n",
        "  * If 1 → all queries share one K/V (MQA).\n",
        "\n",
        "* **Multiplication `d_head × num_kv_heads`:**\n",
        "  This gives the **total dimensionality of the projected K and V matrices**.\n",
        "\n",
        "  * That’s why `w_k` and `w_v` have shapes `[d_kv, d_model]`.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Intuition of the multiplication\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "* Each KV head = a \"view\" into the sequence, of size `d_head`.\n",
        "* If you have multiple KV heads, you stack them side by side.\n",
        "* So the total space you need to project into is just:\n",
        "\n",
        "  $$\n",
        "  \\text{total KV dimension} = d_{head} \\times num\\_kv\\_heads\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Concrete Examples (your config)\n",
        "\n",
        "* **MHA (8 KV heads):**\n",
        "  $d_{kv} = 32 \\times 8 = 256$\n",
        "  → K/V projection: `[256, 256]`\n",
        "\n",
        "* **GQA (2 KV heads):**\n",
        "  $d_{kv} = 32 \\times 2 = 64$\n",
        "  → K/V projection: `[64, 256]`\n",
        "\n",
        "* **MQA (1 KV head):**\n",
        "  $d_{kv} = 32 \\times 1 = 32$\n",
        "  → K/V projection: `[32, 256]`\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Summary**:\n",
        "\n",
        "* `d_head` → how wide each head is.\n",
        "* `num_kv_heads` → how many *independent* K/V sets you maintain.\n",
        "* `d_head × num_kv_heads` → total dimensionality of the K/V projection layer, i.e. how many numbers represent your keys/values per token.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Eyk16yF7oeFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_example():\n",
        "    \"\"\"Create a simple tokenized text example for testing\"\"\"\n",
        "    # Simple vocabulary mapping for demonstration\n",
        "    vocab = {\n",
        "        '<pad>': 0, '<sos>': 1, '<eos>': 2,\n",
        "        'the': 3, 'cat': 4, 'sat': 5, 'on': 6, 'mat': 7,\n",
        "        'dog': 8, 'ran': 9, 'fast': 10, 'bird': 11, 'flew': 12, 'high': 13\n",
        "    }\n",
        "\n",
        "    # Example sentences (tokenized)\n",
        "    sentences = [\n",
        "        \"the cat sat on the mat\",\n",
        "        \"the dog ran fast\",\n",
        "        \"the bird flew high\"\n",
        "    ]\n",
        "\n",
        "    # Convert to token IDs\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tokens = [vocab['<sos>']]  # Start token\n",
        "        tokens.extend([vocab.get(word, 0) for word in sentence.split()])\n",
        "        tokens.append(vocab['<eos>'])  # End token\n",
        "        tokenized_sentences.append(tokens)\n",
        "\n",
        "    return tokenized_sentences, vocab\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, max_len=None, pad_value=0):\n",
        "    \"\"\"Pad sequences to the same length\"\"\"\n",
        "    if max_len is None:\n",
        "        max_len = max(len(seq) for seq in sequences)\n",
        "\n",
        "    padded = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) < max_len:\n",
        "            padded.append(seq + [pad_value] * (max_len - len(seq)))\n",
        "        else:\n",
        "            padded.append(seq[:max_len])\n",
        "\n",
        "    return padded"
      ],
      "metadata": {
        "id": "PmAKAH3jqgdM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences, vocab = create_text_example()\n",
        "tokenized_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ne0bayPqrmk",
        "outputId": "444c5028-61ce-41fe-9d97-0f836842ae33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 3, 4, 5, 6, 3, 7, 2], [1, 3, 8, 9, 10, 2], [1, 3, 11, 12, 13, 2]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_padded_sequences = pad_sequences(tokenized_sentences)\n",
        "test_padded_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_xY62n3q6Ns",
        "outputId": "ac7b4a51-1ea0-4b0f-fabe-3e667d7efd59"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 3, 4, 5, 6, 3, 7, 2],\n",
              " [1, 3, 8, 9, 10, 2, 0, 0],\n",
              " [1, 3, 11, 12, 13, 2, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "test_input_ids = torch.tensor(test_padded_sequences)\n",
        "test_input_ids.shape,test_input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxB0g-L_uH8A",
        "outputId": "aea3afd3-83d1-4a45-abc8-f45683369e84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 8]),\n",
              " tensor([[ 1,  3,  4,  5,  6,  3,  7,  2],\n",
              "         [ 1,  3,  8,  9, 10,  2,  0,  0],\n",
              "         [ 1,  3, 11, 12, 13,  2,  0,  0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Query Attention (MQA) - Memory efficient"
      ],
      "metadata": {
        "id": "eZFs9U4OsPjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Multi-Query Attention (MQA) - Memory efficient\n",
        "class MultiQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)  # Multiple query heads\n",
        "        self.w_k = nn.Linear(d_model, self.d_k)  # Single key head\n",
        "        self.w_v = nn.Linear(d_model, self.d_k)  # Single value head\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Q: [B, T, d_model] -> [B, num_heads, T, d_k]\n",
        "        q = self.w_q(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        # K, V: [B, T, C] -> [B, 1, T, d_k] -> broadcast to [B, num_heads, T, d_k]\n",
        "        k = self.w_k(x).view(B, T, 1, self.d_k).transpose(1, 2).expand(-1, self.num_heads, -1, -1)\n",
        "        v = self.w_v(x).view(B, T, 1, self.d_k).transpose(1, 2).expand(-1, self.num_heads, -1, -1)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        out = torch.matmul(attn_weights, v)  # [B, num_heads, T, d_k]\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.w_o(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "yPHLxVnXogrz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Method  | num\\_heads | num\\_kv\\_heads | Total Params |\n",
        "| ------- | ---------- | -------------- | ------------ |\n",
        "| **MHA** | 8          | 8              | **262,400**  |\n",
        "| **GQA** | 8          | 2              | **164,480**  |\n",
        "| **MQA** | 8          | 1              | **148,032**  |\n",
        "\n",
        "**MHA (full)**: Heaviest — every query head has its own KV projections.\n",
        "\n",
        "**GQA (middle ground)**: Balance — queries are many, but KVs are grouped. (LLaMA-2/3 style)\n",
        "\n",
        "**MQA (extreme case)**: Lightest — one shared KV for all query heads (PaLM, Gemini, Whisper use this).\n",
        "\n",
        "    w_k or w_v [d_kv, d_model] = [32, 256]\n",
        "\n",
        "Because you’re using MQA with num_heads=8 but only num_kv_heads=1, the keys/values don’t need a full 256-d projection. Instead:\n",
        "\n",
        "    Each KV head gets dimension: d_model/num_heads = 265/8 = 32\n",
        "    With 1 KV heads (num_kv_heads=1) = 32 * 2 = 32\n",
        "\n"
      ],
      "metadata": {
        "id": "CZi_Y459KjNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_multi_query_attention():\n",
        "    \"\"\"Test the MultiQueryAttention module with text examples\"\"\"\n",
        "    print(\"Testing Multi-Query Attention with Text Examples\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create text examples\n",
        "    tokenized_sentences, vocab = create_text_example()\n",
        "    print(\"Original sentences:\")\n",
        "    sentences = [\"the cat sat on the mat\", \"the dog ran fast\", \"the bird flew high\"]\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"  {i+1}. '{sentence}' -> {tokenized_sentences[i]}\")\n",
        "\n",
        "    # Pad sequences\n",
        "    max_seq_len = 8\n",
        "    padded_sequences = pad_sequences(tokenized_sentences, max_seq_len)\n",
        "    input_ids = torch.tensor(padded_sequences)\n",
        "\n",
        "    print(f\"\\nPadded input tensor shape: {input_ids.shape}\")\n",
        "    print(\"Padded sequences:\")\n",
        "    for i, seq in enumerate(padded_sequences):\n",
        "        print(f\"  Sequence {i+1}: {seq}\")\n",
        "\n",
        "    # Model configuration\n",
        "    d_model = 256\n",
        "    num_heads = 8\n",
        "    vocab_size = len(vocab)\n",
        "    batch_size, seq_length = input_ids.shape\n",
        "\n",
        "    print(f\"\\nModel Configuration:\")\n",
        "    print(f\"  d_model: {d_model}\")\n",
        "    print(f\"  num_heads: {num_heads}\")\n",
        "    print(f\"  vocab_size: {vocab_size}\")\n",
        "    print(f\"  sequence_length: {seq_length}\")\n",
        "\n",
        "    # Create embedding layer and MQA\n",
        "    embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "    mqa = MultiQueryAttention(d_model, num_heads, dropout=0.1)\n",
        "\n",
        "    # Convert tokens to embeddings\n",
        "    x = embedding(input_ids)  # [batch_size, seq_length, d_model]\n",
        "    print(f\"\\nEmbedding output shape [batch_size, seq_length, d_model]: {x.shape}\")\n",
        "\n",
        "    # Create attention mask (ignore padding tokens)\n",
        "    attention_mask = (input_ids != 0).float()\n",
        "    causal_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "    causal_mask = causal_mask.expand(-1, num_heads, seq_length, -1)\n",
        "\n",
        "    print(f\"\\nAttention mask shape [batch_size, seq_length]: {attention_mask.shape}\")\n",
        "    print(f\"\\nAttention mask shape [batch_size, seq_length, seq_length,seq_length]: {causal_mask.shape}\")\n",
        "    print(\"Attention mask for first sequence (first head):\")\n",
        "    print(causal_mask[0, 0].int())\n",
        "\n",
        "    # Forward pass through GQA\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"Running MQA Forward Pass...\")\n",
        "    mqa.eval()\n",
        "    with torch.no_grad():\n",
        "        output = mqa(x, mask=causal_mask)\n",
        "\n",
        "    print(f\"\\nGQA output shape: {output.shape}\")\n",
        "    print(\"Output statistics:\")\n",
        "    print(f\"  Mean: {output.mean():.4f}\")\n",
        "    print(f\"  Std: {output.std():.4f}\")\n",
        "    print(f\"  Min: {output.min():.4f}\")\n",
        "    print(f\"  Max: {output.max():.4f}\")\n",
        "\n",
        "    # Show parameter breakdown\n",
        "    print(f\"\\nModel Parameter Analysis:\")\n",
        "    total_params = sum(p.numel() for p in mqa.parameters())\n",
        "    print(f\"  Total GQA parameters: {total_params:,}\")\n",
        "\n",
        "    print(\"\\n  Parameter breakdown:\")\n",
        "    for name, param in mqa.named_parameters():\n",
        "        print(f\"    {name}: {param.shape} -> {param.numel():,} parameters\")\n",
        "\n",
        "    print(\"\\nTest completed successfully!\")"
      ],
      "metadata": {
        "id": "96iznZOMp0mU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_multi_query_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fCZYThEp4mM",
        "outputId": "68f71fc5-92a8-4d9b-dbb0-8cfa36e2cf21"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Multi-Query Attention with Text Examples\n",
            "============================================================\n",
            "Original sentences:\n",
            "  1. 'the cat sat on the mat' -> [1, 3, 4, 5, 6, 3, 7, 2]\n",
            "  2. 'the dog ran fast' -> [1, 3, 8, 9, 10, 2]\n",
            "  3. 'the bird flew high' -> [1, 3, 11, 12, 13, 2]\n",
            "\n",
            "Padded input tensor shape: torch.Size([3, 8])\n",
            "Padded sequences:\n",
            "  Sequence 1: [1, 3, 4, 5, 6, 3, 7, 2]\n",
            "  Sequence 2: [1, 3, 8, 9, 10, 2, 0, 0]\n",
            "  Sequence 3: [1, 3, 11, 12, 13, 2, 0, 0]\n",
            "\n",
            "Model Configuration:\n",
            "  d_model: 256\n",
            "  num_heads: 8\n",
            "  vocab_size: 14\n",
            "  sequence_length: 8\n",
            "\n",
            "Embedding output shape [batch_size, seq_length, d_model]: torch.Size([3, 8, 256])\n",
            "\n",
            "Attention mask shape [batch_size, seq_length]: torch.Size([3, 8])\n",
            "\n",
            "Attention mask shape [batch_size, seq_length, seq_length,seq_length]: torch.Size([3, 8, 8, 8])\n",
            "Attention mask for first sequence (first head):\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n",
            "\n",
            "----------------------------------------\n",
            "Running MQA Forward Pass...\n",
            "\n",
            "GQA output shape: torch.Size([3, 8, 256])\n",
            "Output statistics:\n",
            "  Mean: -0.0046\n",
            "  Std: 0.1433\n",
            "  Min: -0.5029\n",
            "  Max: 0.7012\n",
            "\n",
            "Model Parameter Analysis:\n",
            "  Total GQA parameters: 148,032\n",
            "\n",
            "  Parameter breakdown:\n",
            "    w_q.weight: torch.Size([256, 256]) -> 65,536 parameters\n",
            "    w_q.bias: torch.Size([256]) -> 256 parameters\n",
            "    w_k.weight: torch.Size([32, 256]) -> 8,192 parameters\n",
            "    w_k.bias: torch.Size([32]) -> 32 parameters\n",
            "    w_v.weight: torch.Size([32, 256]) -> 8,192 parameters\n",
            "    w_v.bias: torch.Size([32]) -> 32 parameters\n",
            "    w_o.weight: torch.Size([256, 256]) -> 65,536 parameters\n",
            "    w_o.bias: torch.Size([256]) -> 256 parameters\n",
            "\n",
            "Test completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grouped Query Attention (GQA) - Balanced approach"
      ],
      "metadata": {
        "id": "yEy4c3bKsI5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grouped Query Attention (GQA) - Balanced approach\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_kv_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert num_heads % num_kv_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.groups = num_heads // num_kv_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, num_kv_heads * self.d_k)\n",
        "        self.w_v = nn.Linear(d_model, num_kv_heads * self.d_k)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Q: [B, T, d_model] -> [B, num_heads, T, d_k]\n",
        "        q = self.w_q(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        # K, V: [B, T, C] -> [B, num_kv_heads, T, d_k]\n",
        "        k = self.w_k(x).view(B, T, self.num_kv_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(x).view(B, T, self.num_kv_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Repeat K, V for each group\n",
        "        k = k.repeat_interleave(self.groups, dim=1)  # [B, num_heads, T, d_k]\n",
        "        v = v.repeat_interleave(self.groups, dim=1)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        out = torch.matmul(attn_weights, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.w_o(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "avRLh9Txo65k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    w_q [d_model, d_model] = [256, 256]\n",
        "This projects the input embedding of size 256 into queries of the same size (since you have 8 query heads).\n",
        "\n",
        "    w_k or w_v [d_kv, d_model] = [64, 256]\n",
        "\n",
        "Because you’re using GQA with num_heads=8 but only num_kv_heads=2, the keys/values don’t need a full 256-d projection. Instead:\n",
        "\n",
        "    Each KV head gets dimension: d_model/num_heads = 265/8 = 32\n",
        "    With 2 KV heads (num_kv_heads=2) = 32 * 2 = 64\n",
        "\n",
        "After attention scores are applied, you project the concatenated heads back to the model dimension.\n",
        "\n",
        "    w_o [d_model, d_model] = [256, 256]\n",
        "\n",
        "**Why queries are bigger (256-d) but keys/values are smaller (64-d)?**\n",
        "\n",
        "  → Because in Grouped-Query Attention (GQA), you have more query heads (8) than key/value heads (2). This reduces memory/computation since multiple query heads can share the same smaller key/value projections."
      ],
      "metadata": {
        "id": "E2krevd7Il3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_group_query_attention():\n",
        "    \"\"\"Test the GroupQueryAttention module with text examples\"\"\"\n",
        "    print(\"Testing Group-Query Attention with Text Examples\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create text examples\n",
        "    tokenized_sentences, vocab = create_text_example()\n",
        "    print(\"Original sentences:\")\n",
        "    sentences = [\"the cat sat on the mat\", \"the dog ran fast\", \"the bird flew high\"]\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"  {i+1}. '{sentence}' -> {tokenized_sentences[i]}\")\n",
        "\n",
        "    # Pad sequences\n",
        "    max_seq_len = 8\n",
        "    padded_sequences = pad_sequences(tokenized_sentences, max_seq_len)\n",
        "    input_ids = torch.tensor(padded_sequences)\n",
        "\n",
        "    print(f\"\\nPadded input tensor shape: {input_ids.shape}\")\n",
        "    print(\"Padded sequences:\")\n",
        "    for i, seq in enumerate(padded_sequences):\n",
        "        print(f\"  Sequence {i+1}: {seq}\")\n",
        "\n",
        "    # Model configuration\n",
        "    d_model = 256\n",
        "    num_heads = 8\n",
        "    num_kv_heads = 2   # Set for GQA (for example, 2 kv heads for 8 query heads)\n",
        "    vocab_size = len(vocab)\n",
        "    batch_size, seq_length = input_ids.shape\n",
        "\n",
        "    print(f\"\\nModel Configuration:\")\n",
        "    print(f\"  d_model: {d_model}\")\n",
        "    print(f\"  num_heads: {num_heads}\")\n",
        "    print(f\"  num_kv_heads: {num_kv_heads}\")\n",
        "    print(f\"  vocab_size: {vocab_size}\")\n",
        "    print(f\"  sequence_length: {seq_length}\")\n",
        "\n",
        "    # Create embedding layer and GroupedQueryAttention\n",
        "    embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "    gqa = GroupedQueryAttention(d_model, num_heads, num_kv_heads, dropout=0.1)\n",
        "\n",
        "    # Convert tokens to embeddings\n",
        "    x = embedding(input_ids)  # [batch_size, seq_length, d_model]\n",
        "    print(f\"\\nEmbedding output shape: {x.shape}\")\n",
        "\n",
        "    # Create attention mask (ignore padding tokens)\n",
        "    attention_mask = (input_ids != 0).float()\n",
        "    causal_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "    causal_mask = causal_mask.expand(-1, num_heads, seq_length, -1)\n",
        "\n",
        "    print(f\"\\nAttention mask shape [batch_size, seq_length]: {attention_mask.shape}\")\n",
        "    print(f\"\\nAttention mask shape [batch_size, seq_length, seq_length,seq_length]: {causal_mask.shape}\")\n",
        "    print(\"Attention mask for first sequence (first head):\")\n",
        "    print(causal_mask[0, 0].int())\n",
        "\n",
        "    # Forward pass through GQA\n",
        "    print(\"\\n\" + \"-\" * 40)\n",
        "    print(\"Running GQA Forward Pass...\")\n",
        "    gqa.eval()\n",
        "    with torch.no_grad():\n",
        "        output = gqa(x, mask=causal_mask)\n",
        "\n",
        "    print(f\"\\nGQA output shape: {output.shape}\")\n",
        "    print(\"Output statistics:\")\n",
        "    print(f\"  Mean: {output.mean():.4f}\")\n",
        "    print(f\"  Std: {output.std():.4f}\")\n",
        "    print(f\"  Min: {output.min():.4f}\")\n",
        "    print(f\"  Max: {output.max():.4f}\")\n",
        "\n",
        "    # Show parameter breakdown\n",
        "    print(f\"\\nModel Parameter Analysis:\")\n",
        "    total_params = sum(p.numel() for p in gqa.parameters())\n",
        "    print(f\"  Total GQA parameters: {total_params:,}\")\n",
        "\n",
        "    print(\"\\n  Parameter breakdown:\")\n",
        "    for name, param in gqa.named_parameters():\n",
        "        print(f\"    {name}: {param.shape} -> {param.numel():,} parameters\")\n",
        "\n",
        "    print(\"\\nTest completed successfully!\")"
      ],
      "metadata": {
        "id": "5BJS2sKCqQW8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_group_query_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RwKAVQiw0gj",
        "outputId": "89491a87-a366-46db-c5e0-415d7fde07f8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Group-Query Attention with Text Examples\n",
            "============================================================\n",
            "Original sentences:\n",
            "  1. 'the cat sat on the mat' -> [1, 3, 4, 5, 6, 3, 7, 2]\n",
            "  2. 'the dog ran fast' -> [1, 3, 8, 9, 10, 2]\n",
            "  3. 'the bird flew high' -> [1, 3, 11, 12, 13, 2]\n",
            "\n",
            "Padded input tensor shape: torch.Size([3, 8])\n",
            "Padded sequences:\n",
            "  Sequence 1: [1, 3, 4, 5, 6, 3, 7, 2]\n",
            "  Sequence 2: [1, 3, 8, 9, 10, 2, 0, 0]\n",
            "  Sequence 3: [1, 3, 11, 12, 13, 2, 0, 0]\n",
            "\n",
            "Model Configuration:\n",
            "  d_model: 256\n",
            "  num_heads: 8\n",
            "  num_kv_heads: 2\n",
            "  vocab_size: 14\n",
            "  sequence_length: 8\n",
            "\n",
            "Embedding output shape: torch.Size([3, 8, 256])\n",
            "\n",
            "Attention mask shape [batch_size, seq_length, seq_length]: torch.Size([3, 8])\n",
            "\n",
            "Attention mask shape [batch_size, seq_length, seq_length,seq_length]: torch.Size([3, 8, 8, 8])\n",
            "Attention mask for first sequence (first head):\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)\n",
            "\n",
            "----------------------------------------\n",
            "Running GQA Forward Pass...\n",
            "\n",
            "GQA output shape: torch.Size([3, 8, 256])\n",
            "Output statistics:\n",
            "  Mean: 0.0013\n",
            "  Std: 0.1430\n",
            "  Min: -0.5132\n",
            "  Max: 0.4253\n",
            "\n",
            "Model Parameter Analysis:\n",
            "  Total GQA parameters: 164,480\n",
            "\n",
            "  Parameter breakdown:\n",
            "    w_q.weight: torch.Size([256, 256]) -> 65,536 parameters\n",
            "    w_q.bias: torch.Size([256]) -> 256 parameters\n",
            "    w_k.weight: torch.Size([64, 256]) -> 16,384 parameters\n",
            "    w_k.bias: torch.Size([64]) -> 64 parameters\n",
            "    w_v.weight: torch.Size([64, 256]) -> 16,384 parameters\n",
            "    w_v.bias: torch.Size([64]) -> 64 parameters\n",
            "    w_o.weight: torch.Size([256, 256]) -> 65,536 parameters\n",
            "    w_o.bias: torch.Size([256]) -> 256 parameters\n",
            "\n",
            "Test completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FlexAttention - PyTorch 2.5+ with custom score modifiers"
      ],
      "metadata": {
        "id": "pfEbbNUttH6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FlexAttention - PyTorch 2.5+ with custom score modifiers\n",
        "\n",
        "import torch\n",
        "\n",
        "def causal_mask(score, batch, head, q_idx, kv_idx):\n",
        "    return torch.where(q_idx >= kv_idx, score, float('-inf'))\n",
        "\n",
        "def sliding_window_mask(window_size):\n",
        "    def mask_fn(score, batch, head, q_idx, kv_idx):\n",
        "        return torch.where(\n",
        "            torch.abs(q_idx - kv_idx) <= window_size,\n",
        "            score, float('-inf')\n",
        "        )\n",
        "    return mask_fn\n",
        "\n",
        "\n",
        "def attention_bias_mask(bias_value=0.1):\n",
        "    \"\"\"Add a small bias to attention scores\"\"\"\n",
        "    def mask_fn(score, batch, head, q_idx, kv_idx):\n",
        "        return score + bias_value\n",
        "    return mask_fn\n",
        "\n",
        "\n",
        "def position_dependent_mask(decay_factor=0.1):\n",
        "    \"\"\"Apply position-dependent decay to attention scores\"\"\"\n",
        "    def mask_fn(score, batch, head, q_idx, kv_idx):\n",
        "        distance = torch.abs(q_idx - kv_idx).float()\n",
        "        decay = torch.exp(-decay_factor * distance)\n",
        "        return score * decay\n",
        "    return mask_fn\n",
        "\n",
        "\n",
        "class FlexAttentionWrapper(nn.Module):\n",
        "    \"\"\"Wrapper for FlexAttention with different score modifiers\"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, score_mod=None):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        q = self.w_q(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Use FlexAttention if available, otherwise fallback to standard attention\n",
        "        try:\n",
        "            from torch.nn.attention.flex_attention import flex_attention\n",
        "            # FlexAttention expects [B, H, T, D] format\n",
        "            if score_mod is not None:\n",
        "                out = flex_attention(q, k, v, score_mod=score_mod)\n",
        "            else:\n",
        "                out = flex_attention(q, k, v)\n",
        "        except ImportError:\n",
        "            print(\"FlexAttention not available, using standard attention fallback\")\n",
        "            # Fallback to standard attention\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "            # Apply score modifier manually if provided\n",
        "            if score_mod is not None:\n",
        "                print(\"Warning: score_mod ignored in fallback mode\")\n",
        "\n",
        "            attn_weights = F.softmax(scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "            out = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape output\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.w_o(out)\n",
        "\n",
        "# Usage with FlexAttention (PyTorch 2.5+)\n",
        "# from torch.nn.attention.flex_attention import flex_attention\n",
        "# output = flex_attention(query, key, value, score_mod=causal_mask)"
      ],
      "metadata": {
        "id": "OoeGzhnZqQZz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_flex_attention():\n",
        "    \"\"\"Test the FlexAttention wrapper with different score modifiers\"\"\"\n",
        "    print(\"Testing FlexAttention with Text Examples\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create text examples\n",
        "    tokenized_sentences, vocab = create_text_example()\n",
        "    print(\"Original sentences:\")\n",
        "    sentences = [\n",
        "        \"the cat sat on the mat\",\n",
        "        \"the dog ran fast\",\n",
        "        \"the bird flew high\",\n",
        "        \"the quick brown fox jumps\"\n",
        "    ]\n",
        "    # for i, sentence in enumerate(sentences):\n",
        "    #     print(f\"  {i+1}. '{sentence}' -> {tokenized_sentences[i]}\")\n",
        "\n",
        "    # Pad sequences\n",
        "    max_seq_len = 10\n",
        "    padded_sequences = pad_sequences(tokenized_sentences, max_seq_len)\n",
        "    input_ids = torch.tensor(padded_sequences)\n",
        "\n",
        "    print(f\"\\nPadded input tensor shape: {input_ids.shape}\")\n",
        "    print(\"Padded sequences:\")\n",
        "    for i, seq in enumerate(padded_sequences):\n",
        "        print(f\"  Sequence {i+1}: {seq}\")\n",
        "\n",
        "    # Model configuration\n",
        "    d_model = 256\n",
        "    num_heads = 8\n",
        "    vocab_size = len(vocab)\n",
        "    batch_size, seq_length = input_ids.shape\n",
        "\n",
        "    print(f\"\\nModel Configuration:\")\n",
        "    print(f\"  d_model: {d_model}\")\n",
        "    print(f\"  num_heads: {num_heads}\")\n",
        "    print(f\"  vocab_size: {vocab_size}\")\n",
        "    print(f\"  sequence_length: {seq_length}\")\n",
        "\n",
        "    # Create embedding layer and FlexAttention\n",
        "    embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "    flex_attn = FlexAttentionWrapper(d_model, num_heads, dropout=0.1)\n",
        "\n",
        "    # Convert tokens to embeddings\n",
        "    x = embedding(input_ids)  # [batch_size, seq_length, d_model]\n",
        "    print(f\"\\nEmbedding output shape: {x.shape}\")\n",
        "\n",
        "    # Test different score modifiers\n",
        "    score_modifiers = {\n",
        "        \"No modifier\": None,\n",
        "        \"Causal mask\": causal_mask,\n",
        "        \"Sliding window (size=3)\": sliding_window_mask(3),\n",
        "        \"Sliding window (size=5)\": sliding_window_mask(5),\n",
        "        \"Attention bias\": attention_bias_mask(0.1),\n",
        "        \"Position decay\": position_dependent_mask(0.2)\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 50)\n",
        "    print(\"Testing Different Score Modifiers\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    flex_attn.eval()\n",
        "    results = {}\n",
        "\n",
        "    for name, score_mod in score_modifiers.items():\n",
        "        print(f\"\\nTesting: {name}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                output = flex_attn(x, score_mod=score_mod)\n",
        "                results[name] = output\n",
        "\n",
        "                print(f\"  Output shape: {output.shape}\")\n",
        "                print(f\"  Output statistics:\")\n",
        "                print(f\"    Mean: {output.mean():.4f}\")\n",
        "                print(f\"    Std: {output.std():.4f}\")\n",
        "                print(f\"    Min: {output.min():.4f}\")\n",
        "                print(f\"    Max: {output.max():.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\")\n",
        "                results[name] = None\n",
        "\n",
        "    # Compare different score modifiers\n",
        "    print(\"\\n\" + \"-\" * 50)\n",
        "    print(\"Comparing Score Modifiers\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    base_output = results.get(\"No modifier\")\n",
        "    if base_output is not None:\n",
        "        for name, output in results.items():\n",
        "            if output is not None and name != \"No modifier\":\n",
        "                diff = torch.abs(output - base_output).mean()\n",
        "                print(f\"{name:25} vs No modifier: Avg diff = {diff:.6f}\")\n",
        "\n",
        "    # Parameter analysis\n",
        "    print(f\"\\nModel Parameter Analysis:\")\n",
        "    total_params = sum(p.numel() for p in flex_attn.parameters())\n",
        "    print(f\"  Total FlexAttention parameters: {total_params:,}\")\n",
        "\n",
        "    print(\"\\n  Parameter breakdown:\")\n",
        "    for name, param in flex_attn.named_parameters():\n",
        "        print(f\"    {name}: {param.shape} -> {param.numel():,} parameters\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FlexAttention test completed!\")\n",
        "\n",
        "    return flex_attn, results, input_ids, vocab\n",
        "\n",
        "\n",
        "def test_score_modifier_behaviors():\n",
        "    \"\"\"Test specific behaviors of different score modifiers\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Testing Score Modifier Behaviors\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create simple test data\n",
        "    seq_len = 6\n",
        "    batch_size = 1\n",
        "\n",
        "    # Simulate score matrix for visualization\n",
        "    print(\"\\nScore Modifier Behavior Analysis:\")\n",
        "    print(\"(Simulating attention patterns)\")\n",
        "\n",
        "    # Test causal mask\n",
        "    print(\"\\n1. Causal Mask:\")\n",
        "    print(\"   Only allows attention to previous positions\")\n",
        "    for q_idx in range(seq_len):\n",
        "        mask_pattern = []\n",
        "        for kv_idx in range(seq_len):\n",
        "            if q_idx >= kv_idx:\n",
        "                mask_pattern.append(\"✓\")\n",
        "            else:\n",
        "                mask_pattern.append(\"✗\")\n",
        "        print(f\"   Position {q_idx}: {' '.join(mask_pattern)}\")\n",
        "\n",
        "    # Test sliding window\n",
        "    window_size = 2\n",
        "    print(f\"\\n2. Sliding Window (size={window_size}):\")\n",
        "    print(\"   Only allows attention within window\")\n",
        "    for q_idx in range(seq_len):\n",
        "        mask_pattern = []\n",
        "        for kv_idx in range(seq_len):\n",
        "            if abs(q_idx - kv_idx) <= window_size:\n",
        "                mask_pattern.append(\"✓\")\n",
        "            else:\n",
        "                mask_pattern.append(\"✗\")\n",
        "        print(f\"   Position {q_idx}: {' '.join(mask_pattern)}\")\n",
        "\n",
        "    print(\"\\n3. Position Decay:\")\n",
        "    print(\"   Attention strength decreases with distance\")\n",
        "    decay_factor = 0.3\n",
        "    for q_idx in [0, 2, 4]:\n",
        "        decay_values = []\n",
        "        for kv_idx in range(seq_len):\n",
        "            distance = abs(q_idx - kv_idx)\n",
        "            decay = math.exp(-decay_factor * distance)\n",
        "            decay_values.append(f\"{decay:.2f}\")\n",
        "        print(f\"   From pos {q_idx}: {' '.join(decay_values)}\")\n"
      ],
      "metadata": {
        "id": "6POn0je_9jiB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_flex_attention_features():\n",
        "    \"\"\"Demonstrate key features of FlexAttention\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FlexAttention Key Features\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    features = [\n",
        "        \"✓ Custom score modification functions\",\n",
        "        \"✓ Efficient computation with PyTorch 2.5+\",\n",
        "        \"✓ Support for causal masking\",\n",
        "        \"✓ Sliding window attention patterns\",\n",
        "        \"✓ Position-dependent attention weighting\",\n",
        "        \"✓ Flexible attention bias injection\",\n",
        "        \"✓ Memory-efficient implementation\",\n",
        "        \"✓ Backward compatibility fallback\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nSupported Features:\")\n",
        "    for feature in features:\n",
        "        print(f\"  {feature}\")\n",
        "\n",
        "    print(\"\\nUse Cases:\")\n",
        "    use_cases = [\n",
        "        \"• Language modeling with causal attention\",\n",
        "        \"• Local attention for long sequences\",\n",
        "        \"• Position-biased attention mechanisms\",\n",
        "        \"• Custom attention patterns for specific tasks\",\n",
        "        \"• Efficient transformer variants\"\n",
        "    ]\n",
        "\n",
        "    for use_case in use_cases:\n",
        "        print(f\"  {use_case}\")\n"
      ],
      "metadata": {
        "id": "x6ZacKg19zky"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run main test\n",
        "flex_model, results, input_ids, vocab = test_flex_attention()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsRZBHP493oZ",
        "outputId": "7213bbee-9658-4a98-8fd6-39f80cb1a9b8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing FlexAttention with Text Examples\n",
            "============================================================\n",
            "Original sentences:\n",
            "\n",
            "Padded input tensor shape: torch.Size([3, 10])\n",
            "Padded sequences:\n",
            "  Sequence 1: [1, 3, 4, 5, 6, 3, 7, 2, 0, 0]\n",
            "  Sequence 2: [1, 3, 8, 9, 10, 2, 0, 0, 0, 0]\n",
            "  Sequence 3: [1, 3, 11, 12, 13, 2, 0, 0, 0, 0]\n",
            "\n",
            "Model Configuration:\n",
            "  d_model: 256\n",
            "  num_heads: 8\n",
            "  vocab_size: 14\n",
            "  sequence_length: 10\n",
            "\n",
            "Embedding output shape: torch.Size([3, 10, 256])\n",
            "\n",
            "--------------------------------------------------\n",
            "Testing Different Score Modifiers\n",
            "--------------------------------------------------\n",
            "\n",
            "Testing: No modifier\n",
            "------------------------------\n",
            "  Output shape: torch.Size([3, 10, 256])\n",
            "  Output statistics:\n",
            "    Mean: 0.0013\n",
            "    Std: 0.0988\n",
            "    Min: -0.3647\n",
            "    Max: 0.3408\n",
            "\n",
            "Testing: Causal mask\n",
            "------------------------------\n",
            "  Output shape: torch.Size([3, 10, 256])\n",
            "  Output statistics:\n",
            "    Mean: -0.0022\n",
            "    Std: 0.1797\n",
            "    Min: -0.6847\n",
            "    Max: 0.8259\n",
            "\n",
            "Testing: Sliding window (size=3)\n",
            "------------------------------\n",
            "  Output shape: torch.Size([3, 10, 256])\n",
            "  Output statistics:\n",
            "    Mean: 0.0015\n",
            "    Std: 0.1311\n",
            "    Min: -0.5487\n",
            "    Max: 0.5898\n",
            "\n",
            "Testing: Sliding window (size=5)\n",
            "------------------------------\n",
            "  Output shape: torch.Size([3, 10, 256])\n",
            "  Output statistics:\n",
            "    Mean: 0.0018\n",
            "    Std: 0.1140\n",
            "    Min: -0.4379\n",
            "    Max: 0.4796\n",
            "\n",
            "Testing: Attention bias\n",
            "------------------------------\n",
            "  Output shape: torch.Size([3, 10, 256])\n",
            "  Output statistics:\n",
            "    Mean: 0.0013\n",
            "    Std: 0.0988\n",
            "    Min: -0.3647\n",
            "    Max: 0.3408\n",
            "\n",
            "Testing: Position decay\n",
            "------------------------------\n",
            "  Output shape: torch.Size([3, 10, 256])\n",
            "  Output statistics:\n",
            "    Mean: 0.0011\n",
            "    Std: 0.0971\n",
            "    Min: -0.3386\n",
            "    Max: 0.3210\n",
            "\n",
            "--------------------------------------------------\n",
            "Comparing Score Modifiers\n",
            "--------------------------------------------------\n",
            "Causal mask               vs No modifier: Avg diff = 0.082744\n",
            "Sliding window (size=3)   vs No modifier: Avg diff = 0.063178\n",
            "Sliding window (size=5)   vs No modifier: Avg diff = 0.030027\n",
            "Attention bias            vs No modifier: Avg diff = 0.000000\n",
            "Position decay            vs No modifier: Avg diff = 0.006506\n",
            "\n",
            "Model Parameter Analysis:\n",
            "  Total FlexAttention parameters: 263,168\n",
            "\n",
            "  Parameter breakdown:\n",
            "    w_q.weight: torch.Size([256, 256]) -> 65,536 parameters\n",
            "    w_q.bias: torch.Size([256]) -> 256 parameters\n",
            "    w_k.weight: torch.Size([256, 256]) -> 65,536 parameters\n",
            "    w_k.bias: torch.Size([256]) -> 256 parameters\n",
            "    w_v.weight: torch.Size([256, 256]) -> 65,536 parameters\n",
            "    w_v.bias: torch.Size([256]) -> 256 parameters\n",
            "    w_o.weight: torch.Size([256, 256]) -> 65,536 parameters\n",
            "    w_o.bias: torch.Size([256]) -> 256 parameters\n",
            "\n",
            "============================================================\n",
            "FlexAttention test completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.keys(),input_ids.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ludl8lx4-8Cu",
        "outputId": "fd8d1ced-0ccf-4df0-9f5c-03af1cee198b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['No modifier', 'Causal mask', 'Sliding window (size=3)', 'Sliding window (size=5)', 'Attention bias', 'Position decay']),\n",
              " torch.Size([3, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flex_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm4zNYam_Nv-",
        "outputId": "b3c24d34-dd63-49f0-ea71-3bf8d64757ba"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FlexAttentionWrapper(\n",
              "  (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run additional analyses\n",
        "test_score_modifier_behaviors()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ie5coV-93rR",
        "outputId": "05b11b42-9e85-46cb-e65a-af02d1f123ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Testing Score Modifier Behaviors\n",
            "============================================================\n",
            "\n",
            "Score Modifier Behavior Analysis:\n",
            "(Simulating attention patterns)\n",
            "\n",
            "1. Causal Mask:\n",
            "   Only allows attention to previous positions\n",
            "   Position 0: ✓ ✗ ✗ ✗ ✗ ✗\n",
            "   Position 1: ✓ ✓ ✗ ✗ ✗ ✗\n",
            "   Position 2: ✓ ✓ ✓ ✗ ✗ ✗\n",
            "   Position 3: ✓ ✓ ✓ ✓ ✗ ✗\n",
            "   Position 4: ✓ ✓ ✓ ✓ ✓ ✗\n",
            "   Position 5: ✓ ✓ ✓ ✓ ✓ ✓\n",
            "\n",
            "2. Sliding Window (size=2):\n",
            "   Only allows attention within window\n",
            "   Position 0: ✓ ✓ ✓ ✗ ✗ ✗\n",
            "   Position 1: ✓ ✓ ✓ ✓ ✗ ✗\n",
            "   Position 2: ✓ ✓ ✓ ✓ ✓ ✗\n",
            "   Position 3: ✗ ✓ ✓ ✓ ✓ ✓\n",
            "   Position 4: ✗ ✗ ✓ ✓ ✓ ✓\n",
            "   Position 5: ✗ ✗ ✗ ✓ ✓ ✓\n",
            "\n",
            "3. Position Decay:\n",
            "   Attention strength decreases with distance\n",
            "   From pos 0: 1.00 0.74 0.55 0.41 0.30 0.22\n",
            "   From pos 2: 0.55 0.74 1.00 0.74 0.55 0.41\n",
            "   From pos 4: 0.30 0.41 0.55 0.74 1.00 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demonstrate_flex_attention_features()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt0CPK6j-bhU",
        "outputId": "940a1235-2724-4909-e079-c767968dd88f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FlexAttention Key Features\n",
            "============================================================\n",
            "\n",
            "Supported Features:\n",
            "  ✓ Custom score modification functions\n",
            "  ✓ Efficient computation with PyTorch 2.5+\n",
            "  ✓ Support for causal masking\n",
            "  ✓ Sliding window attention patterns\n",
            "  ✓ Position-dependent attention weighting\n",
            "  ✓ Flexible attention bias injection\n",
            "  ✓ Memory-efficient implementation\n",
            "  ✓ Backward compatibility fallback\n",
            "\n",
            "Use Cases:\n",
            "  • Language modeling with causal attention\n",
            "  • Local attention for long sequences\n",
            "  • Position-biased attention mechanisms\n",
            "  • Custom attention patterns for specific tasks\n",
            "  • Efficient transformer variants\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Feed-Forward Networks"
      ],
      "metadata": {
        "id": "o-mokq_B_4Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# SwiGLU Activation (used in LLaMA, PaLM)\n",
        "class SwiGLUFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff, bias=False)  # Gate\n",
        "        self.w2 = nn.Linear(d_ff, d_model, bias=False)   # Down projection\n",
        "        self.w3 = nn.Linear(d_model, d_ff, bias=False)   # Up projection\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = F.silu(self.w1(x))  # SiLU activation\n",
        "        up = self.w3(x)\n",
        "        return self.w2(self.dropout(gate * up))\n",
        "\n",
        "# Mixture of Experts (MoE) Layer\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, num_experts, top_k=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Router network\n",
        "        self.gate = nn.Linear(d_model, num_experts, bias=False)\n",
        "\n",
        "        # Expert networks (batched for efficiency)\n",
        "        self.experts_w1 = nn.Parameter(torch.randn(num_experts, d_model, d_ff))\n",
        "        self.experts_w2 = nn.Parameter(torch.randn(num_experts, d_ff, d_model))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Compute routing weights\n",
        "        router_logits = self.gate(x)  # [B, T, num_experts]\n",
        "        routing_weights = F.softmax(router_logits, dim=-1)\n",
        "\n",
        "        # Select top-k experts\n",
        "        top_k_weights, top_k_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n",
        "        top_k_weights = F.softmax(top_k_weights, dim=-1)\n",
        "\n",
        "        # Reshape for batched expert computation\n",
        "        x_expanded = x.unsqueeze(2).expand(B, T, self.top_k, C)  # [B, T, top_k, C]\n",
        "\n",
        "        # Gather expert weights\n",
        "        expert_w1 = self.experts_w1[top_k_indices]  # [B, T, top_k, C, d_ff]\n",
        "        expert_w2 = self.experts_w2[top_k_indices]  # [B, T, top_k, d_ff, C]\n",
        "\n",
        "        # Apply experts\n",
        "        hidden = torch.matmul(x_expanded.unsqueeze(-2), expert_w1).squeeze(-2)  # [B, T, top_k, d_ff]\n",
        "        hidden = F.gelu(hidden)\n",
        "        hidden = self.dropout(hidden)\n",
        "        outputs = torch.matmul(hidden.unsqueeze(-2), expert_w2).squeeze(-2)  # [B, T, top_k, C]\n",
        "\n",
        "        # Weighted combination\n",
        "        final_output = torch.sum(outputs * top_k_weights.unsqueeze(-1), dim=2)  # [B, T, C]\n",
        "\n",
        "        return final_output, routing_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "XlWqapK89zoS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m0sx21nxUGlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_swiglu_and_moe():\n",
        "    \"\"\"\n",
        "    Test SwiGLUFeedForward and MoELayer using a precomputed input_ids tensor.\n",
        "    Expects input_ids: LongTensor of shape [B, T].\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    # Create text examples\n",
        "    tokenized_sentences, vocab = create_text_example()\n",
        "    print(\"Original sentences:\")\n",
        "    sentences = [\n",
        "        \"the cat sat on the mat\",\n",
        "        \"the dog ran fast\",\n",
        "        \"the bird flew high\",\n",
        "        \"the quick brown fox jumps\"\n",
        "    ]\n",
        "    # for i, sentence in enumerate(sentences):\n",
        "    #     print(f\"  {i+1}. '{sentence}' -> {tokenized_sentences[i]}\")\n",
        "\n",
        "    # Pad sequences\n",
        "    max_seq_len = 10\n",
        "    padded_sequences = pad_sequences(tokenized_sentences, max_seq_len)\n",
        "    input_ids = torch.tensor(padded_sequences)\n",
        "\n",
        "    print(f\"\\nPadded input tensor shape: {input_ids.shape}\")\n",
        "    print(\"Padded sequences:\")\n",
        "    for i, seq in enumerate(padded_sequences):\n",
        "        print(f\"  Sequence {i+1}: {seq}\")\n",
        "\n",
        "    # Model configuration (using GQA config)\n",
        "    d_model = 256\n",
        "    num_heads = 8\n",
        "    d_head = d_model//num_heads # dimension of each KV heads\n",
        "    num_kv_heads=2 # use 1 for MQA\n",
        "    d_kv = d_head * num_kv_heads\n",
        "    n_experts = 8\n",
        "    vocab_size = len(vocab)\n",
        "    batch_size, seq_length = input_ids.shape\n",
        "\n",
        "    print(f\"\\nModel Configuration:\")\n",
        "    print(f\"  d_model: {d_model}\")\n",
        "    print(f\"  num_heads: {num_heads}\")\n",
        "    print(f\"  dimension of each KV heads (d_head): {d_head}\")\n",
        "    print(f\"  number of KV heads (num_kv_heads): {num_kv_heads}\")\n",
        "    print(f\"  dimension of all K/V projections combined (d_kv): {d_kv}\")\n",
        "    print(f\"  number of experts (n_experts): {n_experts}\")\n",
        "    print(f\"  vocab_size: {vocab_size}\")\n",
        "    print(f\"  sequence_length: {seq_length}\")\n",
        "\n",
        "    # Embedding layer to convert IDs → embeddings\n",
        "    # Create embedding layer\n",
        "    embedding = nn.Embedding(vocab_size, d_head, padding_idx=0)\n",
        "\n",
        "    # Convert tokens to embeddings\n",
        "    x = embedding(input_ids)  # [batch_size, seq_length, d_head]\n",
        "    print(f\"\\nEmbedding output shape [batch_size, seq_length, d_head]: {x.shape}\")\n",
        "\n",
        "    # SwiGLU Feed-Forward\n",
        "    ff = SwiGLUFeedForward(d_model=d_head, d_ff=d_kv)\n",
        "    out_ff = ff(x)\n",
        "    print(f\"SwiGLUFeedForward output shape: {out_ff.shape}\")\n",
        "\n",
        "    # MoE Layer\n",
        "    moe = MoELayer(d_model=d_head, d_ff=d_kv, num_experts=n_experts, top_k=2)\n",
        "    out_moe, routing_weights = moe(x)\n",
        "    print(f\"MoELayer output shape:  {out_moe.shape}\")\n",
        "\n",
        "    # Plot average routing weights\n",
        "    avg_weights = routing_weights.mean(dim=(0,1)).detach().numpy()\n",
        "    experts = [f\"E{i}\" for i in range(len(avg_weights))]\n",
        "    plt.bar(experts, avg_weights, color=\"skyblue\")\n",
        "    plt.title(\"Average Routing Weights per Expert\")\n",
        "    plt.xlabel(\"Expert\")\n",
        "    plt.ylabel(\"Avg Weight\")\n",
        "    for i, v in enumerate(avg_weights):\n",
        "        plt.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "6ECAnBG8Hpuj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_swiglu_and_moe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "NHWsQpfyHx0D",
        "outputId": "8641535e-d76f-4b2b-f87c-7991a800ab8f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentences:\n",
            "\n",
            "Padded input tensor shape: torch.Size([3, 10])\n",
            "Padded sequences:\n",
            "  Sequence 1: [1, 3, 4, 5, 6, 3, 7, 2, 0, 0]\n",
            "  Sequence 2: [1, 3, 8, 9, 10, 2, 0, 0, 0, 0]\n",
            "  Sequence 3: [1, 3, 11, 12, 13, 2, 0, 0, 0, 0]\n",
            "\n",
            "Model Configuration:\n",
            "  d_model: 256\n",
            "  num_heads: 8\n",
            "  dimension of each KV heads (d_head): 32\n",
            "  number of KV heads (num_kv_heads): 2\n",
            "  dimension of all K/V projections combined (d_kv): 64\n",
            "  number of experts (n_experts): 8\n",
            "  vocab_size: 14\n",
            "  sequence_length: 10\n",
            "\n",
            "Embedding output shape [batch_size, seq_length, d_head]: torch.Size([3, 10, 32])\n",
            "SwiGLUFeedForward output shape: torch.Size([3, 10, 32])\n",
            "MoELayer output shape:  torch.Size([3, 10, 32])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s32yX9HxXp-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}