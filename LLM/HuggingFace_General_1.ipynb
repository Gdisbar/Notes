{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "HuggingFace-General-1",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "8C6KBzjtU0nX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Model contains following files\n",
        "\n",
        "**config.json** - It includes hyperparameters, model type, and other settings that define how the model is structured and how it should be initialized.\n",
        "\n",
        "**generation_config.json** - This file includes configuration settings specific to the text generation process. This might include parameters like maximum sequence length, sampling strategies, and other settings relevant to generating text.\n",
        "\n",
        "**model.safetensors.index.json** -  This file is associated with the safetensors format and contains metadata and indexing information for efficiently accessing the model weights stored in the safetensors files. It helps in managing large model weights across multiple files.\n",
        "\n",
        "\n",
        "**model-00001-of-00003.safetensors, model-00002-of-00003.safetensors, model-00003-of-00003.safetensors** - These files contain the actual quantized model weights. Since the model is large, the weights are split across multiple files. The .safetensors format is used to safely and efficiently store these weights, ensuring that they are properly serialized and deserialized during model loading.\n",
        "\n",
        "**special_tokens_map.json** - This file maps special tokens used by the tokenizer to their respective identifiers. Special tokens include things like [CLS], [SEP], or any other token that has a specific role in the tokenization process.\n",
        "\n",
        "**tokenizer.json** - This file contains the tokenizer's vocabulary and configuration in a JSON format. It includes mappings from token strings to token IDs and other tokenizer-specific settings.\n",
        "\n",
        "**tokenizer.model** -This file is the actual binary model used by the tokenizer. It typically contains the underlying data structures required for tokenization and detokenization.\n",
        "\n",
        "**tokenizer_config.json** - This file includes configuration settings for the tokenizer itself. It contains details on how the tokenizer should process text, such as pre-tokenization, normalization, and other settings.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "**Configuration Files (config.json, generation_config.json, tokenizer_config.json)**: These files define the setup and parameters for both the model and tokenizer.\n",
        "\n",
        "**Model Weights Files (model-*.safetensors)**: These files contain the trained and quantized weights of the model.\n",
        "\n",
        "**Tokenizer Files (tokenizer.json, tokenizer.model, special_tokens_map.json)**: These files are used for tokenizing and detokenizing text, including the tokenizer's vocabulary and special tokens.\n",
        "\n",
        "\n",
        "When loading the model and tokenizer, you typically need to load all these files to ensure that the model functions correctly and the tokenizer behaves as expected. If you're using a library like transformers, it handles the loading of these files automatically when you use functions like from_pretrained."
      ],
      "metadata": {
        "id": "aVeCR54fU0nY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Alpaca style Prompt\n",
        "\n",
        "```python\n",
        "# {\n",
        "#     \"description\": \"Template used by Alpaca-LoRA.\",\n",
        "#     \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
        "#     \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
        "#     \"response_split\": \"### Response:\"    \n",
        "# }\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def format_prompt(sample):\n",
        "    instructions=sample[\"instruction\"] # here system_prompt\n",
        "    inputs = sample[\"input\"]           # here user_prompt\n",
        "    responses = sample[\"output\"]        # here \"\" preset but will be in training dataset\n",
        "    texts = []\n",
        "    for instruction,input,response in zip(instructions,inputs,responses):\n",
        "        text = alpaca_prompt.format(instruction,input,response)+EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\":texts,} # add data in 1 column for SFTTrainer\n",
        "    \n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\",split=\"train\")\n",
        "dataset = dataset.map(format_prompt,batched=True)\n",
        "\n",
        "\n",
        "def prepare_for_peft(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False  # freeze the model - train adapters later\n",
        "        if param.dim() == 1:\n",
        "            # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "            param.data = param.data.to(torch.float32)\n",
        "\n",
        "    model.config.gradient_checkpointing = True  # enable gradient checkpointing\n",
        "    model.config.use_cache = False  # disable cache for memory efficiency\n",
        "    model.config.output_hidden_states = True  # set to True if you want hidden states\n",
        "    model.config.output_attentions = True  # set to True if you want attention weights\n",
        "\n",
        "    # No need to define a separate class, we can use nn.Sequential directly\n",
        "    model.lm_head = nn.Sequential(nn.Linear(model.config.hidden_size, model.config.vocab_size))\n",
        "    return model\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "kOTmpZKWU0nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PTQ - Post Training Quantization\n",
        "\n",
        "Notes:\n",
        "\n",
        "* **For Quantized Models**: Ensure to apply the same quantization configuration when reloading.\n",
        "* **Model Architecture**: When loading the state dict, make sure the model architecture matches the saved state dict.\n",
        "\n",
        "## Quantization HuggingFace\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/quantization/overview\n",
        "\n",
        "https://www.e2enetworks.com/blog/which-quantization-method-is-best-for-you-gguf-gptq-or-awq"
      ],
      "metadata": {
        "id": "rl1zF4p7U0nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For LLM\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Sample input text\n",
        "input_text = \"Why did the scarecrow become a successful neurosurgeon?\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    original_outputs = model.generate(input_ids, max_length=50)\n",
        "\n",
        "original_text = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "quantized_model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    quantized_outputs = quantized_model.generate(input_ids, max_length=50)\n",
        "\n",
        "quantized_text = tokenizer.decode(quantized_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "**If we compare** ``` quantized_model.generate(...) & model.generate(...)``` **we can observe a significant speed enhancement**"
      ],
      "metadata": {
        "id": "qiPqFnZ_U0ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Mistral-7B 4-bit AWQ\n",
        "\n",
        "```python\n",
        "\n",
        "!pip install -q --upgrade transformers autoawq accelerate\n",
        "\n",
        "model_path = \"mistralai/Mistral-7B-v0.3\"\n",
        "quant_path = \"Mistral-7B-AWQ-4bit\"\n",
        "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\":4}\n",
        "\n",
        "# Load model\n",
        "model = AutoAWQForCausalLM.from_pretrained(model_path,**{\"low_cpu_mem_usage\": True})\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "# Quantize\n",
        "model.quantize(tokenizer, quant_config=quant_config)\n",
        "\n",
        "#save the quantized model\n",
        "model.save_quantized(\"./\"+quant_path, safetensors=True)\n",
        "tokenizer.save_pretrained(\"./\"+quant_path)\n",
        "\n",
        "# load model to huggingface\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "username = \"pritam3355\"\n",
        "MODEL_NAME = quant_path\n",
        "\n",
        "api = HfApi(token=hf_tokens)\n",
        "\n",
        "api.create_repo(repo_id = f\"{username}/{MODEL_NAME}\",repo_type=\"model\")\n",
        "\n",
        "api.upload_folder(repo_id = f\"{username}/{MODEL_NAME}\",\n",
        "                  folder_path = f\"/kaggle/working/{MODEL_NAME}\")\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "L55ZRKpaU0ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference from Mistral-7B-AWQ-4bit - AutoAWQForCausalLM\n",
        "\n",
        "```python\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "model_id=f\"{username}/{MODEL_NAME}\"\n",
        "\n",
        "model = AutoAWQForCausalLM.from_quantized(model_id, fuse_layers=True,\n",
        "                                          trust_remote_code=False, safetensors=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'])\n",
        "```\n"
      ],
      "metadata": {
        "id": "8imNlykYU0nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference from Mistral-7B-AWQ-4bit - AutoModelForCausalLM\n",
        "\n",
        "```python\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
        "\n",
        "# Model and quantization configuration\n",
        "model_id = f\"{username}/{MODEL_NAME}\" # TechxGenus/Mistral-7B-v0.3-AWQ,kaitchup/Mistral-7B-awq-4bit\n",
        "quantization_config = AwqConfig(bits=4,fuse_max_seq_len=512,\n",
        "                                do_fuse=True,attn_implementation=\"flash_attention_2\",)\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             low_cpu_mem_usage=True,\n",
        "                                             device_map=\"auto\",\n",
        "                                             quantization_config=quantization_config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=False)\n",
        "\n",
        "# Define the system and user prompts\n",
        "system_prompt = \"You are an AI assistant knowledgeable in various fields.\"\n",
        "user_prompt = \"Tell me about continuous batching for faster inference in LLM\"\n",
        "\n",
        "# Create the prompt template\n",
        "prompt_template = f'{system_prompt}\\n\\nUser: {user_prompt}\\nAssistant:'\n",
        "\n",
        "# Tokenize the input\n",
        "tokens = tokenizer( prompt_template,return_tensors='pt').input_ids.cuda()\n",
        "\n",
        "# Generate output\n",
        "generation_output = model.generate(tokens,do_sample=True,temperature=0.7,\n",
        "                                   top_p=0.95,top_k=40,max_new_tokens=512)\n",
        "\n",
        "# Decode and print the output\n",
        "print(\"Output: \", tokenizer.decode(generation_output[0], skip_special_tokens=True))\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**Performance drops significantly in AWQ Llama3 rather use Mistral-7B (format is same) but if you're using AutoAWQForCausalLM for just loading the model**"
      ],
      "metadata": {
        "id": "O5ISodFiU0nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Optimized Decision Guide for Hosting & Serving Custom LLM APIs**  \n",
        "*Balancing Availability, Cost, Compliance, and Performance*\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Core Decision Matrix: Factors vs Tools**  \n",
        "| **Factor**          | **Key Impact**                              | **Optimal Tools/Services**                                                                 | **Use Case Alignment**                    |\n",
        "|----------------------|---------------------------------------------|-------------------------------------------------------------------------------------------|--------------------------------------------|\n",
        "| **Availability**     | Uptime, redundancy, failover                | AWS SageMaker, GCP Vertex AI, Kubernetes (EKS/GKE) with auto-scaling                      | Mission-critical APIs (e.g., healthcare)  |\n",
        "| **Scalability**      | Handle traffic spikes, parallel inference   | KServe, Ray Serve, API Gateway (AWS/Cloudflare)                                           | High-traffic public APIs                   |\n",
        "| **Latency**          | Real-time response optimization             | Bare-metal GPUs + Triton/TensorRT, FastAPI + Redis caching, WebSockets                    | Chatbots, trading systems                  |\n",
        "| **Security**         | Data protection, access control             | SageMaker VPC, Azure ML Private Endpoints, HashiCorp Vault, OAuth 2.0                     | Compliance-heavy sectors (banking, healthcare) |\n",
        "| **Maintainability**  | CI/CD, model versioning                     | MLflow, TFX, Kubernetes + ArgoCD                                                          | Rapid iteration environments               |\n",
        "| **Cost**             | Balance compute/operational expenses        | Serverless (Lambda/Cloud Functions), Spot Instances, SageMaker Async Inference            | Startups, batch processing                 |\n",
        "| **Compliance**       | GDPR, HIPAA, SOC2 adherence                 | AWS SageMaker (HIPAA), Azure ML (FedRAMP), GCP Vertex AI (SOC2)                           | Enterprise/regulated industries            |\n",
        "| **Batching**         | Throughput optimization                     | Ray Serve, NVIDIA Triton, SageMaker Batch Transform                                       | Large-scale async tasks (e.g., document processing) |\n",
        "| **Caching**          | Reduce redundant compute                    | Redis, Cloudflare Edge Cache, FastAPI middleware                                          | High-repetition query scenarios            |\n",
        "| **Observability**    | Debugging, performance tracking             | Prometheus + Grafana, AWS CloudWatch, ELK Stack                                           | Complex distributed systems                |\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Strategic Infrastructure Setup**  \n",
        "#### **Compute Layer**  \n",
        "- **Ultra-Low Latency**: NVIDIA Triton + TensorRT on A100/H100 GPUs.  \n",
        "- **Managed Service**: SageMaker/Vertex AI for compliance and scalability.  \n",
        "- **Cost-Effective Scaling**: Kubernetes (KServe/Ray Serve) with cluster autoscaler.  \n",
        "\n",
        "#### **API Layer**  \n",
        "- **Traffic Management**: AWS API Gateway (rate limiting, caching) or Cloudflare Workers (edge caching).  \n",
        "- **Protocols**: WebSockets for real-time apps (e.g., chatbots), REST for general use.  \n",
        "\n",
        "#### **Optimization Layer**  \n",
        "- **Model Compression**: ONNX Runtime, Hugging Face Optimum.  \n",
        "- **Batching**: Triton Dynamic Batching, Ray Serveâ€™s request queuing.  \n",
        "\n",
        "#### **Security Layer**  \n",
        "- **Data**: AES-256 encryption (in-transit via TLS, at-rest via KMS).  \n",
        "- **Access**: IAM roles (AWS), API Gateway JWT authorizers, PrivateLink/VPC.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3. Use Case-Driven Recommendations**  \n",
        "#### **ðŸš€ Startups & Prototyping**  \n",
        "- **Tools**: Hugging Face Inference Endpoints + Lambda + Redis.  \n",
        "- **Why**: Zero infra management, pay-per-use pricing, and fast iteration.  \n",
        "\n",
        "#### **ðŸ“ˆ High-Traffic Public APIs (10M+ requests/day)**  \n",
        "- **Stack**: Kubernetes (KServe) + API Gateway + Redis + Cloudflare.  \n",
        "- **Optimizations**: Model quantization (TensorRT), request caching, autoscaling.  \n",
        "\n",
        "#### **âš¡ Real-Time Systems (Chatbots, Trading)**  \n",
        "- **Stack**: Bare-metal GPU instances + Triton + WebSockets.  \n",
        "- **Tactics**: Preloading models, tokenization optimizations, persistent connections.  \n",
        "\n",
        "#### **ðŸ¦ Compliance-First Workloads (Healthcare, Finance)**  \n",
        "- **Stack**: SageMaker (HIPAA) / Azure ML (FedRAMP) + PrivateLink + Vault.  \n",
        "- **Audits**: Enable CloudTrail/Azure Monitor logs for audit trails.  \n",
        "\n",
        "---\n",
        "\n",
        "### **4. Cost vs Performance Trade-Off Analysis**  \n",
        "| **Scenario**               | **Cost-Optimal Choice**        | **Performance-Optimal Choice**     | **Compromise**                          |\n",
        "|----------------------------|---------------------------------|-------------------------------------|------------------------------------------|\n",
        "| **Low/Spiky Traffic**       | Serverless (Lambda)            | Dedicated GPU instances             | Spot Instances + Auto-Scaling            |\n",
        "| **Batch Processing**        | SageMaker Async Inference       | Ray Serve + Dynamic Batching        | Hybrid batching with Kubernetes          |\n",
        "| **Data-Sensitive Workloads**| Managed Services (SageMaker)    | Self-hosted Triton in VPC            | Private cloud with hybrid encryption     |\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Industry Best Practices**  \n",
        "1. **Start Small**: Begin with serverless + Hugging Face for MVP validation.  \n",
        "2. **Scale Smart**: Transition to Kubernetes when traffic stabilizes (>1k RPM).  \n",
        "3. **Observe Rigorously**: Embed Prometheus/Grafana early to preempt bottlenecks.  \n",
        "4. **Cache Aggressively**: Use Redis for repeated queries (e.g., FAQ bots).  \n",
        "5. **Compliance by Design**: Choose managed services with certifications (SOC2, HIPAA) from day one for regulated sectors.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Final Decision Flowchart**  \n",
        "1. **Define Latency Needs**:  \n",
        "   - **<100ms**: Bare-metal GPUs + Triton.  \n",
        "   - **>100ms**: Managed services (SageMaker) or serverless.  \n",
        "\n",
        "2. **Assess Compliance**:  \n",
        "   - **Yes**: Azure ML/SageMaker with VPC.  \n",
        "   - **No**: Open-source stack (KServe + Redis).  \n",
        "\n",
        "3. **Evaluate Traffic Patterns**:  \n",
        "   - **Spiky**: Serverless + API Gateway.  \n",
        "   - **Steady**: Kubernetes with HPA.  \n",
        "\n",
        "4. **Optimize Costs**:  \n",
        "   - Use spot instances for non-critical workloads.  \n",
        "   - Cache 70%+ repetitive requests with Redis.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PFvKqo-DU0nn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "eprc95TcU0no"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}