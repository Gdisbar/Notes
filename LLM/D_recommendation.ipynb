{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 77759,
          "sourceType": "datasetVersion",
          "datasetId": 339
        },
        {
          "sourceId": 85725,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 72017,
          "modelId": 96961
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "D-recommendation",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'movielens-20m-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F339%2F77759%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240908%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240908T041443Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db8e086afecdbd3416ede04217779ee00526fc1ec42810d4c921e373a4d4e148e1810e6914e5abb6192627c1e4383d5d162dbf45c5b9ba56af4428346366b7a52c573b54dfd6fce8b51e2a361512abf4fb73485562c4373b384a1cafbf97cd52483016cc9bdc80ce8187bb0dae0330e19845e8bfe0b45a53086a9f0dec2d44bd2605145df3db51fa61d251d394234be70452637b9fdca666fcfce2835255066472d4d805d6c8251063f8774fb948aba69c2dcc2907e128b6e829f1528965f6fa5f3fae66f6315199e6ddad5896ec96413cdb0b05273e23f48d6e36c95e5f7d49246ec5f56ba7687b7267591fb50d897c4ee23a3baead625fe3ee0febc681a2e62,d-recommendation/pytorch/v.1.0/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F72017%2F85725%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240908%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240908T041443Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D59eacb17ab1cc2844f8c123402f920a21373b2761fe3e9a650486f45da8e3f0f534206b6a428b1efdd40719fb9be27330080da2a8105b44fa7918827129e0c09e7f4de9758c4d47ce85bd4e95a24d3110205200e22961bfc5cdf890b207a1de02fba17bf17957bdc46ba119686d1ece59434c81a7ae796a2adf52ff642685387871e1781fb9b7af54537d75dae6d37288ac41eb84849b98c4af468d19008fa382817a6681da5b7cc9ba97beb500de645fedc6cd3cff8afd7f883fe01db7f90c6d2e4d0eb43be941221b30e1ad7b2b09fd3dd0668c474c233b9590c07027d6b210c83c6cc4b562867420b48459a605def51cf0b3dc56cfe7a3dfc4299d709225e'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "OvTEoF5t07Bz"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yv8QYRBO1E6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n",
        "# !chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh\n",
        "# !bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-08-07T10:35:03.970898Z",
          "iopub.execute_input": "2024-08-07T10:35:03.971606Z",
          "iopub.status.idle": "2024-08-07T10:35:03.979948Z",
          "shell.execute_reply.started": "2024-08-07T10:35:03.971539Z",
          "shell.execute_reply": "2024-08-07T10:35:03.978155Z"
        },
        "trusted": true,
        "id": "YGWyrUUF07B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !conda install pytorch cudatoolkit=11.3 -c pytorch-nightly -y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:35:14.846967Z",
          "iopub.execute_input": "2024-08-07T10:35:14.847362Z",
          "iopub.status.idle": "2024-08-07T10:35:14.852862Z",
          "shell.execute_reply.started": "2024-08-07T10:35:14.847332Z",
          "shell.execute_reply": "2024-08-07T10:35:14.851701Z"
        },
        "trusted": true,
        "id": "iPNJrAM107B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install torchrec-nightly torchrec"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:40:17.942023Z",
          "iopub.execute_input": "2024-08-07T10:40:17.942498Z",
          "iopub.status.idle": "2024-08-07T10:40:17.947006Z",
          "shell.execute_reply.started": "2024-08-07T10:40:17.942471Z",
          "shell.execute_reply": "2024-08-07T10:40:17.946148Z"
        },
        "trusted": true,
        "id": "QLfMCs5V07B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sklearn"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:35:38.00305Z",
          "iopub.execute_input": "2024-08-07T10:35:38.003508Z",
          "iopub.status.idle": "2024-08-07T10:35:38.009675Z",
          "shell.execute_reply.started": "2024-08-07T10:35:38.003466Z",
          "shell.execute_reply": "2024-08-07T10:35:38.008265Z"
        },
        "trusted": true,
        "id": "dl0LhnYk07B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /usr/local/lib/lib* /usr/lib/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:40:17.949707Z",
          "iopub.execute_input": "2024-08-07T10:40:17.949991Z",
          "iopub.status.idle": "2024-08-07T10:40:17.954837Z",
          "shell.execute_reply.started": "2024-08-07T10:40:17.949968Z",
          "shell.execute_reply": "2024-08-07T10:40:17.953929Z"
        },
        "trusted": true,
        "id": "yndZgUJb07B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Restart Colab runtime before executing this\n",
        "# import sys\n",
        "# sys.path = ['', '/env/python', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7',\n",
        "#             '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages']\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:31:59.485788Z",
          "iopub.execute_input": "2024-08-07T10:31:59.486229Z",
          "iopub.status.idle": "2024-08-07T10:31:59.493133Z",
          "shell.execute_reply.started": "2024-08-07T10:31:59.486187Z",
          "shell.execute_reply": "2024-08-07T10:31:59.491782Z"
        },
        "trusted": true,
        "id": "dBixosPD07B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvcc --version"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:40:17.956131Z",
          "iopub.execute_input": "2024-08-07T10:40:17.956393Z",
          "iopub.status.idle": "2024-08-07T10:40:19.035924Z",
          "shell.execute_reply.started": "2024-08-07T10:40:17.956372Z",
          "shell.execute_reply": "2024-08-07T10:40:19.034409Z"
        },
        "trusted": true,
        "id": "Robo2gS007B-",
        "outputId": "0dbf1074-ed00-4a97-9e05-aa502dc03bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\nCuda compilation tools, release 12.1, V12.1.105\nBuild cuda_12.1.r12.1/compiler.32688072_0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q torch fbgemm-gpu torchrec"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:41:25.369683Z",
          "iopub.execute_input": "2024-08-07T10:41:25.370085Z",
          "iopub.status.idle": "2024-08-07T10:43:00.029367Z",
          "shell.execute_reply.started": "2024-08-07T10:41:25.370052Z",
          "shell.execute_reply": "2024-08-07T10:43:00.028081Z"
        },
        "trusted": true,
        "id": "kGgso9eM07B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import torchrec\n",
        "# import torch.distributed as dist\n",
        "\n",
        "# os.environ[\"RANK\"] = \"0\"\n",
        "# os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "# os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "# os.environ[\"MASTER_PORT\"] = \"29500\"\n",
        "\n",
        "# dist.init_process_group(backend=\"gloo\")\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:45:18.019779Z",
          "iopub.execute_input": "2024-08-07T10:45:18.020775Z",
          "iopub.status.idle": "2024-08-07T10:45:18.0261Z",
          "shell.execute_reply.started": "2024-08-07T10:45:18.020734Z",
          "shell.execute_reply": "2024-08-07T10:45:18.025Z"
        },
        "trusted": true,
        "id": "U3xYTD3C07CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Torchrec to retrieve embeddings from a DistributedParallelModel using the KJT minibatch\n",
        "\n",
        "https://colab.research.google.com/gist/dhruvrnaik/2acd7289df0885184e7e96d38eb153e7/torchrec-on-movie-rating-dataset.ipynb"
      ],
      "metadata": {
        "id": "XrfeJERS07CB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset : https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset\n",
        "\n",
        "Source : https://www.kaggle.com/code/jamesloy/deep-learning-based-recommender-systems#Deep-Learning-based-Recommender-System\n",
        "\n",
        "Source : https://www.kaggle.com/code/willkoehrsen/neural-network-embedding-recommendation-system#Introduction:-Book-Recommendation-System"
      ],
      "metadata": {
        "id": "p9kslss-07CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "np.random.seed(123)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:53:52.829282Z",
          "iopub.execute_input": "2024-08-07T10:53:52.830164Z",
          "iopub.status.idle": "2024-08-07T10:53:56.719424Z",
          "shell.execute_reply.started": "2024-08-07T10:53:52.830127Z",
          "shell.execute_reply": "2024-08-07T10:53:56.718059Z"
        },
        "trusted": true,
        "id": "AcTZ842107CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import os\n",
        "\n",
        "# Function to check memory usage\n",
        "def memory_usage_of_df(df):\n",
        "    return df.memory_usage(deep=True).sum() / (1024**2)  # Memory usage in MB\n",
        "\n",
        "# Display memory usage of the DataFrame\n",
        "def print_memory_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_info = process.memory_info()\n",
        "    print(f\"Memory usage: {mem_info.rss / (1024**2):.2f} MB\")\n",
        "\n",
        "# Load data in chunks and sample userIds\n",
        "chunk_size = 10**6  # Adjust chunk size according to system's memory capacity\n",
        "user_ids = set()\n",
        "\n",
        "print(\"Memory usage before loading user IDs:\")\n",
        "print_memory_usage()\n",
        "\n",
        "# Iterate through chunks to collect unique user IDs\n",
        "for chunk in pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv',\n",
        "                         chunksize=chunk_size, usecols=['userId']):\n",
        "    user_ids.update(chunk['userId'].unique())\n",
        "\n",
        "# Sample 10% of unique userIds\n",
        "user_ids = list(user_ids)\n",
        "sampled_user_ids = np.random.choice(user_ids, size=int(len(user_ids) * 0.1), replace=False)\n",
        "\n",
        "print(\"Memory usage after sampling user IDs:\")\n",
        "print_memory_usage()\n",
        "\n",
        "# Load data again, this time only for sampled userIds\n",
        "ratings = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv',\n",
        "                      parse_dates=['timestamp'],\n",
        "                      chunksize=chunk_size)\n",
        "\n",
        "# Filter data based on sampled userIds and concatenate chunks\n",
        "filtered_chunks = []\n",
        "\n",
        "print(\"Memory usage before filtering data:\")\n",
        "print_memory_usage()\n",
        "\n",
        "for chunk in ratings:\n",
        "    filtered_chunk = chunk[chunk['userId'].isin(sampled_user_ids)]\n",
        "    filtered_chunks.append(filtered_chunk)\n",
        "\n",
        "# Combine all chunks into a single DataFrame\n",
        "filtered_ratings = pd.concat(filtered_chunks, ignore_index=True)\n",
        "\n",
        "print(\"Memory usage after concatenating filtered data:\")\n",
        "print_memory_usage()\n",
        "\n",
        "# Print the number of rows and unique users\n",
        "print(f'There are {len(filtered_ratings)} rows of data from {len(sampled_user_ids)} users')\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(filtered_ratings.head())\n",
        "\n",
        "# Memory usage of the final DataFrame\n",
        "print(f\"Memory usage of filtered_ratings DataFrame: {memory_usage_of_df(filtered_ratings):.2f} MB\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:54:02.438131Z",
          "iopub.execute_input": "2024-08-07T10:54:02.438688Z",
          "iopub.status.idle": "2024-08-07T10:54:49.721218Z",
          "shell.execute_reply.started": "2024-08-07T10:54:02.438654Z",
          "shell.execute_reply": "2024-08-07T10:54:49.720066Z"
        },
        "trusted": true,
        "id": "VO9j3hbZ07CP",
        "outputId": "6e9dc9f7-ef47-4764-e213-0d65914a1db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Memory usage before loading user IDs:\nMemory usage: 322.25 MB\nMemory usage after sampling user IDs:\nMemory usage: 356.51 MB\nMemory usage before filtering data:\nMemory usage: 357.43 MB\nMemory usage after concatenating filtered data:\nMemory usage: 532.67 MB\nThere are 2030571 rows of data from 13849 users\n   userId  movieId  rating           timestamp\n0       3        1     4.0 1999-12-11 13:36:47\n1       3       24     3.0 1999-12-14 12:54:08\n2       3       32     4.0 1999-12-11 13:14:07\n3       3       50     5.0 1999-12-11 13:13:38\n4       3      160     3.0 1999-12-14 12:54:08\nMemory usage of filtered_ratings DataFrame: 61.97 MB\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = filtered_ratings.__deepcopy__()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:54:56.878719Z",
          "iopub.execute_input": "2024-08-07T10:54:56.879154Z",
          "iopub.status.idle": "2024-08-07T10:54:56.918713Z",
          "shell.execute_reply.started": "2024-08-07T10:54:56.879121Z",
          "shell.execute_reply": "2024-08-07T10:54:56.91744Z"
        },
        "trusted": true,
        "id": "op8QirfQ07CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-test split\n",
        "\n",
        "To simulate a time-based split where earlier ratings are used for training and the latest ratings are used for testing or validation - For each user, the most recent review is used as the test set (i.e. leave one out), while the rest will be used as training data .**For each user, the most recent review is used as the test set (i.e. leave one out), while the rest will be used as training data .**\n",
        "\n",
        "## Converting the dataset into an implicit feedback dataset\n",
        "\n",
        "As discussed earlier, we will train a recommender system using implicit feedback. However, the MovieLens dataset that we're using is based on explicit feedback. To convert this dataset into an implicit feedback dataset, we'll simply binarize the ratings such that they are are '1' (i.e. positive class). **The value of '1' represents that the user has interacted with the item.**\n",
        "\n",
        "It is important to note that using implicit feedback reframes the problem that our recommender is trying to solve. **Instead of trying to predict movie ratings (when using explicit feedback), we are trying to predict whether the user will interact (i.e. click/buy/watch) with each movie, with the aim of presenting to users the movies with the highest interaction likelihood.**\n"
      ],
      "metadata": {
        "id": "QafMZ2Xq07CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
        "\n",
        "train_ratings = ratings[ratings['rank_latest'] != 1]\n",
        "test_ratings = ratings[ratings['rank_latest'] == 1]\n",
        "\n",
        "# drop columns that we no longer need\n",
        "train_ratings = train_ratings[['userId', 'movieId', 'rating']]\n",
        "test_ratings = test_ratings[['userId', 'movieId', 'rating']]\n",
        "\n",
        "train_ratings.loc[:, 'rating'] = 1\n",
        "\n",
        "train_ratings.sample(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T10:55:02.161087Z",
          "iopub.execute_input": "2024-08-07T10:55:02.162389Z",
          "iopub.status.idle": "2024-08-07T10:55:02.819078Z",
          "shell.execute_reply.started": "2024-08-07T10:55:02.162335Z",
          "shell.execute_reply": "2024-08-07T10:55:02.817969Z"
        },
        "trusted": true,
        "id": "tl_y0oL007CU",
        "outputId": "15e89588-3333-4f87-e37a-797c0656ef21"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "         userId  movieId  rating\n287750    19107     1466     1.0\n1696350  115574     6127     1.0\n1545058  105607      422     1.0\n294224    19624       28     1.0\n351198    23188     1225     1.0",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>287750</th>\n      <td>19107</td>\n      <td>1466</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1696350</th>\n      <td>115574</td>\n      <td>6127</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1545058</th>\n      <td>105607</td>\n      <td>422</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>294224</th>\n      <td>19624</td>\n      <td>28</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>351198</th>\n      <td>23188</td>\n      <td>1225</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We do have a problem now though. After binarizing our dataset, we see that every sample in the dataset now belongs to the positive class. However we also require negative samples to train our models, to indicate movies that the user has not interacted with. We assume that such movies are those that the user are not interested in - even though this is a sweeping assumption that may not be true, it usually works out rather well in practice.\n",
        "\n",
        "The code below generates 4 negative samples for each row of data. In other words, the ratio of negative to positive samples is 4:1."
      ],
      "metadata": {
        "id": "l-APMiAX07CW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ratings.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T11:08:51.963105Z",
          "iopub.execute_input": "2024-08-07T11:08:51.96355Z",
          "iopub.status.idle": "2024-08-07T11:08:51.978323Z",
          "shell.execute_reply.started": "2024-08-07T11:08:51.963517Z",
          "shell.execute_reply": "2024-08-07T11:08:51.976875Z"
        },
        "trusted": true,
        "id": "tXFyZ1kU07CW",
        "outputId": "3cdbe32f-1074-4ee0-9992-0c473abff2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "     userId  movieId  rating\n5         3      173     2.0\n458      11     5971     5.0\n704      22      303     3.0\n849      30     6378     2.5\n874      36    58293     3.5",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId</th>\n      <th>movieId</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>173</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>11</td>\n      <td>5971</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>22</td>\n      <td>303</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>849</th>\n      <td>30</td>\n      <td>6378</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>874</th>\n      <td>36</td>\n      <td>58293</td>\n      <td>3.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building dataset for Torchrec"
      ],
      "metadata": {
        "id": "iNutBstD07CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find common userIds present in both dataframes\n",
        "common_user_ids = set(train_ratings[\"userId\"]).intersection(set(test_ratings[\"userId\"]))\n",
        "\n",
        "train_data = {}\n",
        "test_data = {}\n",
        "\n",
        "for user_id in tqdm(common_user_ids):\n",
        "    train_movies = train_ratings[train_ratings[\"userId\"] == user_id][\"movieId\"].tolist()\n",
        "    train_data[user_id] = train_movies\n",
        "\n",
        "    test_movies = test_ratings[test_ratings[\"userId\"] == user_id][\"movieId\"].tolist()\n",
        "    test_data[user_id] = test_movies\n",
        "\n",
        "print(len(train_data))\n",
        "i = 0\n",
        "for k,v in test_data.items():\n",
        "    print(k,v)\n",
        "    i+=1\n",
        "    if i==10: break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T11:14:01.893429Z",
          "iopub.execute_input": "2024-08-07T11:14:01.893871Z",
          "iopub.status.idle": "2024-08-07T11:14:43.702567Z",
          "shell.execute_reply.started": "2024-08-07T11:14:01.89384Z",
          "shell.execute_reply": "2024-08-07T11:14:43.701139Z"
        },
        "trusted": true,
        "id": "HUjXSgfp07CX",
        "outputId": "93f517bf-87c2-4509-bb24-f53cd7c7206e",
        "colab": {
          "referenced_widgets": [
            "ab9774a756454ef7bfa81ee0ee63fe37"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/13849 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab9774a756454ef7bfa81ee0ee63fe37"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "13849\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T11:18:36.651056Z",
          "iopub.execute_input": "2024-08-07T11:18:36.651456Z",
          "iopub.status.idle": "2024-08-07T11:18:36.65937Z",
          "shell.execute_reply.started": "2024-08-07T11:18:36.651426Z",
          "shell.execute_reply": "2024-08-07T11:18:36.657773Z"
        },
        "trusted": true,
        "id": "MKvoUMlA07CY",
        "outputId": "c25e2244-cce5-45f0-dbad-8cdc87cb24f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "65537 [1214]\n32771 [54281]\n3 [173]\n32773 [12]\n32774 [5]\n131082 [4996]\n11 [5971]\n65556 [3238]\n98324 [88672]\n22 [303]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_collection = torchrec.EmbeddingBagCollection(\n",
        "#     device=\"meta\",\n",
        "#     tables=[\n",
        "#         torchrec.EmbeddingBagConfig(\n",
        "#             name=\"userId_table\",\n",
        "#             embedding_dim=64,\n",
        "#             num_embeddings=MAX_USERS,\n",
        "#             feature_names=[\"userId\"],\n",
        "#             pooling=torchrec.PoolingType.SUM,\n",
        "#         ),\n",
        "#         torchrec.EmbeddingBagConfig(\n",
        "#             name=\"movieId_table\",\n",
        "#             embedding_dim=64,\n",
        "#             num_embeddings=len(movie_encoder.classes_),\n",
        "#             feature_names=[\"movieId\"],\n",
        "#             pooling=torchrec.PoolingType.SUM,\n",
        "#         )\n",
        "#     ]\n",
        "# )"
      ],
      "metadata": {
        "id": "MyfLxQS407CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torchrec.distributed.DistributedModelParallel(embedding_collection, device=torch.device(\"cpu\"))\n",
        "# print(model)\n",
        "# print(model.plan)"
      ],
      "metadata": {
        "id": "b9bDqsYm07CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = list(train_data.items())[-3:]\n",
        "userids = []\n",
        "movies_seen = []\n",
        "for row in sample:\n",
        "    userids.append(row[0])\n",
        "    movies_seen.append(row[1])\n",
        "\n",
        "userids, movies_seen\n",
        "import copy\n",
        "values = copy.deepcopy(userids)\n",
        "lengths = [1]*len(values)\n",
        "for movie_list in movies_seen:\n",
        "    values.extend(movie_list)\n",
        "    lengths.append(len(movie_list))\n",
        "\n",
        "\n",
        "# kjt = torchrec.KeyedJaggedTensor(\n",
        "#     keys = [\"userId\",\"movieId\"],\n",
        "#     values = torch.tensor(values).cpu(),\n",
        "#     lengths = torch.tensor(lengths, dtype=torch.int64).cpu(),\n",
        "# )\n",
        "\n",
        "# print(kjt.to(torch.device(\"cpu\")))\n",
        "\n",
        "# pooled_embeddings = model(kjt)\n",
        "# print(pooled_embeddings)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-07T11:27:40.68859Z",
          "iopub.execute_input": "2024-08-07T11:27:40.689058Z",
          "iopub.status.idle": "2024-08-07T11:27:40.701504Z",
          "shell.execute_reply.started": "2024-08-07T11:27:40.689024Z",
          "shell.execute_reply": "2024-08-07T11:27:40.700036Z"
        },
        "trusted": true,
        "id": "kcvAYpIP07Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "# from multiprocessing import Pool, cpu_count\n",
        "# from functools import partial\n",
        "\n",
        "# # Convert train_ratings to a set for fast lookup\n",
        "# user_item_set = set(zip(train_ratings['userId'], train_ratings['movieId']))\n",
        "\n",
        "# # Function to generate negative samples for a single user\n",
        "# def generate_negative_samples(user, user_item_set, all_movieIds, num_negatives):\n",
        "#     negative_samples = []\n",
        "#     positives = set(train_ratings[train_ratings['userId'] == user]['movieId'])\n",
        "\n",
        "#     while len(negative_samples) < num_negatives:\n",
        "#         negative_item = np.random.choice(all_movieIds)\n",
        "#         if negative_item not in positives and (user, negative_item) not in user_item_set:\n",
        "#             negative_samples.append(negative_item)\n",
        "\n",
        "#     return [(user, neg_item, 0) for neg_item in negative_samples]\n",
        "\n",
        "# # Generate negative samples using multiprocessing\n",
        "# num_negatives = 4\n",
        "# all_movieIds = ratings['movieId'].unique()\n",
        "\n",
        "# with Pool(cpu_count()) as p:\n",
        "#     negative_samples_list = list(tqdm(p.imap(partial(generate_negative_samples,\n",
        "#                                                       user_item_set=user_item_set,\n",
        "#                                                       all_movieIds=all_movieIds,\n",
        "#                                                       num_negatives=num_negatives),\n",
        "#                                               set(train_ratings['userId'])), total=len(set(train_ratings['userId']))))\n",
        "\n",
        "# # Flatten negative_samples_list\n",
        "# negative_samples = [item for sublist in negative_samples_list for item in sublist]\n",
        "\n",
        "# # Generate training data\n",
        "# train_data = list(user_item_set)\n",
        "# train_data.extend(negative_samples)\n",
        "\n",
        "# users, items, labels = zip(*train_data)\n",
        "# users = np.array(users)\n",
        "# items = np.array(items)\n",
        "# labels = np.array(labels)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T05:49:44.53858Z",
          "iopub.execute_input": "2024-07-31T05:49:44.538939Z",
          "iopub.status.idle": "2024-07-31T05:49:44.544491Z",
          "shell.execute_reply.started": "2024-07-31T05:49:44.538914Z",
          "shell.execute_reply": "2024-07-31T05:49:44.543573Z"
        },
        "trusted": true,
        "id": "W9_EIx_j07Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Collaborative Filtering (NCF)\n",
        "\n",
        "## User Embeddings\n",
        "\n",
        "In this embedding, users with similar movie preferences are placed near to each other, and vice versa.\n",
        "\n",
        "## Learned Embeddings\n",
        "\n",
        "Similarly, we will use a separate item embedding layer to represent the traits of the items (i.e. movies) in a lower dimensional space.\n",
        "\n",
        "**How can we learn the weights of the embedding layer, such that it provides an accurate representation of users and items?**\n",
        "\n",
        "> **Collaborative Filtering** - by using the ratings dataset, we can identify similar users and movies, creating user and item embeddings learned from existing ratings.\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "\n",
        "| userId | movieId | interacted |\n",
        "|--------|---------|------------|\n",
        "| 3      |  1      |   1        |\n",
        "\n",
        "The inputs to the model are the one-hot encoded user and item vector for userId = 3 and movieId = 1. The user input vector and item input vector are fed to the user embedding and item embedding respectively, which results in a smaller, denser user and item vectors.\n",
        "\n",
        "The embedded user and item vectors are concatenated before passing through a series of fully connected layers, which maps the concatenated embeddings into a prediction vector as output. Finally, we apply a Sigmoid function to obtain the most probable class. In the example above, the most probable class is 1 (positive class), since 0.8 > 0.2.\n",
        "\n",
        "![NCF](https://i.imgur.com/cNWbIce.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "4xdZWWZ807Cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MovieLensTrainDataset(Dataset):\n",
        "    \"\"\"MovieLens PyTorch Dataset for Training in PyTorch tensor format\n",
        "\n",
        "    Args:\n",
        "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
        "        all_movieIds (list): List containing all movieIds\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratings, all_movieIds):\n",
        "        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.labels[idx]\n",
        "\n",
        "    def get_dataset(self, ratings, all_movieIds):\n",
        "        users, items, labels = [], [], []\n",
        "        user_item_set = set(zip(ratings['userId'], ratings['movieId']))\n",
        "\n",
        "        num_negatives = 4\n",
        "        for u, i in tqdm(user_item_set):\n",
        "            users.append(u)\n",
        "            items.append(i)\n",
        "            labels.append(1)\n",
        "            for _ in range(num_negatives):\n",
        "                negative_item = np.random.choice(all_movieIds)\n",
        "                while (u, negative_item) in user_item_set:\n",
        "                    negative_item = np.random.choice(all_movieIds)\n",
        "                users.append(u)\n",
        "                items.append(negative_item)\n",
        "                labels.append(0)\n",
        "\n",
        "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:02:08.036247Z",
          "iopub.execute_input": "2024-07-31T07:02:08.036606Z",
          "iopub.status.idle": "2024-07-31T07:02:08.050067Z",
          "shell.execute_reply.started": "2024-07-31T07:02:08.03657Z",
          "shell.execute_reply": "2024-07-31T07:02:08.048626Z"
        },
        "trusted": true,
        "id": "jFheMWt307Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NCF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, ratings, all_movieIds):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n",
        "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n",
        "        self.fc1 = nn.Linear(in_features=16, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.output = nn.Linear(in_features=32, out_features=1)\n",
        "        self.ratings = ratings\n",
        "        self.all_movieIds = all_movieIds\n",
        "\n",
        "    def forward(self, user_input, item_input):\n",
        "        user_embedded = self.user_embedding(user_input)\n",
        "        item_embedded = self.item_embedding(item_input)\n",
        "        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
        "        vector = torch.relu(self.fc1(vector))\n",
        "        vector = torch.relu(self.fc2(vector))\n",
        "        pred = torch.sigmoid(self.output(vector))\n",
        "        return pred"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:02:08.051696Z",
          "iopub.execute_input": "2024-07-31T07:02:08.052118Z",
          "iopub.status.idle": "2024-07-31T07:02:08.066832Z",
          "shell.execute_reply.started": "2024-07-31T07:02:08.052075Z",
          "shell.execute_reply": "2024-07-31T07:02:08.065026Z"
        },
        "trusted": true,
        "id": "vlbGv7uY07Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "num_users = ratings['userId'].max() + 1\n",
        "num_items = ratings['movieId'].max() + 1\n",
        "all_movieIds = ratings['movieId'].unique()\n",
        "\n",
        "# Initialize the model\n",
        "model = NCF(num_users, num_items, ratings, all_movieIds)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:02:08.068446Z",
          "iopub.execute_input": "2024-07-31T07:02:08.068867Z",
          "iopub.status.idle": "2024-07-31T07:02:09.797227Z",
          "shell.execute_reply.started": "2024-07-31T07:02:08.068835Z",
          "shell.execute_reply": "2024-07-31T07:02:09.795843Z"
        },
        "trusted": true,
        "id": "zoeJ_qkA07Cc",
        "outputId": "7f258485-a7f9-4d5f-cccb-aec2acedc531"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "NCF(\n  (user_embedding): Embedding(138492, 8)\n  (item_embedding): Embedding(131159, 8)\n  (fc1): Linear(in_features=16, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=32, bias=True)\n  (output): Linear(in_features=32, out_features=1, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Prepare data loaders\n",
        "train_dataset = MovieLensTrainDataset(ratings, all_movieIds)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=4)\n",
        "\n",
        "# train_loader"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T05:49:46.043444Z",
          "iopub.execute_input": "2024-07-31T05:49:46.044437Z",
          "iopub.status.idle": "2024-07-31T05:53:10.84049Z",
          "shell.execute_reply.started": "2024-07-31T05:49:46.044409Z",
          "shell.execute_reply": "2024-07-31T05:53:10.83957Z"
        },
        "trusted": true,
        "id": "X71Ec3Oq07Cd",
        "outputId": "6301890a-e10a-46b2-95c2-046360e2d1c7",
        "colab": {
          "referenced_widgets": [
            "6b75da45dfcc490fa9ad2c697fe14e25"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2030571 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b75da45dfcc490fa9ad2c697fe14e25"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger('TrainingLogger')\n",
        "\n",
        "\n",
        "# Log model architecture\n",
        "def log_model_summary(model):\n",
        "    logger.info(f\"Model Architecture:\\n{model}\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    logger.info(f\"Total Parameters: {total_params / 1e6:.2f} M\")\n",
        "\n",
        "log_model_summary(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T06:42:57.824534Z",
          "iopub.execute_input": "2024-07-31T06:42:57.82517Z",
          "iopub.status.idle": "2024-07-31T06:42:57.833344Z",
          "shell.execute_reply.started": "2024-07-31T06:42:57.825123Z",
          "shell.execute_reply": "2024-07-31T06:42:57.831873Z"
        },
        "trusted": true,
        "id": "68XewCh707Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Create a new DataLoader instance for the current epoch\n",
        "    train_loader = DataLoader(MovieLensTrainDataset(ratings, all_movieIds), batch_size=512, shuffle=True, num_workers=4)\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for user_input, item_input, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        user_input, item_input, labels = user_input.to(device), item_input.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(user_input, item_input)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, labels.view(-1, 1).float())\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * user_input.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "    logger.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T05:58:58.985073Z",
          "iopub.execute_input": "2024-07-31T05:58:58.98597Z",
          "iopub.status.idle": "2024-07-31T06:15:05.268824Z",
          "shell.execute_reply.started": "2024-07-31T05:58:58.985936Z",
          "shell.execute_reply": "2024-07-31T06:15:05.267753Z"
        },
        "trusted": true,
        "id": "CN8lLnBj07Ce",
        "outputId": "6843e79b-e12d-494f-928c-4b58c6bbc662",
        "colab": {
          "referenced_widgets": [
            "e5b1d87439ba4a41bf2138e92aaa3a0b",
            "838cc9b6c1ce4e28a7611aa3b615a6ba",
            "c73ed20f33384e30841cb7602337c63b",
            "eae554e1fd434051b6d0cd0d867940e3",
            "78a5376673354d5c80afa63a953c5e41",
            "627394dc443f4c30b7cf603aeea560be"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2030571 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5b1d87439ba4a41bf2138e92aaa3a0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Epoch 1/3:   0%|          | 0/19830 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "838cc9b6c1ce4e28a7611aa3b615a6ba"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 1/3, Loss: 0.2393\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2030571 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c73ed20f33384e30841cb7602337c63b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Epoch 2/3:   0%|          | 0/19830 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eae554e1fd434051b6d0cd0d867940e3"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 2/3, Loss: 0.2329\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/2030571 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78a5376673354d5c80afa63a953c5e41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Epoch 3/3:   0%|          | 0/19830 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "627394dc443f4c30b7cf603aeea560be"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Epoch 3/3, Loss: 0.2285\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model checkpoint\n",
        "torch.save(model.state_dict(), 'ncf_model.pth')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T06:15:05.2712Z",
          "iopub.execute_input": "2024-07-31T06:15:05.27161Z",
          "iopub.status.idle": "2024-07-31T06:15:05.296097Z",
          "shell.execute_reply.started": "2024-07-31T06:15:05.27157Z",
          "shell.execute_reply": "2024-07-31T06:15:05.295192Z"
        },
        "trusted": true,
        "id": "WRb6j9Zr07Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Recommender System\n",
        "\n",
        "The key here is that we don't need the user to interact on every single item in the list of recommendations. Instead, we just need the user to interact with at least one item on the list - as long as the user does that, the recommendations have worked.\n",
        "\n",
        "To simulate this, let's run the following evaluation protocol to generate a list of 10 recommended items for each user.\n",
        "\n",
        "        For each user, randomly select 99 items that the user has not interacted with\n",
        "        Combine these 99 items with the test item (the actual item that the user interacted with). We now have 100 items.\n",
        "        Run the model on these 100 items, and rank them according to their predicted probabilities\n",
        "        Select the top 10 items from the list of 100 items. If the test item is present within the top 10 items, then we say that this is a hit.\n",
        "        Repeat the process for all users. The Hit Ratio is then the average hits.\n",
        "        This evaluation protocol is known as Hit Ratio @ 10, and it is commonly used to evaluate recommender systems.\n",
        "\n",
        "\n",
        "**Hit Ratio @ 10 - What % of the users were recommended the actual item (among a list of 10 items) that they eventually interacted with**\n"
      ],
      "metadata": {
        "id": "ULw0X62i07Cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model checkpoint\n",
        "model.load_state_dict(torch.load('/kaggle/input/d-recommendation/pytorch/v.1.0/1/ncf_model.pth',map_location=torch.device('cpu')))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:02:09.79876Z",
          "iopub.execute_input": "2024-07-31T07:02:09.799385Z",
          "iopub.status.idle": "2024-07-31T07:02:09.953665Z",
          "shell.execute_reply.started": "2024-07-31T07:02:09.799349Z",
          "shell.execute_reply": "2024-07-31T07:02:09.952476Z"
        },
        "trusted": true,
        "id": "WlRqnZ4007Cg",
        "outputId": "f504152a-704b-4062-d2f9-cf5b1e1f8b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "NCF(\n  (user_embedding): Embedding(138492, 8)\n  (item_embedding): Embedding(131159, 8)\n  (fc1): Linear(in_features=16, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=32, bias=True)\n  (output): Linear(in_features=32, out_features=1, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Convert test ratings to a set of user-item pairs\n",
        "test_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))\n",
        "\n",
        "# Dict of all items interacted with by each user\n",
        "user_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()\n",
        "\n",
        "hits = []\n",
        "for (u, i) in tqdm(test_user_item_set):\n",
        "    # Get the items interacted with by user u\n",
        "    interacted_items = user_interacted_items.get(u, [])\n",
        "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
        "\n",
        "    # Randomly sample 99 non-interacted items\n",
        "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99, replace=False))\n",
        "    test_items = selected_not_interacted + [i]\n",
        "\n",
        "    # Prepare inputs for the model\n",
        "    user_tensor = torch.tensor([u] * len(test_items)).to(device)\n",
        "    item_tensor = torch.tensor(test_items).to(device)\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        predicted_labels = model(user_tensor, item_tensor).cpu().numpy().flatten()\n",
        "\n",
        "    # Get top 10 items based on predicted labels\n",
        "    top10_items = [test_items[j] for j in np.argsort(predicted_labels)[::-1][:10]]\n",
        "\n",
        "    # Check if the true item is in the top 10 recommendations\n",
        "    hits.append(1 if i in top10_items else 0)\n",
        "\n",
        "#     print(f\"User didn't interacted but recommended : \",[x for x in top10_items if x not in hits])\n",
        "\n",
        "# Calculate and print Hit Ratio @ 10\n",
        "hit_ratio_at_10 = np.mean(hits)\n",
        "print(f\"The Hit Ratio @ 10 is {hit_ratio_at_10:.2f}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T06:43:02.976948Z",
          "iopub.execute_input": "2024-07-31T06:43:02.977907Z",
          "iopub.status.idle": "2024-07-31T06:45:06.672226Z",
          "shell.execute_reply.started": "2024-07-31T06:43:02.977863Z",
          "shell.execute_reply": "2024-07-31T06:45:06.670942Z"
        },
        "trusted": true,
        "id": "jH_wkVLU07Ch",
        "outputId": "3ff043f4-65e1-46e8-e299-faef05926107",
        "colab": {
          "referenced_widgets": [
            "fc4c7f5d928841e68fbe7e59341fe131"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/13849 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc4c7f5d928841e68fbe7e59341fe131"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "The Hit Ratio @ 10 is 0.77\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get similar movie recommendation"
      ],
      "metadata": {
        "id": "ChkBSgR-07Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_item_embeddings(model):\n",
        "    \"\"\"Extract item embeddings from the model.\"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        item_embeddings = model.item_embedding.weight.cpu().numpy()\n",
        "\n",
        "    return item_embeddings\n",
        "\n",
        "# Get item embeddings\n",
        "item_embeddings = get_item_embeddings(model)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:02:16.877715Z",
          "iopub.execute_input": "2024-07-31T07:02:16.878212Z",
          "iopub.status.idle": "2024-07-31T07:02:16.885701Z",
          "shell.execute_reply.started": "2024-07-31T07:02:16.878177Z",
          "shell.execute_reply": "2024-07-31T07:02:16.884273Z"
        },
        "trusted": true,
        "id": "P4TdqUR607Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approximate Nearest Neighbour Search"
      ],
      "metadata": {
        "id": "2ucfDxlI07Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU faiss-cpu"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:02:20.52695Z",
          "iopub.execute_input": "2024-07-31T07:02:20.527428Z",
          "iopub.status.idle": "2024-07-31T07:02:38.285361Z",
          "shell.execute_reply.started": "2024-07-31T07:02:20.527393Z",
          "shell.execute_reply": "2024-07-31T07:02:38.283765Z"
        },
        "trusted": true,
        "id": "BP0GkPNA07Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "def compute_similarities_faiss(item_embeddings, k=10):\n",
        "    \"\"\"Compute approximate nearest neighbors using Faiss.\"\"\"\n",
        "    item_embeddings = np.array(item_embeddings, dtype=np.float32)\n",
        "    dim = item_embeddings.shape[1]\n",
        "\n",
        "    # Create a Faiss index\n",
        "    index = faiss.IndexFlatL2(dim)  # L2 distance\n",
        "    index.add(item_embeddings)  # Add vectors to index\n",
        "\n",
        "    # Query the index for k-nearest neighbors\n",
        "    distances, indices = index.search(item_embeddings, k + 1)  # k + 1 to exclude the item itself\n",
        "    return distances, indices\n",
        "\n",
        "# Compute approximate nearest neighbors with Faiss\n",
        "k = 10\n",
        "distances, indices = compute_similarities_faiss(item_embeddings, k=k)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:02:46.56679Z",
          "iopub.execute_input": "2024-07-31T07:02:46.567306Z",
          "iopub.status.idle": "2024-07-31T07:03:15.884096Z",
          "shell.execute_reply.started": "2024-07-31T07:02:46.567265Z",
          "shell.execute_reply": "2024-07-31T07:03:15.882721Z"
        },
        "trusted": true,
        "id": "3rkZrx1Q07Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# def compute_similarities(item_embeddings):\n",
        "#     \"\"\"Compute similarity matrix from item embeddings.\"\"\"\n",
        "#     similarity_matrix = cosine_similarity(item_embeddings)\n",
        "#     return similarity_matrix\n",
        "\n",
        "# # Compute similarity matrix\n",
        "# similarity_matrix = compute_similarities(item_embeddings)"
      ],
      "metadata": {
        "id": "waAT6Npw07Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_similar_movies(movie_id, movie_ids, distances, indices, k=10):\n",
        "    \"\"\"Recommend similar movies to a given movie.\"\"\"\n",
        "    if movie_id not in movie_ids:\n",
        "        raise ValueError(f\"Movie ID {movie_id} is not in the movie IDs list.\")\n",
        "\n",
        "    # Get the index of the movie\n",
        "    movie_index = movie_ids.index(movie_id)\n",
        "\n",
        "    # Get indices of the top N most similar movies\n",
        "    top_indices = indices[movie_index][1:]  # Exclude the movie itself\n",
        "    top_indices = top_indices[:k]  # Ensure we return exactly top_k items\n",
        "\n",
        "    # Get movie IDs of the top similar movies\n",
        "    similar_movies = [movie_ids[i] for i in top_indices]\n",
        "\n",
        "    return similar_movies\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:03:38.073174Z",
          "iopub.execute_input": "2024-07-31T07:03:38.073656Z",
          "iopub.status.idle": "2024-07-31T07:03:38.081682Z",
          "shell.execute_reply.started": "2024-07-31T07:03:38.073616Z",
          "shell.execute_reply": "2024-07-31T07:03:38.080277Z"
        },
        "trusted": true,
        "id": "cmkaqCQi07Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "movie_ids = list(range(len(item_embeddings)))  # Create a list of movie IDs (adjust as needed)\n",
        "movie_id_to_recommend = 1  # Example movie ID\n",
        "\n",
        "# Get similar movies\n",
        "similar_movies = recommend_similar_movies(movie_id_to_recommend, movie_ids, distances, indices, k=10)\n",
        "\n",
        "print(f\"Movies similar to movie ID {movie_id_to_recommend}: {similar_movies}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:03:40.768091Z",
          "iopub.execute_input": "2024-07-31T07:03:40.768538Z",
          "iopub.status.idle": "2024-07-31T07:03:40.784315Z",
          "shell.execute_reply.started": "2024-07-31T07:03:40.768501Z",
          "shell.execute_reply": "2024-07-31T07:03:40.782935Z"
        },
        "trusted": true,
        "id": "-64AvJbN07Ct",
        "outputId": "8d01ebb4-a8d6-454d-bec2-097732fdd6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Movies similar to movie ID 1: [377, 29742, 173, 1198, 44129, 1197, 1240, 91078, 90331, 42945]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "movie_ids = list(range(len(item_embeddings)))  # Create a list of movie IDs (adjust as needed)\n",
        "movie_id_to_recommend = 3  # Example movie ID\n",
        "\n",
        "# Get similar movies\n",
        "similar_movies = recommend_similar_movies(movie_id_to_recommend, movie_ids, distances, indices, k=10)\n",
        "\n",
        "print(f\"Movies similar to movie ID {movie_id_to_recommend}: {similar_movies}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T07:04:11.691797Z",
          "iopub.execute_input": "2024-07-31T07:04:11.692248Z",
          "iopub.status.idle": "2024-07-31T07:04:11.706054Z",
          "shell.execute_reply.started": "2024-07-31T07:04:11.692215Z",
          "shell.execute_reply": "2024-07-31T07:04:11.704529Z"
        },
        "trusted": true,
        "id": "SQTO3abu07Cu",
        "outputId": "c40ecb65-0171-41b3-da22-f5d49636868c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Movies similar to movie ID 3: [71469, 76370, 91984, 22908, 126212, 112703, 96427, 46499, 88673, 130188]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "Y69NaAM907Cu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}