{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "LLM-Notes-1",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "y_1A4DU9wG0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparseAttention\n",
        "https://github.com/kyegomez/SparseAttention\n",
        "\n",
        "\n",
        "blocksparse_attention_impl\n",
        "-----------------------------------------------------------------------\n",
        "Transpose for Strided Attention: -> strided_transpose()\n",
        "\n",
        "1. If attn_mode == 'strided', the function first applies strided_transpose to tensors q, k, and v. This function reshapes and transposes the tensors to prepare them for a strided attention pattern. This step reorganizes the tensors into blocks and local contexts, as discussed earlier.\n",
        "Compute Attention Weights:\n",
        "\n",
        "2. Calculate attention weights w using matrix multiplication of q and k, followed by softmax normalization. This step computes how much each token (query) should attend to each other token (key).\n",
        "\n",
        "\n",
        "3. Compute the attended output a by multiplying the attention weights w with the value tensor v.\n",
        "Reverse Transpose (if Strided Attention):\n",
        "\n",
        "\n",
        "Purpose:\n",
        "The purpose of strided_transpose is to rearrange the tensor x to facilitate a specific attention pattern\n",
        "known as strided attention. In strided attention, attention is applied sparsely across the sequence by skipping\n",
        "certain tokens, hence reducing the computational complexity while still capturing long-range dependencies.\n",
        "\n",
        "strided_transpose\n",
        "-----------------------------------------------\n",
        "1. bT_ctx is computed as n_ctx // local_attn_ctx. This determines how many blocks (local_attn_ctx) fit into the\n",
        "total context length (n_ctx).\n",
        "\n",
        "2. x = torch.reshape(x, [n, bT_ctx, local_attn_ctx, embd]): Reshapes the tensor x to split the sequence (t) into\n",
        "blocks (bT_ctx) of local context size (local_attn_ctx).\n",
        "Transpose dimensions:\n",
        "\n",
        "3. x = torch.transpose(x, 0, 2, 1, 3): Transposes the tensor dimensions. This step likely reorders the tensor to\n",
        "prepare it for the desired strided attention pattern.\n",
        "\n",
        "4. x = torch.reshape(x, [n, t, embd]): Reshapes the tensor back to its original shape after applying the desired\n",
        "transposition.\n",
        "\n",
        "Usage in SparseAttention (atten_mode = \"stride\"):\n",
        "In the context of SparseAttention with atten_mode = \"stride\", this function would typically be used to\n",
        "preprocess the input tensor x before performing the actual attention computation. Strided attention patterns are\n",
        "beneficial when dealing with long sequences because they reduce the computational cost of attending to all tokens,\n",
        "while still allowing tokens to attend to each other at varying distances.\n",
        "\n",
        "\n",
        "get_attn_mask\n",
        "---------------------------------------------------------------------------------------------\n",
        "n: Number of time steps or sequence length, determining the size of the mask.\n",
        "attn_mode: Specifies the type of attention mask to generate ('all', 'local', 'strided').\n",
        "\n",
        "\n",
        "1. 'all' Mode: Creates a lower triangular matrix where all positions below the main diagonal\n",
        "\tare set to 1 (torch.tril(torch.ones([n, n]))).\n",
        "2. 'local' Mode: Limits the attention range to a specified local_attn_ctx by setting all positions beyond\n",
        "\tctx (bandwidth) below the diagonal to 0.\n",
        "3. 'strided' Mode: Implements a strided attention pattern:\n",
        "\t1. Constructs tensors q and k using torch.arange and torch.transpose to create matrices of indices.\n",
        "\t2. Checks conditions (c1, c2) to determine which elements should be attended to based on a stride\n",
        "\t\tcondition (local_attn_ctx).\n",
        "\t3. Combines conditions to form a binary mask b where 1 indicates allowed attention and 0 indicates\n",
        "\t\tforbidden attention.\n",
        "4. Reshaping: Reshapes the mask tensor b to [1, 1, n, n] to align with the dimensions expected by the attention mechanism.\n",
        "\n",
        "attention_impl\n",
        "-------------------------------------------------------------------------------------------\n",
        "1. Split Heads: Splits the query, key, and value tensors into multiple heads using the split_heads\n",
        "function : (batch, pixel, state) -> (batch, pixel, head, head_state) , here m= head_state = head//x.size()[-1]\n",
        "\n",
        "2. Get Mask: Generates the attention mask (mask) using get_attn_mask.\n",
        "\n",
        "3. Compute Attention Weights (w):\n",
        "\t1. Calculates the attention scores by computing the dot product of queries and keys\n",
        "\t\t(torch.matmul(q, k.transpose(-2, -1))).\n",
        "\t2. Scales the scores by scale_amount (typically 1 / sqrt(d_k) where d_k is the dimension of queries or keys).\n",
        "\t3. Applies the attention mask (w = w * mask + -1e9 * (1 - mask)) to zero out forbidden attention weights\n",
        "\t\t(ensures softmax does not attend to masked positions).\n",
        "\t4. Applies softmax to compute normalized attention weights (F.softmax(w, dim=-1)).\n",
        "4. Compute Attended Output (a):\n",
        "\t1. Uses the attention weights w to compute the weighted sum of values (torch.matmul(w, v)).\n",
        "\t2. Merge Heads: Merges the output of multiple heads back into a single tensor using the merge_heads\n",
        "\t\tfunction (not provided)."
      ],
      "metadata": {
        "id": "9K0MmYLswG0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlashAttention\n",
        "\n",
        "https://github.com/kyegomez/FlashAttention20\n",
        "\n",
        "\n",
        "\n",
        "### `FlashAttentionFunction` Class (Custom Autograd Function)\n",
        "\n",
        "### Purpose\n",
        "\n",
        "The `forward` method computes the forward pass of the FlashAttention mechanism. It performs attention calculation between query (`q`), key (`k`), and value (`v`) tensors, handles chunking of these tensors to manage memory efficiently, applies attention masks, supports causal attention, and computes output tensors (`o`) along with necessary intermediate values.\n",
        "\n",
        "### Breakdown of the `forward` Method\n",
        "\n",
        "\n",
        "- **Parameters:**\n",
        "  - `q`, `k`, `v`: Query, key, and value tensors.\n",
        "  - `mask`: Attention mask.\n",
        "  - `causal`: Boolean flag indicating if the attention is causal (only attends to previous positions).\n",
        "  - `q_bucket_size`, `k_bucket_size`: Sizes of query and key buckets for chunking.\n",
        "\n",
        "1. **Initialization and Setup:**\n",
        "   - **Device and Data Type Handling:** Determines the device of the input query tensor `q` (`device = q.device`) and computes the maximum negative value (`max_neg_value`) for masking.\n",
        "   - **Dimension and Scaling:** Computes the length difference between `k` and `q` (`qk_len_diff`) to adjust for variable lengths in attention matrices. Sets up tensors (`o`, `all_row_sums`, `all_row_maxes`) for output, row-wise sums, and maximum values.\n",
        "\n",
        "2. **Mask Handling:**\n",
        "   - **Existence Check and Rearrangement:** Checks if `mask` exists and reshapes it if it has 2 dimensions (`mask.ndim == 2`). If `mask` does not exist, initializes `col_masks` and repeats them based on `num_row_tiles` and `num_col_tiles`.\n",
        "\n",
        "3. **Chunking and Iteration:**\n",
        "   - **Row and Column Splits:** Divides the query (`q`), output (`o`), mask (`row_mask`), row sums (`row_sums`), and row maxes (`row_maxes`) into chunks (`q.split(...)`, `o.split(...)`, `mask`, etc.).\n",
        "   - **Nested Loops:** Iterates over each chunk of queries (`qc`) and corresponding masks (`row_mask`), then within each chunk, iterates over chunks of keys (`kc`) and values (`vc`).\n",
        "\n",
        "4. **Attention Calculation:**\n",
        "   - **Matrix Multiplication and Scaling:** Computes attention weights (`attn_weights`) using Einstein summation (`einsum`) of queries (`qc`) and keys (`kc`), scaled by `scale`.\n",
        "\n",
        "5. **Mask Application:**\n",
        "   - **Column Masking:** Applies column-wise mask (`col_mask`) to attention weights (`attn_weights`), filling masked positions with `max_neg_value` to ignore those positions during softmax calculation.\n",
        "\n",
        "6. **Causal Attention Handling:**\n",
        "   - **Causal Masking:** If `causal` is `True`, applies a causal mask (`causal_mask`) to `attn_weights` to ensure the model only attends to previous positions.\n",
        "\n",
        "7. **Normalization and Update:**\n",
        "   - **Row-wise Operations:** Computes block-wise maximum (`block_row_maxes`) and updates (`new_row_maxes`), exponentiates attention weights (`exp_weights`), and sums them (`block_row_sums`). Computes exponential row differences (`exp_row_max_diff`), updates row sums (`new_row_sums`), and updates output (`oc`) accordingly.\n",
        "\n",
        "8. **Normalization of Output:**\n",
        "   - **Output Normalization:** Normalizes `oc` by dividing by `row_sums` to get the final attention outputs for each chunk.\n",
        "\n",
        "9. **Log-Sum-Exp Calculation:**\n",
        "   - **Log-Sum-Exp (lse):** Computes log-sum-exp (`lse`) of `all_row_sums` and `all_row_maxes`, preparing them for gradient calculations.\n",
        "\n",
        "10. **Context Management:**\n",
        "    - **Context Setup:** Stores necessary parameters and tensors (`causal`, `scale`, `mask`, `q_bucket_size`, `k_bucket_size`, `q`, `k`, `v`, `o`, `lse`) in the context (`ctx`) for use during the backward pass (`backward` method).\n",
        "\n",
        "11. **Return:**\n",
        "    - **Return Output:** Returns the computed output tensor `o` representing the attended values.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The `forward` method efficiently computes the FlashAttention mechanism's forward pass, handling large tensors by chunking them (`q_bucket_size`, `k_bucket_size`), applying attention masks (`mask`), supporting causal attention (`causal`), and computing outputs (`o`) while managing memory and computational efficiency. It utilizes PyTorch tensor operations (`einsum`, `masked_fill_`, `clamp`, `exp`, etc.) and context management (`ctx`) for autograd to enable gradient computation in the `backward` pass. This approach is crucial for handling large-scale datasets and memory-intensive operations in deep learning models effectively.\n"
      ],
      "metadata": {
        "id": "DsA7ZzDPwG09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's break down the `backward` method of the `FlashAttentionFunction` class. This method implements the backward pass for the FlashAttention mechanism using PyTorch's custom autograd function.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "The `backward` method computes the gradients of the inputs (`q`, `k`, `v`) with respect to the output gradients (`do`). It utilizes the saved tensors (`q`, `k`, `v`, `o`, `lse`) and other context parameters (`causal`, `scale`, `mask`, `q_bucket_size`, `k_bucket_size`) from the forward pass to efficiently compute the gradients using backpropagation.\n",
        "\n",
        "### Breakdown of the `backward` Method\n",
        "\n",
        "\n",
        "### Detailed Explanation\n",
        "\n",
        "1. **Initialization and Setup:**\n",
        "   - **Context Retrieval:** Retrieves saved tensors (`q`, `k`, `v`, `o`, `lse`) and context parameters (`causal`, `scale`, `mask`, `q_bucket_size`, `k_bucket_size`) from the context (`ctx`).\n",
        "\n",
        "2. **Tensor Initialization:**\n",
        "   - **Gradient Initialization:** Initializes tensors `dq`, `dk`, and `dv` to zeros with the same shape as `q`, `k`, and `v`, respectively, to accumulate gradients.\n",
        "\n",
        "3. **Chunking and Iteration:**\n",
        "   - **Row Splits:** Splits `q`, `o`, `do`, `mask`, `lse`, and `dq` into chunks (`q.split(...)`, `o.split(...)`, etc.) for memory efficiency during computation.\n",
        "\n",
        "4. **Nested Loops for Gradient Computation:**\n",
        "   - **Column Splits:** Iterates over chunks of `k`, `v`, `dk`, `dv`, and `row_mask`.\n",
        "   - **Attention Calculation:** Computes attention weights (`attn_weights`) using `qc` (query chunk) and `kc` (key chunk) scaled by `scale`.\n",
        "\n",
        "5. **Mask Application:**\n",
        "   - **Causal Masking:** If `causal` is `True`, applies a causal mask (`causal_mask`) to `attn_weights` to mask out future positions.\n",
        "\n",
        "6. **Gradient Calculation:**\n",
        "   - **Probability and Gradient Calculation:** Computes probabilities (`p`) from `attn_weights` and `lse`, masks `p` with `col_mask`, and computes gradients (`dv_chunk`, `dp`, `ds`) based on attention weights, output gradients (`doc`), and values (`vc`).\n",
        "\n",
        "7. **Backpropagation Update:**\n",
        "   - **Gradient Accumulation:** Accumulates gradients (`dq_chunk`, `dk_chunk`, `dv_chunk`) computed for each chunk of `qc`, `kc`, and `vc` into `dqc`, `dkc`, and `dvc`, respectively.\n",
        "\n",
        "8. **Return Gradients:**\n",
        "   - **Return:** Returns computed gradients `dq`, `dk`, `dv`, and `None` for additional `ctx` arguments that don't require gradients.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The `backward` method efficiently computes gradients for the FlashAttention mechanism using backpropagation. It iterates over chunks of tensors (`q`, `k`, `v`, `o`, `lse`) to manage memory usage, applies masks (`mask`, `causal_mask`) to attention weights (`attn_weights`), computes gradients (`dq`, `dk`, `dv`) based on output gradients (`do`), and accumulates them into corresponding tensors. This method leverages PyTorch's tensor operations (`einsum`, `masked_fill_`, etc.) and context management (`ctx`) for efficient gradient computation in deep learning models."
      ],
      "metadata": {
        "id": "kvFi-wWuwG1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### `FlashAttention` Class (Module Wrapper)\n",
        "\n",
        "This class encapsulates the FlashAttention mechanism as a PyTorch module, providing an easy-to-use interface for integration into neural network architectures.\n",
        "\n",
        "- **Initialization (`__init__` method):**\n",
        "  - Sets up parameters for the FlashAttention mechanism, including number of heads, dimensions, bucket sizes, etc.\n",
        "  - Defines linear transformations (`to_q`, `to_kv`, `to_out`) for queries, keys, and values.\n",
        "  - Optional configurations for parallel execution (`parallel`) and mixed precision training (`mixed_precision`).\n",
        "\n",
        "- **Forward Pass (`forward` method):**\n",
        "  - Handles the forward pass of the attention mechanism.\n",
        "  - Splits input data into chunks based on `q_bucket_size`.\n",
        "  - Optionally parallelizes computation across multiple GPUs.\n",
        "  - Optionally applies mixed precision for faster computation.\n",
        "  - Utilizes `FlashAttentionFunction.apply` to compute attention and rearranges outputs.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- **Chunking:** Divides input tensors (`q`, `k`, `v`) into smaller chunks (`q_bucket_size`, `k_bucket_size`) to manage memory and computation efficiently.\n",
        "- **Causal Attention:** Supports causal attention by masking future positions during computation.\n",
        "- **Memory Efficiency:** Uses incremental updates and chunk-wise computations to handle large-scale data efficiently.\n",
        "- **Custom Autograd:** Implements custom forward and backward methods using `torch.autograd.Function` to define the FlashAttention mechanism.\n",
        "\n",
        "### Usage\n",
        "\n",
        "To use this implementation:\n",
        "\n",
        "```python\n",
        "# Instantiate FlashAttention module\n",
        "flash_attn = FlashAttention(dim=512, heads=8, dim_head=64, causal=True)\n",
        "\n",
        "# Example usage\n",
        "x = torch.randn(32, 100, 512)  # Batch size of 32, sequence length of 100, dimension of 512\n",
        "output = flash_attn(x)\n",
        "```\n",
        "\n",
        "This setup allows you to integrate the FlashAttention mechanism into your neural network architectures, particularly useful for handling large-scale datasets or scenarios requiring efficient attention mechanisms. Adjust parameters (`q_bucket_size`, `k_bucket_size`, etc.) as needed based on your specific application requirements."
      ],
      "metadata": {
        "id": "jSl1yYK0wG1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replacing existing Attention Layer with Custom Attention Layer"
      ],
      "metadata": {
        "id": "tLkgsScXwG1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import OPTForCausalLM, OPTConfig\n",
        "\n",
        "class SparseAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_dim, \"Embedding dimension must be divisible by num_heads\"\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        assert embed_dim == self.embed_dim, \"Input embedding dimension must match the module's embedding dimension\"\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.q_linear(x)  # (batch_size, seq_length, embed_dim)\n",
        "        k = self.k_linear(x)  # (batch_size, seq_length, embed_dim)\n",
        "        v = self.v_linear(x)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        k = k.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        v = v.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1))  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        # Apply local (sparse) attention mask\n",
        "        mask = self._create_local_attention_mask(seq_length)\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n",
        "\n",
        "        # Compute attention output\n",
        "        attn_output = torch.matmul(attn_probs, v)  # (batch_size, num_heads, seq_length, head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.out_linear(attn_output)  # (batch_size, seq_length, embed_dim)\n",
        "        return output\n",
        "\n",
        "    def _create_local_attention_mask(self, seq_length):\n",
        "        mask = torch.zeros(seq_length, seq_length)\n",
        "        for i in range(seq_length):\n",
        "            start = max(0, i - self.window_size // 2)\n",
        "            end = min(seq_length, i + self.window_size // 2 + 1)\n",
        "            mask[start:end, i] = 1\n",
        "        return mask.unsqueeze(0).unsqueeze(0).to(torch.bool)  # (1, 1, seq_length, seq_length)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T03:50:29.336942Z",
          "iopub.execute_input": "2024-07-25T03:50:29.337442Z",
          "iopub.status.idle": "2024-07-25T03:50:29.356851Z",
          "shell.execute_reply.started": "2024-07-25T03:50:29.337404Z",
          "shell.execute_reply": "2024-07-25T03:50:29.355193Z"
        },
        "trusted": true,
        "id": "3-qBlzzgwG1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomOPTModel(OPTForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super(CustomOPTModel, self).__init__(config)\n",
        "        self._replace_attention_layers()\n",
        "\n",
        "    def _replace_attention_layers(self):\n",
        "        for layer_name, layer_module in self.named_modules():\n",
        "            if isinstance(layer_module, nn.MultiheadAttention):\n",
        "                # Replace the self-attention layer with SparseAttention\n",
        "                setattr(self, layer_name, SparseAttention(\n",
        "                    embed_dim=layer_module.embed_dim,\n",
        "                    num_heads=layer_module.num_heads,\n",
        "                    window_size=3  # Example window size\n",
        "                ))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T03:54:14.30697Z",
          "iopub.execute_input": "2024-07-25T03:54:14.307386Z",
          "iopub.status.idle": "2024-07-25T03:54:14.316091Z",
          "shell.execute_reply.started": "2024-07-25T03:54:14.307356Z",
          "shell.execute_reply": "2024-07-25T03:54:14.314488Z"
        },
        "trusted": true,
        "id": "4c8oaskPwG1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "# Load tokenizer appropriate for your model\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
        "\n",
        "# Example input\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors='pt')\n",
        "\n",
        "# Load pre-trained model configuration and model\n",
        "config = AutoConfig.from_pretrained('facebook/opt-350m')\n",
        "model = AutoModelForCausalLM.from_pretrained('facebook/opt-350m')\n",
        "\n",
        "# Example usage\n",
        "outputs = model(**inputs)\n",
        "print(inputs.items())\n",
        "print(inputs.tokens())\n",
        "print(outputs.logits.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:08:29.385503Z",
          "iopub.execute_input": "2024-07-25T04:08:29.38595Z",
          "iopub.status.idle": "2024-07-25T04:08:30.755549Z",
          "shell.execute_reply.started": "2024-07-25T04:08:29.385916Z",
          "shell.execute_reply": "2024-07-25T04:08:30.754194Z"
        },
        "trusted": true,
        "id": "gcWgWe00wG1R",
        "outputId": "5023b038-f665-4de2-c29a-a172b2a37f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "dict_items([('input_ids', tensor([[    2, 31414,     6,   127,  2335,    16, 11962]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1, 1]]))])\n['</s>', 'Hello', ',', 'Ġmy', 'Ġdog', 'Ġis', 'Ġcute']\ntorch.Size([1, 7, 50272])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Model Structure**: The code assumes the model has a modular structure, typically found in transformer models.\n",
        "\n",
        "**Layer Access**: Accessing and replacing layers depends on the model’s architecture. For instance, in BERT, you access encoder layers through model.encoder.layer, while in GPT or OPT, you might need to adjust according to the specific layer structure.\n",
        "\n",
        "**Modification Scope**: This method can be adapted to replace other components like feed-forward layers, normalization layers, etc.\n",
        "\n",
        "**Model-Specific Adjustments**: The exact implementation may vary based on the model. For instance, GPT models use transformer.h instead of encoder.layer"
      ],
      "metadata": {
        "id": "qRYIKRonwG1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General pattern for Layer replacement"
      ],
      "metadata": {
        "id": "Z-KKNRdiwG1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from transformers import BertModel\n",
        "\n",
        "# class CustomAttention(nn.Module):\n",
        "#     def __init__(self, embed_dim, num_heads):\n",
        "#         super(CustomAttention, self).__init__()\n",
        "#         self.multihead_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "#     def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None,\n",
        "#                 encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "#         # Use the multihead_attention module to perform the attention operation\n",
        "#         return self.multihead_attention(hidden_states, hidden_states, hidden_states,\n",
        "#                                         attn_mask=attention_mask, key_padding_mask=attention_mask)[0]\n"
      ],
      "metadata": {
        "id": "QEFTmQ2mwG1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom LayerNorm + Residual connection + Attention"
      ],
      "metadata": {
        "id": "j9dBXAF3wG1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class CustomLayerNorm(nn.LayerNorm):\n",
        "    def __init__(self, normalized_shape, eps=1e-12):\n",
        "        super(CustomLayerNorm, self).__init__(normalized_shape, eps=eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Custom behavior, if needed, can be added here\n",
        "        return super(CustomLayerNorm, self).forward(x)\n",
        "\n",
        "\n",
        "class CustomResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout_prob=0.1):\n",
        "        super(CustomResidualConnection, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        # Custom residual connection implementation\n",
        "        return x + self.dropout(residual)\n",
        "\n",
        "\n",
        "class CustomAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(CustomAttention, self).__init__()\n",
        "        self.multihead_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.layer_norm = CustomLayerNorm(normalized_shape=embed_dim)\n",
        "        self.residual_connection = CustomResidualConnection()\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None,\n",
        "                encoder_attention_mask=None, past_key_value=None, output_attentions=False):\n",
        "        # Perform attention operation\n",
        "        attention_output, _ = self.multihead_attention(\n",
        "            hidden_states, hidden_states, hidden_states,\n",
        "            attn_mask=attention_mask, key_padding_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Apply layer normalization and residual connection\n",
        "        normalized_output = self.layer_norm(attention_output)\n",
        "        residual_output = self.residual_connection(normalized_output, hidden_states)\n",
        "\n",
        "        # Return as a tuple\n",
        "        return (residual_output,)\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:37:44.586948Z",
          "iopub.execute_input": "2024-07-25T04:37:44.587425Z",
          "iopub.status.idle": "2024-07-25T04:37:44.601617Z",
          "shell.execute_reply.started": "2024-07-25T04:37:44.58739Z",
          "shell.execute_reply": "2024-07-25T04:37:44.600357Z"
        },
        "trusted": true,
        "id": "26nH-p2awG1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integration of Custom Layers"
      ],
      "metadata": {
        "id": "a9pu5X3TwG1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoConfig\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self._replace_attention_layers()\n",
        "\n",
        "    def _replace_attention_layers(self):\n",
        "        for layer_name, layer_module in self.named_modules():\n",
        "            if isinstance(layer_module, nn.MultiheadAttention):\n",
        "                # Replace the self-attention layer\n",
        "                embed_dim = layer_module.embed_dim\n",
        "                num_heads = layer_module.num_heads\n",
        "                setattr(self, layer_name, CustomAttention(embed_dim, num_heads))\n",
        "                print(f\"Replaced attention layer {layer_name}\")\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None,\n",
        "                head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None,\n",
        "                return_dict=None):\n",
        "        # Ensure the forward method matches the input signature of BERT\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict\n",
        "        )\n",
        "        return outputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:37:51.735478Z",
          "iopub.execute_input": "2024-07-25T04:37:51.735907Z",
          "iopub.status.idle": "2024-07-25T04:37:51.749752Z",
          "shell.execute_reply.started": "2024-07-25T04:37:51.735875Z",
          "shell.execute_reply": "2024-07-25T04:37:51.748212Z"
        },
        "trusted": true,
        "id": "xQ5Xi38ywG1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model_name = 'bert-base-uncased'\n",
        "custom_model = CustomModel(model_name)\n",
        "\n",
        "# Example input (adjust according to the model's tokenizer)\n",
        "input_ids = torch.tensor([[101, 1045, 2064, 1005, 1055, 1037, 1000, 102]], dtype=torch.long)\n",
        "attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "outputs = custom_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "print(outputs.keys())\n",
        "print(outputs.last_hidden_state.shape)  # Example output shape\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T04:41:24.578345Z",
          "iopub.execute_input": "2024-07-25T04:41:24.578826Z",
          "iopub.status.idle": "2024-07-25T04:41:24.936887Z",
          "shell.execute_reply.started": "2024-07-25T04:41:24.578791Z",
          "shell.execute_reply": "2024-07-25T04:41:24.93544Z"
        },
        "trusted": true,
        "id": "BBxjI1XMwG1a",
        "outputId": "b4e3522b-b954-4d0a-8a59-12c964b3026b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "odict_keys(['last_hidden_state', 'pooler_output'])\ntorch.Size([1, 8, 768])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Local Attention & Global Attention\n",
        "\n",
        "https://github.com/lucidrains/local-attention\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xCrvB5pIwG1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Local attention and global attention are two strategies used in attention mechanisms, especially in the context of sequence modeling tasks such as machine translation, text generation, and image captioning. Here’s an explanation of each:\n",
        "\n",
        "### Local Attention\n",
        "\n",
        "**Definition:**\n",
        "Local attention focuses only on a subset of the entire input sequence or image at each step of the attention mechanism. Instead of attending to all positions or regions globally, it restricts the attention to a fixed-size window or neighborhood around the current position. This window can be centered around the position of interest and is often defined by a predefined window size or radius.\n",
        "\n",
        "**Characteristics:**\n",
        "- **Computationally Efficient:** Since local attention only considers a limited number of positions or regions, it reduces the computational cost compared to global attention, especially for long sequences or large images.\n",
        "  \n",
        "- **Contextual Relevance:** By focusing on a local neighborhood, local attention can potentially capture more relevant information that is closer in proximity to the current position or region.\n",
        "\n",
        "- **Fixed Context Window:** The size of the window or neighborhood is fixed and does not change dynamically based on the input sequence or task. This fixed context may limit the model’s ability to capture long-range dependencies or context that extends beyond the defined window.\n",
        "\n",
        "**Applications:**\n",
        "Local attention is commonly used in tasks where the input sequences are long and maintaining computational efficiency is crucial. For example:\n",
        "- Language modeling and text generation where the model needs to focus on nearby words for coherence and fluency.\n",
        "- Image processing tasks where the model needs to attend to neighboring pixels rather than the entire image for feature extraction or segmentation.\n",
        "\n",
        "### Global Attention\n",
        "\n",
        "**Definition:**\n",
        "Global attention, also known as full attention or unrestricted attention, allows the model to attend to all positions or regions across the entire input sequence or image simultaneously. Unlike local attention, there are no restrictions or fixed-size windows dictating where the model can attend.\n",
        "\n",
        "**Characteristics:**\n",
        "- **Comprehensive Context:** Global attention considers all positions or regions, providing the model with a comprehensive view of the entire input sequence or image. This helps capture long-range dependencies and context that spans the entire input.\n",
        "\n",
        "- **Higher Computational Cost:** Because global attention attends to all positions or regions, it is computationally more expensive compared to local attention, especially for large inputs.\n",
        "\n",
        "- **Dynamic Relevance:** The relevance of each position or region is dynamically determined based on the attention weights computed during the attention mechanism. This dynamic relevance helps the model adaptively focus on important parts of the input.\n",
        "\n",
        "**Applications:**\n",
        "Global attention is suitable for tasks where capturing long-range dependencies and maintaining a comprehensive understanding of the input is critical. For example:\n",
        "- Machine translation where the model needs to align words or tokens from the source and target languages across the entire sentence.\n",
        "- Image classification or object detection tasks where the model needs to consider all parts of the image for recognizing objects or patterns.\n",
        "\n",
        "### Comparison\n",
        "\n",
        "- **Scope of Attention:** Local attention focuses on a limited neighborhood or window around the current position, whereas global attention attends to all positions or regions across the entire input.\n",
        "  \n",
        "- **Computational Efficiency:** Local attention is more computationally efficient due to its restricted scope, whereas global attention is more resource-intensive but captures more comprehensive context.\n",
        "  \n",
        "- **Suitability:** The choice between local and global attention depends on the task requirements, input size, and computational constraints. Local attention is favored for efficiency and tasks with local dependencies, while global attention is preferred for tasks requiring a broader context and long-range dependencies.\n",
        "\n",
        "In practice, some models may use hybrid approaches that combine aspects of both local and global attention to balance efficiency and context capture, such as incorporating global attention for overall context and local attention for fine-grained details.\n",
        "\n",
        "\n",
        "1. **LocalAttention Class**: This class defines a custom local attention layer. It computes attention scores using query, key, and value projections similar to standard attention mechanisms but applies a local window mask to restrict attention to a fixed window size.\n",
        "\n",
        "2. **CustomTransformer Class**: This class integrates both `LocalAttention` and `MultiheadAttention` (for global attention). It combines the outputs of these attention mechanisms and passes them through a feedforward neural network for final processing.\n",
        "\n",
        "3. **Example Usage**: In the `__main__` block:\n",
        "   - We instantiate `CustomTransformer` with specified dimensions (`embed_dim`, `num_heads`) and `window_size`.\n",
        "   - Generate a random dummy input tensor (`input_tensor`).\n",
        "   - Perform a forward pass through the model (`model`) and print the shapes of the input and output tensors.\n",
        "\n",
        "### Notes:\n",
        "- **LocalAttention**: The `LocalAttention` layer in this example applies a simple left-aligned mask to restrict attention within a local window size (`window_size`). This can be further customized based on specific requirements or applications.\n",
        "  \n",
        "- **CustomTransformer**: Integrates both local and global attention mechanisms in a simple feedforward neural network architecture. In practice, depending on the task and requirements, you might adjust the specifics of each attention layer, such as window size for local attention or the number of heads for global attention.\n",
        "\n",
        "This example provides a basic framework for understanding how local and global attention mechanisms can be integrated within a transformer-based model in PyTorch. Adjustments and enhancements can be made based on specific use cases or tasks."
      ],
      "metadata": {
        "id": "SgWz3eP8wG1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Custom Local Attention Layer\n",
        "class LocalAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, window_size=5):\n",
        "        super(LocalAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.window_size = window_size\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "#         attention_scores = torch.matmul(q, k.transpose(-1, -2)) / (self.embed_dim ** 0.5)\n",
        "        print(attention_scores.shape)\n",
        "        # Apply local window mask\n",
        "        mask = torch.zeros_like(attention_scores)\n",
        "        mask[:, :, :self.window_size] = -float('inf')\n",
        "        print(mask.shape)\n",
        "        attention_scores = attention_scores + mask\n",
        "\n",
        "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "        context = torch.matmul(attention_weights, v)\n",
        "\n",
        "        return context\n",
        "\n",
        "# Custom Transformer Model using both Local and Global Attention\n",
        "class CustomTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, window_size=5):\n",
        "        super(CustomTransformer, self).__init__()\n",
        "        self.local_attention = LocalAttention(embed_dim, window_size)\n",
        "        self.global_attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Local Attention\n",
        "        local_context = self.local_attention(x)\n",
        "\n",
        "        # Global Attention\n",
        "        global_context, _ = self.global_attention(x, x, x)\n",
        "\n",
        "        # Combine local and global contexts\n",
        "        combined_context = local_context + global_context\n",
        "\n",
        "        # Feedforward layer\n",
        "        output = self.feedforward(combined_context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    embed_dim = 16\n",
        "    num_heads = 4\n",
        "    window_size = 5\n",
        "    batch_size = 8\n",
        "    seq_len = 10\n",
        "\n",
        "    # Create an instance of the custom transformer model\n",
        "    model = CustomTransformer(embed_dim, num_heads, window_size)\n",
        "\n",
        "    # Generate a dummy input tensor\n",
        "    input_tensor = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output_tensor = model(input_tensor)\n",
        "\n",
        "    print(\"Input Tensor Shape:\", input_tensor.shape)\n",
        "    print(\"Output Tensor Shape:\", output_tensor.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-25T05:50:11.53156Z",
          "iopub.execute_input": "2024-07-25T05:50:11.532724Z",
          "iopub.status.idle": "2024-07-25T05:50:11.551228Z",
          "shell.execute_reply.started": "2024-07-25T05:50:11.532683Z",
          "shell.execute_reply": "2024-07-25T05:50:11.549956Z"
        },
        "trusted": true,
        "id": "Z2egUv_9wG1d",
        "outputId": "70dd343c-7c34-4a14-9ce1-2651bcc58420"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([8, 10, 10])\ntorch.Size([8, 10, 10])\nInput Tensor Shape: torch.Size([8, 10, 16])\nOutput Tensor Shape: torch.Size([8, 10, 16])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "aZ8YcFiwwG1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# class SparseAttention(nn.Module):\n",
        "#     def __init__(self, input_dim, embed_dim, num_heads, window):\n",
        "#         super(SparseAttention, self).__init__()\n",
        "#         self.head_dim = embed_dim // num_heads\n",
        "#         self.num_heads = num_heads\n",
        "#         self.window = window\n",
        "#         self.q_proj = nn.Linear(input_dim, embed_dim)\n",
        "#         self.k_proj = nn.Linear(input_dim, embed_dim)\n",
        "#         self.v_proj = nn.Linear(input_dim, embed_dim)\n",
        "#         self.output_proj = nn.Linear(embed_dim, input_dim)\n",
        "\n",
        "#     def _create_local_attention_mask(self, seq):\n",
        "#         mask = torch.zeros(seq, seq)  # [seq, seq]\n",
        "#         for i in range(seq):\n",
        "#             start = max(0, i - self.window // 2)\n",
        "#             end = min(seq, i + self.window // 2 + 1)\n",
        "#             mask[start:end, i] = 1\n",
        "#         mask = mask.unsqueeze(0).unsqueeze(0).to(torch.bool)  # [1, 1, seq, seq]\n",
        "#         return mask\n",
        "\n",
        "#     def forward(self, x, local_attn=False):\n",
        "#         batch, seq, embed_dim = x.size()\n",
        "#         q = self.q_proj(x)\n",
        "#         k = self.k_proj(x)\n",
        "#         v = self.v_proj(x)\n",
        "\n",
        "#         q = q.view(batch, seq, self.num_heads, self.head_dim).permute(0, 2, 1, 3) # [batch,n_heads,seq,head_dim]\n",
        "#         k = k.view(batch, seq, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "#         v = v.view(batch, seq, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "#         attn_scores = torch.matmul(q, k.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5) # [batch,n_heads,q_seq,k_seq]\n",
        "\n",
        "#         if local_attn:\n",
        "#             mask = self._create_local_attention_mask(seq)\n",
        "#             attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "#         attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "#         attn_output = torch.matmul(attn_weights, v) # [batch,n_heads,q_seq,head_dim]\n",
        "\n",
        "#         attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(batch, seq, -1) # [batch,q_seq,input_dim]\n",
        "#         attn_output = self.output_proj(attn_output)\n",
        "\n",
        "#         return attn_output, attn_weights\n",
        "\n",
        "class SparseAttention(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, num_heads, window):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.window = window\n",
        "        self.q_proj = nn.Linear(input_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(input_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(input_dim, embed_dim)\n",
        "        self.output_proj = nn.Linear(embed_dim, input_dim)\n",
        "\n",
        "    def _create_local_attention_mask(self, seq):\n",
        "        mask = torch.zeros(seq, seq, dtype=torch.bool)  # [seq, seq]\n",
        "        for i in range(seq):\n",
        "            start = max(0, i - self.window // 2)\n",
        "            end = min(seq, i + self.window // 2 + 1)\n",
        "            mask[start:end, i] = 1\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, seq]\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x, local_attn=False):\n",
        "        batch_size, seq_len, embed_dim = x.size() # [batch,seq_len,head_dim * num_heads]\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "\n",
        "        # Reshape and transpose for multi-head attention\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
        "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
        "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [batch, num_heads, seq_len, seq_len]\n",
        "\n",
        "        if local_attn:\n",
        "            mask = self._create_local_attention_mask(seq_len)\n",
        "            attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, v)  # [batch, num_heads, seq_len, head_dim]\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, embed_dim)  # [batch, seq_len, embed_dim]\n",
        "        attn_output = self.output_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = AutoModelForCausalLM.from_pretrained('facebook/opt-350m')\n",
        "        self._replace_attention_layer() # can add more custom layers like this\n",
        "\n",
        "    def _replace_attention_layer(self):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, nn.MultiheadAttention):\n",
        "                # Extract parameters from the existing module\n",
        "                input_dim = module.embed_dim\n",
        "                num_heads = module.num_heads\n",
        "                # Create and replace the attention layer\n",
        "                new_attention_layer = SparseAttention(\n",
        "                    input_dim=input_dim,\n",
        "                    embed_dim=input_dim,\n",
        "                    num_heads=num_heads,\n",
        "                    window=4,  # Example window size\n",
        "\n",
        "                )\n",
        "                setattr(self.model, name, new_attention_layer)\n",
        "\n",
        "    def forward(self,*args, local_attn=False, **kwargs):\n",
        "        return self.model(*args,**kwargs)\n",
        "\n",
        "\n",
        "# Tokenization and Model Execution\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-350m')\n",
        "inputs = tokenizer(\"If you modify the model\",return_tensors='pt')\n",
        "inputs.items()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:35:01.232525Z",
          "iopub.execute_input": "2024-08-29T04:35:01.233036Z",
          "iopub.status.idle": "2024-08-29T04:35:01.599872Z",
          "shell.execute_reply.started": "2024-08-29T04:35:01.233Z",
          "shell.execute_reply": "2024-08-29T04:35:01.598434Z"
        },
        "trusted": true,
        "id": "eR4QYf7PwG1f",
        "outputId": "1d9c0cad-6a8f-4175-e412-2a52084e194c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "dict_items([('input_ids', tensor([[    2,  1106,    47, 23209,     5,  1421]])), ('attention_mask', tensor([[1, 1, 1, 1, 1, 1]]))])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input_ids\"][0].shape[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:22:37.692491Z",
          "iopub.execute_input": "2024-08-29T04:22:37.692939Z",
          "iopub.status.idle": "2024-08-29T04:22:37.701772Z",
          "shell.execute_reply.started": "2024-08-29T04:22:37.692905Z",
          "shell.execute_reply": "2024-08-29T04:22:37.700202Z"
        },
        "trusted": true,
        "id": "s87rOyMvwG1g",
        "outputId": "531cff9e-f10a-4880-b511-c60039dde016"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "6"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_local_attention_mask(seq,window = 3):\n",
        "        mask = torch.zeros(seq, seq, dtype=torch.bool)  # [seq, seq]\n",
        "        for i in range(seq):\n",
        "            start = max(0, i - window // 2)\n",
        "            end = min(seq, i + window // 2 + 1)\n",
        "            mask[start:end, i] = 1\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq, seq]\n",
        "        return mask\n",
        "\n",
        "window = 3\n",
        "seq = inputs[\"input_ids\"][0].shape[0]\n",
        "mask = create_local_attention_mask(seq)\n",
        "print(seq)\n",
        "print(mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:35:08.151978Z",
          "iopub.execute_input": "2024-08-29T04:35:08.152575Z",
          "iopub.status.idle": "2024-08-29T04:35:08.16488Z",
          "shell.execute_reply.started": "2024-08-29T04:35:08.152533Z",
          "shell.execute_reply": "2024-08-29T04:35:08.163512Z"
        },
        "trusted": true,
        "id": "dl_k11EtwG1h",
        "outputId": "6909c135-1ac6-4d61-8c77-ce8bd5dd9393"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "6\ntensor([[[[ True,  True, False, False, False, False],\n          [ True,  True,  True, False, False, False],\n          [False,  True,  True,  True, False, False],\n          [False, False,  True,  True,  True, False],\n          [False, False, False,  True,  True,  True],\n          [False, False, False, False,  True,  True]]]])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Word Prediction"
      ],
      "metadata": {
        "id": "B5OyU-F5wG1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel()\n",
        "outputs = model(**inputs, local_attn=True)\n",
        "\n",
        "\n",
        "logits = outputs.logits  # Raw logits (scores for each token in the vocabulary)\n",
        "hidden_states = outputs.hidden_states  # Optional, if return_dict=True and output_hidden_states=True\n",
        "attention_weights = outputs.attentions  # Optional, if return_dict=True and output_attentions=True\n",
        "print(\"Logits shape:\", logits.shape) # [batch_size, seq_length, vocab_size]\n",
        "predicted_token_ids = torch.argmax(logits, dim=-1)  # [batch, seq]\n",
        "decoded_output = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Decoded output:\", decoded_output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:35:17.961667Z",
          "iopub.execute_input": "2024-08-29T04:35:17.962099Z",
          "iopub.status.idle": "2024-08-29T04:35:19.229974Z",
          "shell.execute_reply.started": "2024-08-29T04:35:17.962065Z",
          "shell.execute_reply": "2024-08-29T04:35:19.228617Z"
        },
        "trusted": true,
        "id": "nuMNXwq8wG1i",
        "outputId": "15770b4a-8a45-42da-9f52-5bb37d2d32d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Logits shape: torch.Size([1, 6, 50272])\nDecoded output: [\"\\n you're the game,\"]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained('facebook/opt-350m')\n",
        "\n",
        "# Example usage\n",
        "base_outputs = base_model(**inputs)\n",
        "logits_base_model = base_outputs.logits\n",
        "pred_token_id_base_model = torch.argmax(logits_base_model ,dim=-1)\n",
        "decoded_output_base_model = tokenizer.batch_decode(pred_token_id_base_model, skip_special_tokens=True)\n",
        "print(\"Logits shape(Base model):\", logits_base_model .shape)\n",
        "print(\"Decoded output(Base model):\", decoded_output_base_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-29T04:35:24.079375Z",
          "iopub.execute_input": "2024-08-29T04:35:24.080339Z",
          "iopub.status.idle": "2024-08-29T04:35:25.812781Z",
          "shell.execute_reply.started": "2024-08-29T04:35:24.080299Z",
          "shell.execute_reply": "2024-08-29T04:35:25.811028Z"
        },
        "trusted": true,
        "id": "zoHu-GT2wG1i",
        "outputId": "7c27794d-b7e3-4c34-c2ca-80c2ce24ba1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Logits shape(Base model): torch.Size([1, 6, 50272])\nDecoded output(Base model): [\"\\n you're the game,\"]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-16T08:33:15.437491Z",
          "iopub.execute_input": "2024-08-16T08:33:15.438827Z",
          "iopub.status.idle": "2024-08-16T08:33:15.443546Z",
          "shell.execute_reply.started": "2024-08-16T08:33:15.438789Z",
          "shell.execute_reply": "2024-08-16T08:33:15.442393Z"
        },
        "trusted": true,
        "id": "9jP_kD8ywG1j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}