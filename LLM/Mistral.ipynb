{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Mistral",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "5liKsxRW1rbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IJqpIH_w1rbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/hkproj/mistral-llm-notes\n",
        "    \n",
        "    \n",
        "https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/master/deep-dives/001-mistral-7b/README.md\n",
        "\n",
        "\n",
        "https://github.com/DongmingShenDS/Mistral_From_Scratch\n"
      ],
      "metadata": {
        "id": "Vohg9Sph1rbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T06:20:57.629461Z",
          "iopub.execute_input": "2024-08-31T06:20:57.629801Z",
          "iopub.status.idle": "2024-08-31T06:20:57.935685Z",
          "shell.execute_reply.started": "2024-08-31T06:20:57.629765Z",
          "shell.execute_reply": "2024-08-31T06:20:57.934766Z"
        },
        "trusted": true,
        "id": "0oY5TcnT1rbu",
        "outputId": "c9bfe515-97e0-4b8d-f841-30410a497422",
        "colab": {
          "referenced_widgets": [
            "d813c9163f6d454bb0b54aaf745d1261"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d813c9163f6d454bb0b54aaf745d1261"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sliding Window attention - Receptive Field\n",
        "\n",
        "Initially, the sequence is a list of sets, all containing a single token.\n",
        "\n",
        "Layer 1 input:\n",
        "\n",
        "    0: ['the']\n",
        "    1: ['cat']\n",
        "    2: ['is']\n",
        "    3: ['on']\n",
        "    4: ['a']\n",
        "    5: ['chair']\n",
        "\n",
        "After the first layer, considering a sliding window size of 3, the output of the attention mechanism is:\n",
        "\n",
        "Layer 1 output:\n",
        "\n",
        "    0: ['the']\n",
        "    1: ['the', 'cat']\n",
        "    2: ['the', 'cat', 'is']\n",
        "    3: ['cat', 'is', 'on']\n",
        "    4: ['is', 'on', 'a']\n",
        "    5: ['on', 'a', 'chair']\n",
        "\n",
        "The output of the first layer becomes the input of the second layer. The output of the second layer is:\n",
        "\n",
        "Layer 2 output:\n",
        "\n",
        "    0: ['the']\n",
        "    1: ['the', 'cat']\n",
        "    2: ['the', 'cat', 'is']\n",
        "    3: ['the', 'cat', 'is', 'on']\n",
        "    4: ['the', 'cat', 'is', 'on', 'a']\n",
        "    5: ['cat', 'is', 'on', 'a', 'chair']\n",
        "\n",
        "As we can see, even with a sliding window of size 3, after just two layers, the attention mechanism can capture long-range dependencies. This is because the output of the first layer is used as the input of the second layer, and the attention mechanism is applied again. This is similar to the idea of stacking multiple layers of CNNs to increase the receptive field."
      ],
      "metadata": {
        "id": "xE6Gr7481rb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_order = ['the', 'cat', 'is', 'on', 'a', 'chair']\n",
        "sequence = [{print_order[i]} for i in range(len(print_order))]\n",
        "sequence"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T19:11:02.951914Z",
          "iopub.execute_input": "2024-08-25T19:11:02.952853Z",
          "iopub.status.idle": "2024-08-25T19:11:02.984535Z",
          "shell.execute_reply.started": "2024-08-25T19:11:02.952804Z",
          "shell.execute_reply": "2024-08-25T19:11:02.983498Z"
        },
        "trusted": true,
        "id": "MC2556ih1rb8",
        "outputId": "62431490-eede-41ef-8b21-20882ba0afea"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[{'the'}, {'cat'}, {'is'}, {'on'}, {'a'}, {'chair'}]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sliding_window_size = 3\n",
        "\n",
        "def sliding_window_attention(seq: list[set[str]], w: int):\n",
        "    seq_len = len(seq)\n",
        "    attention_scores: list[list[set]] = [[None for _ in range(seq_len)] for _ in range(seq_len)]\n",
        "    for i, q_tokens_set in enumerate(seq):\n",
        "        for j, k_tokens_set in enumerate(seq):\n",
        "            # The upper triangle is all None\n",
        "            if j > i:\n",
        "                continue\n",
        "            # Each token can only attend to the previous W tokens\n",
        "            if i - j >= w:\n",
        "                continue\n",
        "\n",
        "            attention = set()\n",
        "            # Add all tokens from q_tokens_set to attention_result\n",
        "            attention.update(q_tokens_set)\n",
        "            # Add all tokens from k_tokens_set to attention_resul\n",
        "            attention.update(k_tokens_set)\n",
        "\n",
        "            attention_scores[i][j] = attention\n",
        "    return attention_scores\n",
        "\n",
        "def multiple_by_v(attention_scores: list[list[set]], v_sequence: list[set[str]]) -> list[set[str]]:\n",
        "    seq_len = len(v_sequence)\n",
        "    result = [set() for _ in range(seq_len)]\n",
        "    for i in range(seq_len):\n",
        "        for j in range(seq_len):\n",
        "            attention = attention_scores[i][j]\n",
        "            v = v_sequence[j]\n",
        "            r = result[i]\n",
        "            # Add all the tokens in the attention (if not None) to r\n",
        "            if attention is not None:\n",
        "                # Add all the tokens in v to r\n",
        "                r.update(v)\n",
        "                r.update(attention)\n",
        "    return result\n",
        "\n",
        "def print_attention(attention_scores: list[list[set[str]]]):\n",
        "    for i, row in enumerate(attention_scores):\n",
        "        for j, attention in enumerate(row):\n",
        "            if attention is None:\n",
        "                print('None', end='\\t')\n",
        "            else:\n",
        "                print(f'{sorted(attention, key=lambda x: print_order.index(x))}', end='\\t')\n",
        "        print()\n",
        "\n",
        "def print_sequence(seq: list[set[str]]):\n",
        "    for i, tokens_set in enumerate(seq):\n",
        "        print(f'{i}: {sorted(tokens_set, key=lambda x: print_order.index(x))}')\n",
        "\n",
        "def print_layer(input: list[set[str]], layer_num: int) -> list[set[str]]:\n",
        "    print(f'Layer {layer_num} input:')\n",
        "    print_sequence(input)\n",
        "    attention_scores = sliding_window_attention(input, sliding_window_size)\n",
        "    print()\n",
        "    print(f'Layer {layer_num} attention scores:')\n",
        "    print_attention(attention_scores)\n",
        "    output = multiple_by_v(attention_scores, input)\n",
        "    print()\n",
        "    print(f'Layer {layer_num} output:')\n",
        "    print_sequence(output)\n",
        "    return output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T19:19:21.360352Z",
          "iopub.execute_input": "2024-08-25T19:19:21.36081Z",
          "iopub.status.idle": "2024-08-25T19:19:21.37562Z",
          "shell.execute_reply.started": "2024-08-25T19:19:21.360768Z",
          "shell.execute_reply": "2024-08-25T19:19:21.374213Z"
        },
        "trusted": true,
        "id": "SJ2f135j1rcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 1\n",
        "output_layer_1 = print_layer(sequence, 1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T19:19:24.590039Z",
          "iopub.execute_input": "2024-08-25T19:19:24.590482Z",
          "iopub.status.idle": "2024-08-25T19:19:24.597242Z",
          "shell.execute_reply.started": "2024-08-25T19:19:24.590442Z",
          "shell.execute_reply": "2024-08-25T19:19:24.596053Z"
        },
        "trusted": true,
        "id": "I5MLOpk91rcF",
        "outputId": "367cb172-736e-467d-c2d0-83c717cc23f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Layer 1 input:\n0: ['the']\n1: ['cat']\n2: ['is']\n3: ['on']\n4: ['a']\n5: ['chair']\n\nLayer 1 attention scores:\n['the']\tNone\tNone\tNone\tNone\tNone\t\n['the', 'cat']\t['cat']\tNone\tNone\tNone\tNone\t\n['the', 'is']\t['cat', 'is']\t['is']\tNone\tNone\tNone\t\nNone\t['cat', 'on']\t['is', 'on']\t['on']\tNone\tNone\t\nNone\tNone\t['is', 'a']\t['on', 'a']\t['a']\tNone\t\nNone\tNone\tNone\t['on', 'chair']\t['a', 'chair']\t['chair']\t\n\nLayer 1 output:\n0: ['the']\n1: ['the', 'cat']\n2: ['the', 'cat', 'is']\n3: ['cat', 'is', 'on']\n4: ['is', 'on', 'a']\n5: ['on', 'a', 'chair']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 2\n",
        "output_layer_2 = print_layer(output_layer_1, 2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T19:20:01.513504Z",
          "iopub.execute_input": "2024-08-25T19:20:01.513911Z",
          "iopub.status.idle": "2024-08-25T19:20:01.52022Z",
          "shell.execute_reply.started": "2024-08-25T19:20:01.513873Z",
          "shell.execute_reply": "2024-08-25T19:20:01.519018Z"
        },
        "trusted": true,
        "id": "9CFFNAAo1rcJ",
        "outputId": "4b0b9e51-c57f-40ec-8c5d-004881be8c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Layer 2 input:\n0: ['the']\n1: ['the', 'cat']\n2: ['the', 'cat', 'is']\n3: ['cat', 'is', 'on']\n4: ['is', 'on', 'a']\n5: ['on', 'a', 'chair']\n\nLayer 2 attention scores:\n['the']\tNone\tNone\tNone\tNone\tNone\t\n['the', 'cat']\t['the', 'cat']\tNone\tNone\tNone\tNone\t\n['the', 'cat', 'is']\t['the', 'cat', 'is']\t['the', 'cat', 'is']\tNone\tNone\tNone\t\nNone\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\t['cat', 'is', 'on']\tNone\tNone\t\nNone\tNone\t['the', 'cat', 'is', 'on', 'a']\t['cat', 'is', 'on', 'a']\t['is', 'on', 'a']\tNone\t\nNone\tNone\tNone\t['cat', 'is', 'on', 'a', 'chair']\t['is', 'on', 'a', 'chair']\t['on', 'a', 'chair']\t\n\nLayer 2 output:\n0: ['the']\n1: ['the', 'cat']\n2: ['the', 'cat', 'is']\n3: ['the', 'cat', 'is', 'on']\n4: ['the', 'cat', 'is', 'on', 'a']\n5: ['cat', 'is', 'on', 'a', 'chair']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer 3\n",
        "output_layer_3 = print_layer(output_layer_2, 3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T19:20:32.740365Z",
          "iopub.execute_input": "2024-08-25T19:20:32.741194Z",
          "iopub.status.idle": "2024-08-25T19:20:32.747275Z",
          "shell.execute_reply.started": "2024-08-25T19:20:32.741139Z",
          "shell.execute_reply": "2024-08-25T19:20:32.746003Z"
        },
        "trusted": true,
        "id": "jJYkjlr41rcM",
        "outputId": "920f7fdb-4160-40e3-ae69-fb8f808f4b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Layer 3 input:\n0: ['the']\n1: ['the', 'cat']\n2: ['the', 'cat', 'is']\n3: ['the', 'cat', 'is', 'on']\n4: ['the', 'cat', 'is', 'on', 'a']\n5: ['cat', 'is', 'on', 'a', 'chair']\n\nLayer 3 attention scores:\n['the']\tNone\tNone\tNone\tNone\tNone\t\n['the', 'cat']\t['the', 'cat']\tNone\tNone\tNone\tNone\t\n['the', 'cat', 'is']\t['the', 'cat', 'is']\t['the', 'cat', 'is']\tNone\tNone\tNone\t\nNone\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\t['the', 'cat', 'is', 'on']\tNone\tNone\t\nNone\tNone\t['the', 'cat', 'is', 'on', 'a']\t['the', 'cat', 'is', 'on', 'a']\t['the', 'cat', 'is', 'on', 'a']\tNone\t\nNone\tNone\tNone\t['the', 'cat', 'is', 'on', 'a', 'chair']\t['the', 'cat', 'is', 'on', 'a', 'chair']\t['cat', 'is', 'on', 'a', 'chair']\t\n\nLayer 3 output:\n0: ['the']\n1: ['the', 'cat']\n2: ['the', 'cat', 'is']\n3: ['the', 'cat', 'is', 'on']\n4: ['the', 'cat', 'is', 'on', 'a']\n5: ['the', 'cat', 'is', 'on', 'a', 'chair']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q xformers"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T19:26:44.401288Z",
          "iopub.execute_input": "2024-08-25T19:26:44.401724Z",
          "iopub.status.idle": "2024-08-25T19:26:59.369909Z",
          "shell.execute_reply.started": "2024-08-25T19:26:44.401682Z",
          "shell.execute_reply": "2024-08-25T19:26:59.368302Z"
        },
        "trusted": true,
        "id": "fBHRiDI-1rcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xformers.ops.fmha.attn_bias import (\n",
        "    AttentionBias,\n",
        "    BlockDiagonalCausalMask,\n",
        "    BlockDiagonalCausalWithOffsetPaddedKeysMask,\n",
        "    BlockDiagonalMask,\n",
        ")\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T19:29:44.821414Z",
          "iopub.execute_input": "2024-08-25T19:29:44.821863Z",
          "iopub.status.idle": "2024-08-25T19:29:49.493743Z",
          "shell.execute_reply.started": "2024-08-25T19:29:44.821821Z",
          "shell.execute_reply": "2024-08-25T19:29:49.492714Z"
        },
        "trusted": true,
        "id": "cAnNRY631rcQ",
        "outputId": "128a1540-85e4-47db-8083-c87d39c95b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_dict = {0.0: '#90EE90', float('-inf'): '#FA8072'}\n",
        "def colour_cell(val):\n",
        "    if val in col_dict:\n",
        "        return 'Background-color: %s' % col_dict[val]\n",
        "    return ''\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"The cat sat on the mat and purred.\",  # 7 words\n",
        "    \"The dog ran fast today.\",              # 5 words\n",
        "    \"The quick brown fox jumps over.\"      # 6 words\n",
        "]\n",
        "\n",
        "def get_flattened_words(sentences):\n",
        "    words = []\n",
        "    for sentence in sentences:\n",
        "        words.extend(sentence.split())\n",
        "    return words\n",
        "\n",
        "# flattened list of words\n",
        "labels = get_flattened_words(sentences)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T20:02:55.151024Z",
          "iopub.execute_input": "2024-08-25T20:02:55.152138Z",
          "iopub.status.idle": "2024-08-25T20:02:55.159097Z",
          "shell.execute_reply.started": "2024-08-25T20:02:55.152056Z",
          "shell.execute_reply": "2024-08-25T20:02:55.15777Z"
        },
        "trusted": true,
        "id": "hsZqyNnS1rcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BlockDiagonalCausalMask\n",
        "\n",
        "seqlens = [7, 5, 6]\n",
        "sliding_window_size = 3\n",
        "\n",
        "mask = BlockDiagonalCausalMask.from_seqlens(seqlens).make_local_attention(sliding_window_size)\n",
        "\n",
        "batch_size = 1\n",
        "total_seq_len = sum(seqlens)\n",
        "mask_tensor = mask.materialize((batch_size, total_seq_len, total_seq_len))\n",
        "\n",
        "df = pd.DataFrame(mask_tensor[0, :, :].numpy())\n",
        "df.style.applymap(colour_cell)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T20:03:00.298287Z",
          "iopub.execute_input": "2024-08-25T20:03:00.29873Z",
          "iopub.status.idle": "2024-08-25T20:03:00.326103Z",
          "shell.execute_reply.started": "2024-08-25T20:03:00.298688Z",
          "shell.execute_reply": "2024-08-25T20:03:00.324801Z"
        },
        "trusted": true,
        "id": "WNROh8xh1rcU",
        "outputId": "a060ed15-aa9e-456e-e10a-56d9c1614ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/1176867877.py:13: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n  df.style.applymap(colour_cell)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 37,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<pandas.io.formats.style.Styler at 0x7ac2be3232b0>",
            "text/html": "<style type=\"text/css\">\n#T_ce7ea_row0_col0, #T_ce7ea_row1_col0, #T_ce7ea_row1_col1, #T_ce7ea_row2_col0, #T_ce7ea_row2_col1, #T_ce7ea_row2_col2, #T_ce7ea_row3_col1, #T_ce7ea_row3_col2, #T_ce7ea_row3_col3, #T_ce7ea_row4_col2, #T_ce7ea_row4_col3, #T_ce7ea_row4_col4, #T_ce7ea_row5_col3, #T_ce7ea_row5_col4, #T_ce7ea_row5_col5, #T_ce7ea_row6_col4, #T_ce7ea_row6_col5, #T_ce7ea_row6_col6, #T_ce7ea_row7_col7, #T_ce7ea_row8_col7, #T_ce7ea_row8_col8, #T_ce7ea_row9_col7, #T_ce7ea_row9_col8, #T_ce7ea_row9_col9, #T_ce7ea_row10_col8, #T_ce7ea_row10_col9, #T_ce7ea_row10_col10, #T_ce7ea_row11_col9, #T_ce7ea_row11_col10, #T_ce7ea_row11_col11, #T_ce7ea_row12_col12, #T_ce7ea_row13_col12, #T_ce7ea_row13_col13, #T_ce7ea_row14_col12, #T_ce7ea_row14_col13, #T_ce7ea_row14_col14, #T_ce7ea_row15_col13, #T_ce7ea_row15_col14, #T_ce7ea_row15_col15, #T_ce7ea_row16_col14, #T_ce7ea_row16_col15, #T_ce7ea_row16_col16, #T_ce7ea_row17_col15, #T_ce7ea_row17_col16, #T_ce7ea_row17_col17 {\n  Background-color: #90EE90;\n}\n#T_ce7ea_row0_col1, #T_ce7ea_row0_col2, #T_ce7ea_row0_col3, #T_ce7ea_row0_col4, #T_ce7ea_row0_col5, #T_ce7ea_row0_col6, #T_ce7ea_row0_col7, #T_ce7ea_row0_col8, #T_ce7ea_row0_col9, #T_ce7ea_row0_col10, #T_ce7ea_row0_col11, #T_ce7ea_row0_col12, #T_ce7ea_row0_col13, #T_ce7ea_row0_col14, #T_ce7ea_row0_col15, #T_ce7ea_row0_col16, #T_ce7ea_row0_col17, #T_ce7ea_row1_col2, #T_ce7ea_row1_col3, #T_ce7ea_row1_col4, #T_ce7ea_row1_col5, #T_ce7ea_row1_col6, #T_ce7ea_row1_col7, #T_ce7ea_row1_col8, #T_ce7ea_row1_col9, #T_ce7ea_row1_col10, #T_ce7ea_row1_col11, #T_ce7ea_row1_col12, #T_ce7ea_row1_col13, #T_ce7ea_row1_col14, #T_ce7ea_row1_col15, #T_ce7ea_row1_col16, #T_ce7ea_row1_col17, #T_ce7ea_row2_col3, #T_ce7ea_row2_col4, #T_ce7ea_row2_col5, #T_ce7ea_row2_col6, #T_ce7ea_row2_col7, #T_ce7ea_row2_col8, #T_ce7ea_row2_col9, #T_ce7ea_row2_col10, #T_ce7ea_row2_col11, #T_ce7ea_row2_col12, #T_ce7ea_row2_col13, #T_ce7ea_row2_col14, #T_ce7ea_row2_col15, #T_ce7ea_row2_col16, #T_ce7ea_row2_col17, #T_ce7ea_row3_col0, #T_ce7ea_row3_col4, #T_ce7ea_row3_col5, #T_ce7ea_row3_col6, #T_ce7ea_row3_col7, #T_ce7ea_row3_col8, #T_ce7ea_row3_col9, #T_ce7ea_row3_col10, #T_ce7ea_row3_col11, #T_ce7ea_row3_col12, #T_ce7ea_row3_col13, #T_ce7ea_row3_col14, #T_ce7ea_row3_col15, #T_ce7ea_row3_col16, #T_ce7ea_row3_col17, #T_ce7ea_row4_col0, #T_ce7ea_row4_col1, #T_ce7ea_row4_col5, #T_ce7ea_row4_col6, #T_ce7ea_row4_col7, #T_ce7ea_row4_col8, #T_ce7ea_row4_col9, #T_ce7ea_row4_col10, #T_ce7ea_row4_col11, #T_ce7ea_row4_col12, #T_ce7ea_row4_col13, #T_ce7ea_row4_col14, #T_ce7ea_row4_col15, #T_ce7ea_row4_col16, #T_ce7ea_row4_col17, #T_ce7ea_row5_col0, #T_ce7ea_row5_col1, #T_ce7ea_row5_col2, #T_ce7ea_row5_col6, #T_ce7ea_row5_col7, #T_ce7ea_row5_col8, #T_ce7ea_row5_col9, #T_ce7ea_row5_col10, #T_ce7ea_row5_col11, #T_ce7ea_row5_col12, #T_ce7ea_row5_col13, #T_ce7ea_row5_col14, #T_ce7ea_row5_col15, #T_ce7ea_row5_col16, #T_ce7ea_row5_col17, #T_ce7ea_row6_col0, #T_ce7ea_row6_col1, #T_ce7ea_row6_col2, #T_ce7ea_row6_col3, #T_ce7ea_row6_col7, #T_ce7ea_row6_col8, #T_ce7ea_row6_col9, #T_ce7ea_row6_col10, #T_ce7ea_row6_col11, #T_ce7ea_row6_col12, #T_ce7ea_row6_col13, #T_ce7ea_row6_col14, #T_ce7ea_row6_col15, #T_ce7ea_row6_col16, #T_ce7ea_row6_col17, #T_ce7ea_row7_col0, #T_ce7ea_row7_col1, #T_ce7ea_row7_col2, #T_ce7ea_row7_col3, #T_ce7ea_row7_col4, #T_ce7ea_row7_col5, #T_ce7ea_row7_col6, #T_ce7ea_row7_col8, #T_ce7ea_row7_col9, #T_ce7ea_row7_col10, #T_ce7ea_row7_col11, #T_ce7ea_row7_col12, #T_ce7ea_row7_col13, #T_ce7ea_row7_col14, #T_ce7ea_row7_col15, #T_ce7ea_row7_col16, #T_ce7ea_row7_col17, #T_ce7ea_row8_col0, #T_ce7ea_row8_col1, #T_ce7ea_row8_col2, #T_ce7ea_row8_col3, #T_ce7ea_row8_col4, #T_ce7ea_row8_col5, #T_ce7ea_row8_col6, #T_ce7ea_row8_col9, #T_ce7ea_row8_col10, #T_ce7ea_row8_col11, #T_ce7ea_row8_col12, #T_ce7ea_row8_col13, #T_ce7ea_row8_col14, #T_ce7ea_row8_col15, #T_ce7ea_row8_col16, #T_ce7ea_row8_col17, #T_ce7ea_row9_col0, #T_ce7ea_row9_col1, #T_ce7ea_row9_col2, #T_ce7ea_row9_col3, #T_ce7ea_row9_col4, #T_ce7ea_row9_col5, #T_ce7ea_row9_col6, #T_ce7ea_row9_col10, #T_ce7ea_row9_col11, #T_ce7ea_row9_col12, #T_ce7ea_row9_col13, #T_ce7ea_row9_col14, #T_ce7ea_row9_col15, #T_ce7ea_row9_col16, #T_ce7ea_row9_col17, #T_ce7ea_row10_col0, #T_ce7ea_row10_col1, #T_ce7ea_row10_col2, #T_ce7ea_row10_col3, #T_ce7ea_row10_col4, #T_ce7ea_row10_col5, #T_ce7ea_row10_col6, #T_ce7ea_row10_col7, #T_ce7ea_row10_col11, #T_ce7ea_row10_col12, #T_ce7ea_row10_col13, #T_ce7ea_row10_col14, #T_ce7ea_row10_col15, #T_ce7ea_row10_col16, #T_ce7ea_row10_col17, #T_ce7ea_row11_col0, #T_ce7ea_row11_col1, #T_ce7ea_row11_col2, #T_ce7ea_row11_col3, #T_ce7ea_row11_col4, #T_ce7ea_row11_col5, #T_ce7ea_row11_col6, #T_ce7ea_row11_col7, #T_ce7ea_row11_col8, #T_ce7ea_row11_col12, #T_ce7ea_row11_col13, #T_ce7ea_row11_col14, #T_ce7ea_row11_col15, #T_ce7ea_row11_col16, #T_ce7ea_row11_col17, #T_ce7ea_row12_col0, #T_ce7ea_row12_col1, #T_ce7ea_row12_col2, #T_ce7ea_row12_col3, #T_ce7ea_row12_col4, #T_ce7ea_row12_col5, #T_ce7ea_row12_col6, #T_ce7ea_row12_col7, #T_ce7ea_row12_col8, #T_ce7ea_row12_col9, #T_ce7ea_row12_col10, #T_ce7ea_row12_col11, #T_ce7ea_row12_col13, #T_ce7ea_row12_col14, #T_ce7ea_row12_col15, #T_ce7ea_row12_col16, #T_ce7ea_row12_col17, #T_ce7ea_row13_col0, #T_ce7ea_row13_col1, #T_ce7ea_row13_col2, #T_ce7ea_row13_col3, #T_ce7ea_row13_col4, #T_ce7ea_row13_col5, #T_ce7ea_row13_col6, #T_ce7ea_row13_col7, #T_ce7ea_row13_col8, #T_ce7ea_row13_col9, #T_ce7ea_row13_col10, #T_ce7ea_row13_col11, #T_ce7ea_row13_col14, #T_ce7ea_row13_col15, #T_ce7ea_row13_col16, #T_ce7ea_row13_col17, #T_ce7ea_row14_col0, #T_ce7ea_row14_col1, #T_ce7ea_row14_col2, #T_ce7ea_row14_col3, #T_ce7ea_row14_col4, #T_ce7ea_row14_col5, #T_ce7ea_row14_col6, #T_ce7ea_row14_col7, #T_ce7ea_row14_col8, #T_ce7ea_row14_col9, #T_ce7ea_row14_col10, #T_ce7ea_row14_col11, #T_ce7ea_row14_col15, #T_ce7ea_row14_col16, #T_ce7ea_row14_col17, #T_ce7ea_row15_col0, #T_ce7ea_row15_col1, #T_ce7ea_row15_col2, #T_ce7ea_row15_col3, #T_ce7ea_row15_col4, #T_ce7ea_row15_col5, #T_ce7ea_row15_col6, #T_ce7ea_row15_col7, #T_ce7ea_row15_col8, #T_ce7ea_row15_col9, #T_ce7ea_row15_col10, #T_ce7ea_row15_col11, #T_ce7ea_row15_col12, #T_ce7ea_row15_col16, #T_ce7ea_row15_col17, #T_ce7ea_row16_col0, #T_ce7ea_row16_col1, #T_ce7ea_row16_col2, #T_ce7ea_row16_col3, #T_ce7ea_row16_col4, #T_ce7ea_row16_col5, #T_ce7ea_row16_col6, #T_ce7ea_row16_col7, #T_ce7ea_row16_col8, #T_ce7ea_row16_col9, #T_ce7ea_row16_col10, #T_ce7ea_row16_col11, #T_ce7ea_row16_col12, #T_ce7ea_row16_col13, #T_ce7ea_row16_col17, #T_ce7ea_row17_col0, #T_ce7ea_row17_col1, #T_ce7ea_row17_col2, #T_ce7ea_row17_col3, #T_ce7ea_row17_col4, #T_ce7ea_row17_col5, #T_ce7ea_row17_col6, #T_ce7ea_row17_col7, #T_ce7ea_row17_col8, #T_ce7ea_row17_col9, #T_ce7ea_row17_col10, #T_ce7ea_row17_col11, #T_ce7ea_row17_col12, #T_ce7ea_row17_col13, #T_ce7ea_row17_col14 {\n  Background-color: #FA8072;\n}\n</style>\n<table id=\"T_ce7ea\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_ce7ea_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n      <th id=\"T_ce7ea_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n      <th id=\"T_ce7ea_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n      <th id=\"T_ce7ea_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n      <th id=\"T_ce7ea_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n      <th id=\"T_ce7ea_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n      <th id=\"T_ce7ea_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n      <th id=\"T_ce7ea_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n      <th id=\"T_ce7ea_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n      <th id=\"T_ce7ea_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n      <th id=\"T_ce7ea_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n      <th id=\"T_ce7ea_level0_col11\" class=\"col_heading level0 col11\" >11</th>\n      <th id=\"T_ce7ea_level0_col12\" class=\"col_heading level0 col12\" >12</th>\n      <th id=\"T_ce7ea_level0_col13\" class=\"col_heading level0 col13\" >13</th>\n      <th id=\"T_ce7ea_level0_col14\" class=\"col_heading level0 col14\" >14</th>\n      <th id=\"T_ce7ea_level0_col15\" class=\"col_heading level0 col15\" >15</th>\n      <th id=\"T_ce7ea_level0_col16\" class=\"col_heading level0 col16\" >16</th>\n      <th id=\"T_ce7ea_level0_col17\" class=\"col_heading level0 col17\" >17</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_ce7ea_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_ce7ea_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n      <td id=\"T_ce7ea_row0_col1\" class=\"data row0 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col2\" class=\"data row0 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col3\" class=\"data row0 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col4\" class=\"data row0 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col5\" class=\"data row0 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col6\" class=\"data row0 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col7\" class=\"data row0 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col8\" class=\"data row0 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col9\" class=\"data row0 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col10\" class=\"data row0 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col11\" class=\"data row0 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col12\" class=\"data row0 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col13\" class=\"data row0 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col14\" class=\"data row0 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col15\" class=\"data row0 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col16\" class=\"data row0 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row0_col17\" class=\"data row0 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_ce7ea_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n      <td id=\"T_ce7ea_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n      <td id=\"T_ce7ea_row1_col2\" class=\"data row1 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col3\" class=\"data row1 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col4\" class=\"data row1 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col5\" class=\"data row1 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col6\" class=\"data row1 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col7\" class=\"data row1 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col8\" class=\"data row1 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col9\" class=\"data row1 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col10\" class=\"data row1 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col11\" class=\"data row1 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col12\" class=\"data row1 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col13\" class=\"data row1 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col14\" class=\"data row1 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col15\" class=\"data row1 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col16\" class=\"data row1 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row1_col17\" class=\"data row1 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_ce7ea_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n      <td id=\"T_ce7ea_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n      <td id=\"T_ce7ea_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n      <td id=\"T_ce7ea_row2_col3\" class=\"data row2 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col4\" class=\"data row2 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col5\" class=\"data row2 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col6\" class=\"data row2 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col7\" class=\"data row2 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col8\" class=\"data row2 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col9\" class=\"data row2 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col10\" class=\"data row2 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col11\" class=\"data row2 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col12\" class=\"data row2 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col13\" class=\"data row2 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col14\" class=\"data row2 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col15\" class=\"data row2 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col16\" class=\"data row2 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row2_col17\" class=\"data row2 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_ce7ea_row3_col0\" class=\"data row3 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n      <td id=\"T_ce7ea_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n      <td id=\"T_ce7ea_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n      <td id=\"T_ce7ea_row3_col4\" class=\"data row3 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col5\" class=\"data row3 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col6\" class=\"data row3 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col7\" class=\"data row3 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col8\" class=\"data row3 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col9\" class=\"data row3 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col10\" class=\"data row3 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col11\" class=\"data row3 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col12\" class=\"data row3 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col13\" class=\"data row3 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col14\" class=\"data row3 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col15\" class=\"data row3 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col16\" class=\"data row3 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row3_col17\" class=\"data row3 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_ce7ea_row4_col0\" class=\"data row4 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col1\" class=\"data row4 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col2\" class=\"data row4 col2\" >0.000000</td>\n      <td id=\"T_ce7ea_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n      <td id=\"T_ce7ea_row4_col4\" class=\"data row4 col4\" >0.000000</td>\n      <td id=\"T_ce7ea_row4_col5\" class=\"data row4 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col6\" class=\"data row4 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col7\" class=\"data row4 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col8\" class=\"data row4 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col9\" class=\"data row4 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col10\" class=\"data row4 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col11\" class=\"data row4 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col12\" class=\"data row4 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col13\" class=\"data row4 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col14\" class=\"data row4 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col15\" class=\"data row4 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col16\" class=\"data row4 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row4_col17\" class=\"data row4 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_ce7ea_row5_col0\" class=\"data row5 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col1\" class=\"data row5 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col2\" class=\"data row5 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col3\" class=\"data row5 col3\" >0.000000</td>\n      <td id=\"T_ce7ea_row5_col4\" class=\"data row5 col4\" >0.000000</td>\n      <td id=\"T_ce7ea_row5_col5\" class=\"data row5 col5\" >0.000000</td>\n      <td id=\"T_ce7ea_row5_col6\" class=\"data row5 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col7\" class=\"data row5 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col8\" class=\"data row5 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col9\" class=\"data row5 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col10\" class=\"data row5 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col11\" class=\"data row5 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col12\" class=\"data row5 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col13\" class=\"data row5 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col14\" class=\"data row5 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col15\" class=\"data row5 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col16\" class=\"data row5 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row5_col17\" class=\"data row5 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_ce7ea_row6_col0\" class=\"data row6 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col1\" class=\"data row6 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col2\" class=\"data row6 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col3\" class=\"data row6 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col4\" class=\"data row6 col4\" >0.000000</td>\n      <td id=\"T_ce7ea_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n      <td id=\"T_ce7ea_row6_col6\" class=\"data row6 col6\" >0.000000</td>\n      <td id=\"T_ce7ea_row6_col7\" class=\"data row6 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col8\" class=\"data row6 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col9\" class=\"data row6 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col10\" class=\"data row6 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col11\" class=\"data row6 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col12\" class=\"data row6 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col13\" class=\"data row6 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col14\" class=\"data row6 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col15\" class=\"data row6 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col16\" class=\"data row6 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row6_col17\" class=\"data row6 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_ce7ea_row7_col0\" class=\"data row7 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col1\" class=\"data row7 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col2\" class=\"data row7 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col3\" class=\"data row7 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col4\" class=\"data row7 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col5\" class=\"data row7 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col6\" class=\"data row7 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col7\" class=\"data row7 col7\" >0.000000</td>\n      <td id=\"T_ce7ea_row7_col8\" class=\"data row7 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col9\" class=\"data row7 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col10\" class=\"data row7 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col11\" class=\"data row7 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col12\" class=\"data row7 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col13\" class=\"data row7 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col14\" class=\"data row7 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col15\" class=\"data row7 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col16\" class=\"data row7 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row7_col17\" class=\"data row7 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n      <td id=\"T_ce7ea_row8_col0\" class=\"data row8 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col1\" class=\"data row8 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col2\" class=\"data row8 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col3\" class=\"data row8 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col4\" class=\"data row8 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col5\" class=\"data row8 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col6\" class=\"data row8 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col7\" class=\"data row8 col7\" >0.000000</td>\n      <td id=\"T_ce7ea_row8_col8\" class=\"data row8 col8\" >0.000000</td>\n      <td id=\"T_ce7ea_row8_col9\" class=\"data row8 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col10\" class=\"data row8 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col11\" class=\"data row8 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col12\" class=\"data row8 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col13\" class=\"data row8 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col14\" class=\"data row8 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col15\" class=\"data row8 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col16\" class=\"data row8 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row8_col17\" class=\"data row8 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n      <td id=\"T_ce7ea_row9_col0\" class=\"data row9 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col1\" class=\"data row9 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col2\" class=\"data row9 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col3\" class=\"data row9 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col4\" class=\"data row9 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col5\" class=\"data row9 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col6\" class=\"data row9 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col7\" class=\"data row9 col7\" >0.000000</td>\n      <td id=\"T_ce7ea_row9_col8\" class=\"data row9 col8\" >0.000000</td>\n      <td id=\"T_ce7ea_row9_col9\" class=\"data row9 col9\" >0.000000</td>\n      <td id=\"T_ce7ea_row9_col10\" class=\"data row9 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col11\" class=\"data row9 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col12\" class=\"data row9 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col13\" class=\"data row9 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col14\" class=\"data row9 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col15\" class=\"data row9 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col16\" class=\"data row9 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row9_col17\" class=\"data row9 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n      <td id=\"T_ce7ea_row10_col0\" class=\"data row10 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col1\" class=\"data row10 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col2\" class=\"data row10 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col3\" class=\"data row10 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col4\" class=\"data row10 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col5\" class=\"data row10 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col6\" class=\"data row10 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col7\" class=\"data row10 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col8\" class=\"data row10 col8\" >0.000000</td>\n      <td id=\"T_ce7ea_row10_col9\" class=\"data row10 col9\" >0.000000</td>\n      <td id=\"T_ce7ea_row10_col10\" class=\"data row10 col10\" >0.000000</td>\n      <td id=\"T_ce7ea_row10_col11\" class=\"data row10 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col12\" class=\"data row10 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col13\" class=\"data row10 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col14\" class=\"data row10 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col15\" class=\"data row10 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col16\" class=\"data row10 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row10_col17\" class=\"data row10 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n      <td id=\"T_ce7ea_row11_col0\" class=\"data row11 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col1\" class=\"data row11 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col2\" class=\"data row11 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col3\" class=\"data row11 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col4\" class=\"data row11 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col5\" class=\"data row11 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col6\" class=\"data row11 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col7\" class=\"data row11 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col8\" class=\"data row11 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col9\" class=\"data row11 col9\" >0.000000</td>\n      <td id=\"T_ce7ea_row11_col10\" class=\"data row11 col10\" >0.000000</td>\n      <td id=\"T_ce7ea_row11_col11\" class=\"data row11 col11\" >0.000000</td>\n      <td id=\"T_ce7ea_row11_col12\" class=\"data row11 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col13\" class=\"data row11 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col14\" class=\"data row11 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col15\" class=\"data row11 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col16\" class=\"data row11 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row11_col17\" class=\"data row11 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n      <td id=\"T_ce7ea_row12_col0\" class=\"data row12 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col1\" class=\"data row12 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col2\" class=\"data row12 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col3\" class=\"data row12 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col4\" class=\"data row12 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col5\" class=\"data row12 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col6\" class=\"data row12 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col7\" class=\"data row12 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col8\" class=\"data row12 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col9\" class=\"data row12 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col10\" class=\"data row12 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col11\" class=\"data row12 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col12\" class=\"data row12 col12\" >0.000000</td>\n      <td id=\"T_ce7ea_row12_col13\" class=\"data row12 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col14\" class=\"data row12 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col15\" class=\"data row12 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col16\" class=\"data row12 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row12_col17\" class=\"data row12 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n      <td id=\"T_ce7ea_row13_col0\" class=\"data row13 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col1\" class=\"data row13 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col2\" class=\"data row13 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col3\" class=\"data row13 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col4\" class=\"data row13 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col5\" class=\"data row13 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col6\" class=\"data row13 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col7\" class=\"data row13 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col8\" class=\"data row13 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col9\" class=\"data row13 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col10\" class=\"data row13 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col11\" class=\"data row13 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col12\" class=\"data row13 col12\" >0.000000</td>\n      <td id=\"T_ce7ea_row13_col13\" class=\"data row13 col13\" >0.000000</td>\n      <td id=\"T_ce7ea_row13_col14\" class=\"data row13 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col15\" class=\"data row13 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col16\" class=\"data row13 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row13_col17\" class=\"data row13 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n      <td id=\"T_ce7ea_row14_col0\" class=\"data row14 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col1\" class=\"data row14 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col2\" class=\"data row14 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col3\" class=\"data row14 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col4\" class=\"data row14 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col5\" class=\"data row14 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col6\" class=\"data row14 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col7\" class=\"data row14 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col8\" class=\"data row14 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col9\" class=\"data row14 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col10\" class=\"data row14 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col11\" class=\"data row14 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col12\" class=\"data row14 col12\" >0.000000</td>\n      <td id=\"T_ce7ea_row14_col13\" class=\"data row14 col13\" >0.000000</td>\n      <td id=\"T_ce7ea_row14_col14\" class=\"data row14 col14\" >0.000000</td>\n      <td id=\"T_ce7ea_row14_col15\" class=\"data row14 col15\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col16\" class=\"data row14 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row14_col17\" class=\"data row14 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n      <td id=\"T_ce7ea_row15_col0\" class=\"data row15 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col1\" class=\"data row15 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col2\" class=\"data row15 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col3\" class=\"data row15 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col4\" class=\"data row15 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col5\" class=\"data row15 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col6\" class=\"data row15 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col7\" class=\"data row15 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col8\" class=\"data row15 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col9\" class=\"data row15 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col10\" class=\"data row15 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col11\" class=\"data row15 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col12\" class=\"data row15 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col13\" class=\"data row15 col13\" >0.000000</td>\n      <td id=\"T_ce7ea_row15_col14\" class=\"data row15 col14\" >0.000000</td>\n      <td id=\"T_ce7ea_row15_col15\" class=\"data row15 col15\" >0.000000</td>\n      <td id=\"T_ce7ea_row15_col16\" class=\"data row15 col16\" >-inf</td>\n      <td id=\"T_ce7ea_row15_col17\" class=\"data row15 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n      <td id=\"T_ce7ea_row16_col0\" class=\"data row16 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col1\" class=\"data row16 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col2\" class=\"data row16 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col3\" class=\"data row16 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col4\" class=\"data row16 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col5\" class=\"data row16 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col6\" class=\"data row16 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col7\" class=\"data row16 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col8\" class=\"data row16 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col9\" class=\"data row16 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col10\" class=\"data row16 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col11\" class=\"data row16 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col12\" class=\"data row16 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col13\" class=\"data row16 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row16_col14\" class=\"data row16 col14\" >0.000000</td>\n      <td id=\"T_ce7ea_row16_col15\" class=\"data row16 col15\" >0.000000</td>\n      <td id=\"T_ce7ea_row16_col16\" class=\"data row16 col16\" >0.000000</td>\n      <td id=\"T_ce7ea_row16_col17\" class=\"data row16 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_ce7ea_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n      <td id=\"T_ce7ea_row17_col0\" class=\"data row17 col0\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col1\" class=\"data row17 col1\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col2\" class=\"data row17 col2\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col3\" class=\"data row17 col3\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col4\" class=\"data row17 col4\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col5\" class=\"data row17 col5\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col6\" class=\"data row17 col6\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col7\" class=\"data row17 col7\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col8\" class=\"data row17 col8\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col9\" class=\"data row17 col9\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col10\" class=\"data row17 col10\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col11\" class=\"data row17 col11\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col12\" class=\"data row17 col12\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col13\" class=\"data row17 col13\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col14\" class=\"data row17 col14\" >-inf</td>\n      <td id=\"T_ce7ea_row17_col15\" class=\"data row17 col15\" >0.000000</td>\n      <td id=\"T_ce7ea_row17_col16\" class=\"data row17 col16\" >0.000000</td>\n      <td id=\"T_ce7ea_row17_col17\" class=\"data row17 col17\" >0.000000</td>\n    </tr>\n  </tbody>\n</table>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "w1t2MdmR1rcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BlockDiagonalMask\n",
        "\n",
        "q_seqlens = [3, 5]\n",
        "kv_seqlens = [10, 8] # (3 + 7, 5 + 3)\n",
        "sliding_window_size = 3\n",
        "\n",
        "mask = BlockDiagonalMask.from_seqlens(q_seqlens, kv_seqlens).make_local_attention_from_bottomright(sliding_window_size)\n",
        "\n",
        "batch_size = 1\n",
        "total_seq_len = sum(q_seqlens)\n",
        "total_kv_seq_len = sum(kv_seqlens)\n",
        "mask_tensor = mask.materialize((batch_size, total_seq_len, total_kv_seq_len))\n",
        "\n",
        "df = pd.DataFrame(mask_tensor[0, :, :].numpy())\n",
        "df.style.applymap(colour_cell)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T20:03:07.243702Z",
          "iopub.execute_input": "2024-08-25T20:03:07.244846Z",
          "iopub.status.idle": "2024-08-25T20:03:07.267047Z",
          "shell.execute_reply.started": "2024-08-25T20:03:07.2448Z",
          "shell.execute_reply": "2024-08-25T20:03:07.265762Z"
        },
        "trusted": true,
        "id": "d5OBrAzV1rcY",
        "outputId": "79c24b95-d0fd-42fb-a949-52862e57b821"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/1506970582.py:15: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n  df.style.applymap(colour_cell)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 38,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<pandas.io.formats.style.Styler at 0x7ac2befde3e0>",
            "text/html": "<style type=\"text/css\">\n#T_d4ad9_row0_col0, #T_d4ad9_row0_col1, #T_d4ad9_row0_col2, #T_d4ad9_row0_col3, #T_d4ad9_row0_col4, #T_d4ad9_row0_col8, #T_d4ad9_row0_col9, #T_d4ad9_row0_col10, #T_d4ad9_row0_col11, #T_d4ad9_row0_col12, #T_d4ad9_row0_col13, #T_d4ad9_row0_col14, #T_d4ad9_row0_col15, #T_d4ad9_row0_col16, #T_d4ad9_row0_col17, #T_d4ad9_row1_col0, #T_d4ad9_row1_col1, #T_d4ad9_row1_col2, #T_d4ad9_row1_col3, #T_d4ad9_row1_col4, #T_d4ad9_row1_col5, #T_d4ad9_row1_col9, #T_d4ad9_row1_col10, #T_d4ad9_row1_col11, #T_d4ad9_row1_col12, #T_d4ad9_row1_col13, #T_d4ad9_row1_col14, #T_d4ad9_row1_col15, #T_d4ad9_row1_col16, #T_d4ad9_row1_col17, #T_d4ad9_row2_col0, #T_d4ad9_row2_col1, #T_d4ad9_row2_col2, #T_d4ad9_row2_col3, #T_d4ad9_row2_col4, #T_d4ad9_row2_col5, #T_d4ad9_row2_col6, #T_d4ad9_row2_col10, #T_d4ad9_row2_col11, #T_d4ad9_row2_col12, #T_d4ad9_row2_col13, #T_d4ad9_row2_col14, #T_d4ad9_row2_col15, #T_d4ad9_row2_col16, #T_d4ad9_row2_col17, #T_d4ad9_row3_col0, #T_d4ad9_row3_col1, #T_d4ad9_row3_col2, #T_d4ad9_row3_col3, #T_d4ad9_row3_col4, #T_d4ad9_row3_col5, #T_d4ad9_row3_col6, #T_d4ad9_row3_col7, #T_d4ad9_row3_col8, #T_d4ad9_row3_col9, #T_d4ad9_row3_col10, #T_d4ad9_row3_col14, #T_d4ad9_row3_col15, #T_d4ad9_row3_col16, #T_d4ad9_row3_col17, #T_d4ad9_row4_col0, #T_d4ad9_row4_col1, #T_d4ad9_row4_col2, #T_d4ad9_row4_col3, #T_d4ad9_row4_col4, #T_d4ad9_row4_col5, #T_d4ad9_row4_col6, #T_d4ad9_row4_col7, #T_d4ad9_row4_col8, #T_d4ad9_row4_col9, #T_d4ad9_row4_col10, #T_d4ad9_row4_col11, #T_d4ad9_row4_col15, #T_d4ad9_row4_col16, #T_d4ad9_row4_col17, #T_d4ad9_row5_col0, #T_d4ad9_row5_col1, #T_d4ad9_row5_col2, #T_d4ad9_row5_col3, #T_d4ad9_row5_col4, #T_d4ad9_row5_col5, #T_d4ad9_row5_col6, #T_d4ad9_row5_col7, #T_d4ad9_row5_col8, #T_d4ad9_row5_col9, #T_d4ad9_row5_col10, #T_d4ad9_row5_col11, #T_d4ad9_row5_col12, #T_d4ad9_row5_col16, #T_d4ad9_row5_col17, #T_d4ad9_row6_col0, #T_d4ad9_row6_col1, #T_d4ad9_row6_col2, #T_d4ad9_row6_col3, #T_d4ad9_row6_col4, #T_d4ad9_row6_col5, #T_d4ad9_row6_col6, #T_d4ad9_row6_col7, #T_d4ad9_row6_col8, #T_d4ad9_row6_col9, #T_d4ad9_row6_col10, #T_d4ad9_row6_col11, #T_d4ad9_row6_col12, #T_d4ad9_row6_col13, #T_d4ad9_row6_col17, #T_d4ad9_row7_col0, #T_d4ad9_row7_col1, #T_d4ad9_row7_col2, #T_d4ad9_row7_col3, #T_d4ad9_row7_col4, #T_d4ad9_row7_col5, #T_d4ad9_row7_col6, #T_d4ad9_row7_col7, #T_d4ad9_row7_col8, #T_d4ad9_row7_col9, #T_d4ad9_row7_col10, #T_d4ad9_row7_col11, #T_d4ad9_row7_col12, #T_d4ad9_row7_col13, #T_d4ad9_row7_col14 {\n  Background-color: #FA8072;\n}\n#T_d4ad9_row0_col5, #T_d4ad9_row0_col6, #T_d4ad9_row0_col7, #T_d4ad9_row1_col6, #T_d4ad9_row1_col7, #T_d4ad9_row1_col8, #T_d4ad9_row2_col7, #T_d4ad9_row2_col8, #T_d4ad9_row2_col9, #T_d4ad9_row3_col11, #T_d4ad9_row3_col12, #T_d4ad9_row3_col13, #T_d4ad9_row4_col12, #T_d4ad9_row4_col13, #T_d4ad9_row4_col14, #T_d4ad9_row5_col13, #T_d4ad9_row5_col14, #T_d4ad9_row5_col15, #T_d4ad9_row6_col14, #T_d4ad9_row6_col15, #T_d4ad9_row6_col16, #T_d4ad9_row7_col15, #T_d4ad9_row7_col16, #T_d4ad9_row7_col17 {\n  Background-color: #90EE90;\n}\n</style>\n<table id=\"T_d4ad9\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_d4ad9_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n      <th id=\"T_d4ad9_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n      <th id=\"T_d4ad9_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n      <th id=\"T_d4ad9_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n      <th id=\"T_d4ad9_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n      <th id=\"T_d4ad9_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n      <th id=\"T_d4ad9_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n      <th id=\"T_d4ad9_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n      <th id=\"T_d4ad9_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n      <th id=\"T_d4ad9_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n      <th id=\"T_d4ad9_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n      <th id=\"T_d4ad9_level0_col11\" class=\"col_heading level0 col11\" >11</th>\n      <th id=\"T_d4ad9_level0_col12\" class=\"col_heading level0 col12\" >12</th>\n      <th id=\"T_d4ad9_level0_col13\" class=\"col_heading level0 col13\" >13</th>\n      <th id=\"T_d4ad9_level0_col14\" class=\"col_heading level0 col14\" >14</th>\n      <th id=\"T_d4ad9_level0_col15\" class=\"col_heading level0 col15\" >15</th>\n      <th id=\"T_d4ad9_level0_col16\" class=\"col_heading level0 col16\" >16</th>\n      <th id=\"T_d4ad9_level0_col17\" class=\"col_heading level0 col17\" >17</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_d4ad9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_d4ad9_row0_col0\" class=\"data row0 col0\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col1\" class=\"data row0 col1\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col2\" class=\"data row0 col2\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col3\" class=\"data row0 col3\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col4\" class=\"data row0 col4\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n      <td id=\"T_d4ad9_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n      <td id=\"T_d4ad9_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n      <td id=\"T_d4ad9_row0_col8\" class=\"data row0 col8\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col9\" class=\"data row0 col9\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col10\" class=\"data row0 col10\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col11\" class=\"data row0 col11\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col12\" class=\"data row0 col12\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col13\" class=\"data row0 col13\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col14\" class=\"data row0 col14\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col15\" class=\"data row0 col15\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col16\" class=\"data row0 col16\" >-inf</td>\n      <td id=\"T_d4ad9_row0_col17\" class=\"data row0 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_d4ad9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_d4ad9_row1_col0\" class=\"data row1 col0\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col1\" class=\"data row1 col1\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col2\" class=\"data row1 col2\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col3\" class=\"data row1 col3\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col4\" class=\"data row1 col4\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col5\" class=\"data row1 col5\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n      <td id=\"T_d4ad9_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n      <td id=\"T_d4ad9_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n      <td id=\"T_d4ad9_row1_col9\" class=\"data row1 col9\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col10\" class=\"data row1 col10\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col11\" class=\"data row1 col11\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col12\" class=\"data row1 col12\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col13\" class=\"data row1 col13\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col14\" class=\"data row1 col14\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col15\" class=\"data row1 col15\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col16\" class=\"data row1 col16\" >-inf</td>\n      <td id=\"T_d4ad9_row1_col17\" class=\"data row1 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_d4ad9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_d4ad9_row2_col0\" class=\"data row2 col0\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col1\" class=\"data row2 col1\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col2\" class=\"data row2 col2\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col3\" class=\"data row2 col3\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col4\" class=\"data row2 col4\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col5\" class=\"data row2 col5\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col6\" class=\"data row2 col6\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n      <td id=\"T_d4ad9_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n      <td id=\"T_d4ad9_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n      <td id=\"T_d4ad9_row2_col10\" class=\"data row2 col10\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col11\" class=\"data row2 col11\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col12\" class=\"data row2 col12\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col13\" class=\"data row2 col13\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col14\" class=\"data row2 col14\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col15\" class=\"data row2 col15\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col16\" class=\"data row2 col16\" >-inf</td>\n      <td id=\"T_d4ad9_row2_col17\" class=\"data row2 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_d4ad9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_d4ad9_row3_col0\" class=\"data row3 col0\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col1\" class=\"data row3 col1\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col2\" class=\"data row3 col2\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col3\" class=\"data row3 col3\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col4\" class=\"data row3 col4\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col5\" class=\"data row3 col5\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col6\" class=\"data row3 col6\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col7\" class=\"data row3 col7\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col8\" class=\"data row3 col8\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col9\" class=\"data row3 col9\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col10\" class=\"data row3 col10\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col11\" class=\"data row3 col11\" >0.000000</td>\n      <td id=\"T_d4ad9_row3_col12\" class=\"data row3 col12\" >0.000000</td>\n      <td id=\"T_d4ad9_row3_col13\" class=\"data row3 col13\" >0.000000</td>\n      <td id=\"T_d4ad9_row3_col14\" class=\"data row3 col14\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col15\" class=\"data row3 col15\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col16\" class=\"data row3 col16\" >-inf</td>\n      <td id=\"T_d4ad9_row3_col17\" class=\"data row3 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_d4ad9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_d4ad9_row4_col0\" class=\"data row4 col0\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col1\" class=\"data row4 col1\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col2\" class=\"data row4 col2\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col3\" class=\"data row4 col3\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col4\" class=\"data row4 col4\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col5\" class=\"data row4 col5\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col6\" class=\"data row4 col6\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col7\" class=\"data row4 col7\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col8\" class=\"data row4 col8\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col9\" class=\"data row4 col9\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col10\" class=\"data row4 col10\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col11\" class=\"data row4 col11\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col12\" class=\"data row4 col12\" >0.000000</td>\n      <td id=\"T_d4ad9_row4_col13\" class=\"data row4 col13\" >0.000000</td>\n      <td id=\"T_d4ad9_row4_col14\" class=\"data row4 col14\" >0.000000</td>\n      <td id=\"T_d4ad9_row4_col15\" class=\"data row4 col15\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col16\" class=\"data row4 col16\" >-inf</td>\n      <td id=\"T_d4ad9_row4_col17\" class=\"data row4 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_d4ad9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_d4ad9_row5_col0\" class=\"data row5 col0\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col1\" class=\"data row5 col1\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col2\" class=\"data row5 col2\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col3\" class=\"data row5 col3\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col4\" class=\"data row5 col4\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col5\" class=\"data row5 col5\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col6\" class=\"data row5 col6\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col7\" class=\"data row5 col7\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col8\" class=\"data row5 col8\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col9\" class=\"data row5 col9\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col10\" class=\"data row5 col10\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col11\" class=\"data row5 col11\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col12\" class=\"data row5 col12\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col13\" class=\"data row5 col13\" >0.000000</td>\n      <td id=\"T_d4ad9_row5_col14\" class=\"data row5 col14\" >0.000000</td>\n      <td id=\"T_d4ad9_row5_col15\" class=\"data row5 col15\" >0.000000</td>\n      <td id=\"T_d4ad9_row5_col16\" class=\"data row5 col16\" >-inf</td>\n      <td id=\"T_d4ad9_row5_col17\" class=\"data row5 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_d4ad9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_d4ad9_row6_col0\" class=\"data row6 col0\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col1\" class=\"data row6 col1\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col2\" class=\"data row6 col2\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col3\" class=\"data row6 col3\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col4\" class=\"data row6 col4\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col5\" class=\"data row6 col5\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col6\" class=\"data row6 col6\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col7\" class=\"data row6 col7\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col8\" class=\"data row6 col8\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col9\" class=\"data row6 col9\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col10\" class=\"data row6 col10\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col11\" class=\"data row6 col11\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col12\" class=\"data row6 col12\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col13\" class=\"data row6 col13\" >-inf</td>\n      <td id=\"T_d4ad9_row6_col14\" class=\"data row6 col14\" >0.000000</td>\n      <td id=\"T_d4ad9_row6_col15\" class=\"data row6 col15\" >0.000000</td>\n      <td id=\"T_d4ad9_row6_col16\" class=\"data row6 col16\" >0.000000</td>\n      <td id=\"T_d4ad9_row6_col17\" class=\"data row6 col17\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_d4ad9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_d4ad9_row7_col0\" class=\"data row7 col0\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col1\" class=\"data row7 col1\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col2\" class=\"data row7 col2\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col3\" class=\"data row7 col3\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col4\" class=\"data row7 col4\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col5\" class=\"data row7 col5\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col6\" class=\"data row7 col6\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col7\" class=\"data row7 col7\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col8\" class=\"data row7 col8\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col9\" class=\"data row7 col9\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col10\" class=\"data row7 col10\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col11\" class=\"data row7 col11\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col12\" class=\"data row7 col12\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col13\" class=\"data row7 col13\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col14\" class=\"data row7 col14\" >-inf</td>\n      <td id=\"T_d4ad9_row7_col15\" class=\"data row7 col15\" >0.000000</td>\n      <td id=\"T_d4ad9_row7_col16\" class=\"data row7 col16\" >0.000000</td>\n      <td id=\"T_d4ad9_row7_col17\" class=\"data row7 col17\" >0.000000</td>\n    </tr>\n  </tbody>\n</table>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## BlockDiagonalCausalWithOffsetPaddedKeysMask\n",
        "\n",
        "# We use this mask with padding because the overall size of the KV-Cache is the same for all the prompts, but for each KV-Cache we may need to use only some of the items.\n",
        "\n",
        "q_seqlen = [1, 1]\n",
        "kv_seq_len = [3, 5]\n",
        "kv_padding = 6\n",
        "\n",
        "mask = BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens(q_seqlen=q_seqlen, kv_padding=kv_padding, kv_seqlen=kv_seq_len)\n",
        "\n",
        "batch_size = 1\n",
        "total_seq_len = sum(q_seqlen)\n",
        "total_kv_seq_len = kv_padding * len(kv_seq_len)\n",
        "\n",
        "mask_tensor = mask.materialize((batch_size, total_seq_len, total_kv_seq_len))\n",
        "\n",
        "df = pd.DataFrame(mask_tensor[0, :, :].numpy())\n",
        "df.style.applymap(colour_cell)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-25T20:03:13.558652Z",
          "iopub.execute_input": "2024-08-25T20:03:13.559486Z",
          "iopub.status.idle": "2024-08-25T20:03:13.575737Z",
          "shell.execute_reply.started": "2024-08-25T20:03:13.559438Z",
          "shell.execute_reply": "2024-08-25T20:03:13.574681Z"
        },
        "trusted": true,
        "id": "zMF3bNe01rcZ",
        "outputId": "860ffdb7-85e4-4841-e6b4-9b9646129aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_36/2290532406.py:18: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n  df.style.applymap(colour_cell)\n",
          "output_type": "stream"
        },
        {
          "execution_count": 39,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<pandas.io.formats.style.Styler at 0x7ac2be3e8220>",
            "text/html": "<style type=\"text/css\">\n#T_8a986_row0_col0, #T_8a986_row0_col1, #T_8a986_row0_col2, #T_8a986_row1_col6, #T_8a986_row1_col7, #T_8a986_row1_col8, #T_8a986_row1_col9, #T_8a986_row1_col10 {\n  Background-color: #90EE90;\n}\n#T_8a986_row0_col3, #T_8a986_row0_col4, #T_8a986_row0_col5, #T_8a986_row0_col6, #T_8a986_row0_col7, #T_8a986_row0_col8, #T_8a986_row0_col9, #T_8a986_row0_col10, #T_8a986_row0_col11, #T_8a986_row1_col0, #T_8a986_row1_col1, #T_8a986_row1_col2, #T_8a986_row1_col3, #T_8a986_row1_col4, #T_8a986_row1_col5, #T_8a986_row1_col11 {\n  Background-color: #FA8072;\n}\n</style>\n<table id=\"T_8a986\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_8a986_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n      <th id=\"T_8a986_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n      <th id=\"T_8a986_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n      <th id=\"T_8a986_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n      <th id=\"T_8a986_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n      <th id=\"T_8a986_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n      <th id=\"T_8a986_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n      <th id=\"T_8a986_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n      <th id=\"T_8a986_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n      <th id=\"T_8a986_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n      <th id=\"T_8a986_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n      <th id=\"T_8a986_level0_col11\" class=\"col_heading level0 col11\" >11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_8a986_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_8a986_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n      <td id=\"T_8a986_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n      <td id=\"T_8a986_row0_col2\" class=\"data row0 col2\" >0.000000</td>\n      <td id=\"T_8a986_row0_col3\" class=\"data row0 col3\" >-inf</td>\n      <td id=\"T_8a986_row0_col4\" class=\"data row0 col4\" >-inf</td>\n      <td id=\"T_8a986_row0_col5\" class=\"data row0 col5\" >-inf</td>\n      <td id=\"T_8a986_row0_col6\" class=\"data row0 col6\" >-inf</td>\n      <td id=\"T_8a986_row0_col7\" class=\"data row0 col7\" >-inf</td>\n      <td id=\"T_8a986_row0_col8\" class=\"data row0 col8\" >-inf</td>\n      <td id=\"T_8a986_row0_col9\" class=\"data row0 col9\" >-inf</td>\n      <td id=\"T_8a986_row0_col10\" class=\"data row0 col10\" >-inf</td>\n      <td id=\"T_8a986_row0_col11\" class=\"data row0 col11\" >-inf</td>\n    </tr>\n    <tr>\n      <th id=\"T_8a986_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_8a986_row1_col0\" class=\"data row1 col0\" >-inf</td>\n      <td id=\"T_8a986_row1_col1\" class=\"data row1 col1\" >-inf</td>\n      <td id=\"T_8a986_row1_col2\" class=\"data row1 col2\" >-inf</td>\n      <td id=\"T_8a986_row1_col3\" class=\"data row1 col3\" >-inf</td>\n      <td id=\"T_8a986_row1_col4\" class=\"data row1 col4\" >-inf</td>\n      <td id=\"T_8a986_row1_col5\" class=\"data row1 col5\" >-inf</td>\n      <td id=\"T_8a986_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n      <td id=\"T_8a986_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n      <td id=\"T_8a986_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n      <td id=\"T_8a986_row1_col9\" class=\"data row1 col9\" >0.000000</td>\n      <td id=\"T_8a986_row1_col10\" class=\"data row1 col10\" >0.000000</td>\n      <td id=\"T_8a986_row1_col11\" class=\"data row1 col11\" >-inf</td>\n    </tr>\n  </tbody>\n</table>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "llama_model.py --> without use of fairscale parallelism in Attention mechanism\n",
        "\n",
        "Transformer\n",
        "==================================\n",
        "1. nn.Embedding(vocab,dim) : token_embeddings\n",
        "2. nn.ModuleList(TransformerBlock(args) X n_layers) # different LLM architecture implemented\n",
        "3. RMSNorm(dim,eps)\n",
        "4. nn.Linear(dim,vocab) : output\n",
        "5. precompute_freqs_pos_frequencis(dim//n_heads,max_seq*2,device,rope_theta) : freqs_complex # per attention head\n",
        "# i.e dim // n_heads = head_dim , for Llama rope_theta is not passed\n",
        "\n",
        "@property # helpful for training-inference device switch error\n",
        "dtype() : return data type of parameter of the model\n",
        "device() : device on which model parameters are stored\n",
        "\n",
        "\n",
        "forward()\n",
        "----------------------------------------------\n",
        "\"\"\"\n",
        "note that with the KV Cache, only need the latest tokens, no need all tokens: info about previous tokens are saved in the cache\n",
        "NOTE: this is only for inference, not training (in training there's no KV cache)\n",
        "\"\"\"\n",
        "1. get hidden_state = token_embeddings(tokens) # (B,seq) -> (B,seq,dim) [1 token at a time --> map to dim]\n",
        "2. retrive pairs(m,theta) corresponding to position [start_pos, start_pos + seq_len] from freqs_complex\n",
        "3. hidden_states = layer(hidden_states,start_pos,freqs_complex) for layer in layers\n",
        "# apply precomputed frequencies to the encoding layers for positional encoding , each layer is (Nx transformer blocks)\n",
        "4. apply RMSNorm on combined hidden_states from step 3\n",
        "5. pass through output\n",
        "\n",
        "\n",
        "TransformerBlock\n",
        "===============================================================\n",
        "1. head_dim = dim//n_heads\n",
        "2. SelfAttention(args) : attention # Decoder only with causal attention (only work for inference)\n",
        "3. FeedForward(args) : feed_forward\n",
        "4. RMSNorm(dim,eps) : attention_norm # RMSNorm before attention\n",
        "5. RMSNorm(dim,eps) : ffn_norm # RMSNorm befor feed_forwardd\n",
        "\n",
        "\n",
        "forward()\n",
        "-----------------------------------------------------------------\n",
        "1. hidden_states = x + attention.forward(attention_norm(x),start_pos,freqs_complex)\n",
        "# (B, seq_Len, dim) + (B, seq_Len, dim) => (B, seq_Len, dim) we're dealing\n",
        "# with 1 token at a time start_pos : current token we're dealing\n",
        "2. out = hidden_states + feed_forward(ffn_norm(hidden_states)) # (B, seq_Len, dim) + (B, seq_Len, dim) => (B, seq_Len, dim)\n",
        "3. return out\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "jI5b0pUS1rcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####"
      ],
      "metadata": {
        "id": "X26vet-81rcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####"
      ],
      "metadata": {
        "id": "k0xHnGaK1rce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Transformer\n",
        "\n",
        "```python\n",
        "\n",
        "1. nn.Embedding(vocab,dim) : token_embeddings\n",
        "2. nn.ModuleList(TransformerBlock(args) X n_layers) # different LLM architecture implemented\n",
        "3. RMSNorm(dim,eps)\n",
        "4. nn.Linear(dim,vocab) : output\n",
        "5. precompute_freqs_pos_frequencis(dim//n_heads,max_seq*2,device,rope_theta) : freqs_complex # per attention head\n",
        "# i.e dim // n_heads = head_dim , for Llama rope_theta is not passed\n",
        "\n",
        "@property # helpful for training-inference device switch error\n",
        "dtype() : return data type of parameter of the model\n",
        "device() : device on which model parameters are stored\n",
        "\n",
        "\n",
        "forward()\n",
        "----------------------------------------------\n",
        "\"\"\"\n",
        "note that with the KV Cache, only need the latest tokens, no need all tokens: info about previous tokens are saved in the cache\n",
        "NOTE: this is only for inference, not training (in training there's no KV cache)\n",
        "\"\"\"\n",
        "1. get hidden_state = token_embeddings(tokens) # (B,seq) -> (B,seq,dim) [1 token at a time --> map to dim]\n",
        "2. retrive pairs(m,theta) corresponding to position [start_pos, start_pos + seq_len] from freqs_complex\n",
        "3. hidden_states = layer(hidden_states,start_pos,freqs_complex) for layer in layers\n",
        "# apply precomputed frequencies to the encoding layers for positional encoding , each layer is (Nx transformer blocks)\n",
        "4. apply RMSNorm on combined hidden_states from step 3\n",
        "5. pass through output\n",
        "\n",
        "```\n",
        "\n",
        "# TransformerBlock\n",
        "\n",
        "```python\n",
        "\n",
        "\n",
        "\"\"\" a single transformer block (different for Llama & Mistral) \"\"\"\n",
        "1. head_dim = dim//n_heads\n",
        "2. RMSNorm(dim,eps) : rms_norm # before attention & feed_forwardd\n",
        "2. SelfAttention(args) : attention # Decoder only with causal attention (only work for inference)\n",
        "3. MOE(experts=[FeedForward(args) X moe.n_experts,gate=nn.Linear(dim,moe.n_experts),args.moe]\n",
        "# Feed Forward Layer (with MoE support) otherwise FeedForward(args) : feed_forwad\n",
        "\n",
        "\n",
        "forward()\n",
        "-----------------------------------------------------------------\n",
        "1. hidden_states = x + attention.forward(rms_norm(x),start_pos,freqs_complex)\n",
        "# (B, seq_Len, dim) + (B, seq_Len, dim) => (B, seq_Len, dim) we're dealing\n",
        "# with 1 token at a time start_pos : current token we're dealing\n",
        "2. out = hidden_states + feed_forward(rms_norm(hidden_states)) # (B, seq_Len, dim) + (B, seq_Len, dim) => (B, seq_Len, dim)\n",
        "3. return out\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "XNagYYGu1rcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSNorm\n",
        "```python\n",
        "\n",
        "\"\"\"same for both\"\"\"\n",
        "1. nn.Parameter(torch.ones(dim)) : weight\n",
        "# gamma(g) parameter trainable to perform rescaling on the norm\n",
        "\n",
        "_norm()\n",
        "-------------------------------------------------------------------------\n",
        "# RMSNorm stat : (B, Seq_Len, Dim) * (B, Seq_Len, 1) = (B, Seq_Len, Dim)\n",
        "1. return x * 1/rms\n",
        "\n",
        "forward()\n",
        "-------------------------------------------------------------------------\n",
        "# (Dim) * (B, Seq_Len, Dim) = (B, Seq_Len, Dim)\n",
        "# auto-broadcasting expands (Dim) to (1, 1, Dim) to multiplied to the last dimension of (B, Seq_Len, Dim)\n",
        "# recall: Automatic broadcasting in PyTorch occurs when dimensions match or are broadcastable starting from the trailing dimensions (i.e., from right to left)\n",
        "1. weight * _norm(x) # typecast to float & verify if it's same type as x\n",
        "\n",
        "```\n",
        "\n",
        "# FeedForward\n",
        "\n",
        "```python\n",
        "\n",
        "\"\"\"same for both but Llama uses multiple_of parameter for hidden_dim\"\"\"\n",
        "1. hidden_dim = multiple_of * ((int(hidden_dim) + multiple_of - 1) // multiple_of\n",
        "# round hidden_dim to the nearest multiple of the args.multiple_of parameter (bigger or equal)\n",
        "# just a design choice to look cool :)\n",
        "1. gate : w1(dim,hidden_dim) , up : w2(hidden_dim,dim), down : w3(dim,hidden_dim)\n",
        "\n",
        "forward()\n",
        "---------------------------------------------------------------\n",
        "1. xw1 = w1(x)\n",
        "# (S(XW1) * XV)XW2 = (ss)XW2; goal shape: (B, seq_len, Dim) => (B, seq_len, Dim)\n",
        "# (B, seq_len, Dim) w1=> (B, seq_len, Hidden_Dim)\n",
        "2. sxw1 = silu(xw1)\n",
        "# (B, seq_len, Hidden_Dim) => (B, seq_len, Hidden_Dim) # silu\n",
        "3. xv = w3(x)\n",
        "# (B, seq_len, Dim) w3=> (B, seq_len, Hidden_Dim)\n",
        "4. sxw1xv = sxw1 * xv\n",
        "# (B, seq_len, Hidden_Dim) * (B, seq_len, Hidden_Dim) = (B, seq_len, Hidden_Dim) = element wise multiplication\n",
        "5. return w2(sxw1xv)\n",
        "# (B, seq_len, Hidden_Dim) w2=> (B, seq_len, Dim)\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "DBsjye_V1rci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOE\n",
        "```python\n",
        "\"\"\"the only difference in Mixtral MOE: after attention, instead of RMS=>MLP, it has RMS=>MOE\"\"\"\n",
        "1. nn.ModuleList(experts) : experts\n",
        "2. gate , moe_args\n",
        "\n",
        "forward()\n",
        "------------------------------------------------------------------------------------\n",
        "\"\"\"NOTE: in the mistral paper, all input/output size used are (B * seq_len, Dim) instead of (B, seq_len, Dim)\"\"\"\n",
        "# goal shape: (B, seq_len, Dim) = > (B, seq_len, Dim)\n",
        "1. flat_x = reshape input shape to (B * seq_len, Dim)\n",
        "2. gate_logic = gate(flat_x)  # recall gate is linear with (Dim, n_experts)\n",
        "# (B * seq_len, Dim) gate=> (B * seq_len, n_experts) for each input token\n",
        "3. weights, selected_experts = torch.topk(gate_logits, self.moe_args.n_experts_per_tok)\n",
        "# Get the top k experts for each input token, using torch.topk\n",
        "# weights=logits, selected_experts=indices\n",
        "# (B * seq_len, n_experts) => (B * seq_len, n_experts_per_tok)\n",
        "4. weights = F.softmax(weights, dim=1, dtype=torch.float).to(x.dtype)\n",
        "# # Normalize the weights with softmax, to get the selected top k experts' weights on the tokens\n",
        "5. results = torch.zeros_like(flat_x)\n",
        "# init results: (B * seq_len, Dim)\n",
        "6. # Iterate over each expert to compute the weighted sum of the outputs from each selected top k experts,\n",
        "for i, expert in enumerate(self.experts):\n",
        "    # for each expert: retrieves only the batch_idx & selected_exp_idx this expert is responsible for\n",
        "    batch_idx, selected_exp_idx = torch.where(selected_experts == i)\n",
        "    # (K, Dim) => (K, Dim), where K is how many tokens this expert is responsible for\n",
        "    expert_out = expert(flat_x[batch_idx])  # recall expert is FFN with Dim=>Dim\n",
        "    # (K, 1), where K is how many tokens this expert is responsible for\n",
        "    expert_w = weights[batch_idx, selected_exp_idx, None]\n",
        "    # add the experts' weighted sum output to the corresponding tokens\n",
        "    # expert_w * expert_out: (K, 1) * (K, Dim) => (K, Dim)\n",
        "    # results: still (B * seq_len, Dim), where the corresponding tokens are updated\n",
        "    results[batch_idx] += expert_w * expert_out\n",
        "7. results = results.view(B, seq_len, dim)    \n",
        "# reshape results: (B * seq_len, Dim) => (batch_size, seq_len, dim)\n",
        "8. return results\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "24ZtShl61rcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SelfAttention\n",
        "\n",
        "```python\n",
        "\"\"\"Decoder only with causal attention (only work for inference)\n",
        "only care about current token and its corresponding attention (with support from the KV Cache)\n",
        "Extended support for GQA (grouped query attention)\"\"\"\n",
        "\n",
        "1. n_kv_heads : n_heads (for MHA) , n_kv_heads (for GQA)\n",
        "2. n_heads_q = n_ heads # no. of Q heads should be n_heads\n",
        "3. n_rep = n_heads_q // n_kv_heads\n",
        "4. head_dim = dim / n_heads # part of embedding each head responsible for)\n",
        "5. attn_window = if not specified max_seq_len\n",
        "6. wq = (dim,n_heads_q * head_dim) , wk,wv = (dim,n_kv_heads * head_dim), wo = (n_heads_q * head_dim,dim)\n",
        "7. kv_cache = RollingBufferKVCache(max_batch_size, attn_window, n_kv_heads, head_dim)\n",
        "# KV Cache with support of Sliding Window Attention & Rolling Buffer Cache\n",
        "# this is modified from the Llama implementation which does not support rolling buffer\n",
        "\n",
        "\n",
        "repeat_kv\n",
        "------------------------------------------------------------------------------\n",
        "# in GQA, each Q group shares the same KV heads, thus just repeat KV heads for the Q in the same group\n",
        "# goal shape: (B, prefix_seq_len, n_kv_heads, Head_Dim) => (B, prefix_seq_len, n_heads_q, Head_Dim)\n",
        "1. n_rep == 1 then return kv # (Q and KV are 1-to-1 (just a normal MHA))\n",
        "2. return # for GQA\n",
        "kv[:, :, :, None, :]\n",
        "## (B, prefix_seq_len, n_kv_heads, 1, Head_Dim)\n",
        ".expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
        ".reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim) #just copy n_rep times\n",
        "#n_kv_heads * n_rep = n_heads_q\n",
        "\n",
        "forward\n",
        "-------------------------------------------------------------------------------\n",
        "1. get Q,K,V : xq,xk,xv using wq,wk,wv\n",
        "# (B, 1, Dim) => (B, 1, n_heads_q * Head_Dim)\n",
        "2. reshape Q K V to get individual single heads (Qi, Ki, Vi)\n",
        "# (B, 1, n_heads_q * Head_Dim) => (B, 1, n_heads_q, Head_Dim)\n",
        "3. apply_rotary_embeddings on xq,xk , Q,K must have same shape before & after RoPE\n",
        "# (B, 1, n_heads_q, Head_Dim)\n",
        "4. kv_cache.update_cache(xk, xv, batch_size, start_pos)\n",
        "# replace the entry in the KV cache's respective position (aka update KV Cache)\n",
        "# fill (:B, idx) part of the (max_B, max_seq_len, n_kv_heads, Head_Dim) cache with (B, 1, n_kv_heads, Head_Dim)\n",
        "5. keys, values = kv_cache.retrieve_cache(batch_size, start_pos + seq_len)\n",
        "  # retrieve complete K and V from KV Cache\n",
        "  # (B, prefix_seq_len, n_kv_heads, Head_Dim)\n",
        "6. keys, values =  repeat_kv(keys),repeat_kv(values)     \n",
        "# in GQA, each Q group shares the same KV heads, thus just repeat KV heads for the Q in the same group\n",
        "# (B, prefix_seq_len, n_kv_heads, Head_Dim) => (B, prefix_seq_len, n_heads_q, Head_Dim)\n",
        "7. xq # transpose : (B, 1, n_heads_q, Head_Dim) => (B, n_heads_q, 1, Head_Dim)\n",
        "   keys,values # transpose :  (B, prefix_seq_len, n_heads_q, Head_Dim) => (B, n_heads_q, prefix_seq_len, Head_Dim)\n",
        "8. scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "   # (B, n_heads_q, 1, Head_Dim) @ (B, n_heads_q, Head_Dim, prefix_seq_len) => (B, n_heads_q, 1, prefix_seq_len)\n",
        "\"\"\"NOTE about MATMUL: for tensors with more than 2 dimensions, torch.matmul treats the last two dimensions as matrices and performs batch matrix multiplication on the other dimensions. The result is a tensor where each batch element is the result of matrix multiplication on the corresponding batch elements of the input tensors\"\"\"\n",
        "   scores = F.softmax(scores.float(), dim=-1).type_as(xq)  # dim=-1 means softmax along last dimension (sum=1)\n",
        "   # softmax(QK/sqrt(dk)): (B, n_heads_q, 1, prefix_seq_len) => (B, n_heads_q, 1, prefix_seq_len)\n",
        "9. output = torch.matmul(scores, values)\n",
        "   # (B, n_heads_q, 1, prefix_seq_len) @ (B, n_heads_q, prefix_seq_len, Head_Dim) => (B, n_heads_q, 1, Head_Dim)\n",
        "    output = output.transpose(1, 2).contiguous()\n",
        "   # (B, n_heads_q, 1, Head_Dim) => (B, 1, n_heads_q, Head_Dim) and make sure contiguous in memory\n",
        "    output = output.view(batch_size, seq_len, -1)   # -1 means infer the last dimension's shape\n",
        "   # (B, 1, n_heads_q, Head_Dim) => (B, 1, n_heads_q * Head_Dim) = (B, 1, dim)\n",
        "    output = wo(output)# apply the attention's output layer\n",
        "   # (B, 1, dim) => (B, 1, dim)\n",
        "\n",
        "\n",
        "apply_rotary_embeddings\n",
        "-------------------------------------------------------------------------------\n",
        "1. x_complex # Separate the last dim pairs of 2 values (aka real and imaginary parts of the complex number) => make complex, Each pair of 2 consecutive values in head_dim is transformed into a single complex number (thus head_dim / 2\n",
        "# (B, seq_len, H=n_heads, head_dim) => (B, seq_len, H, head_dim / 2)\n",
        "2. freqs_complex # feqs_complex to match the shape of the x_complex tensor\n",
        "# (seq_len, head_dim / 2) => (1, seq_len, 1, head_dim / 2)\n",
        "3. x_rotated # Element-wise multiplication with broadcasting\n",
        "# (B, seq_len, H, head_dim / 2) * (1, seq_len, 1, head_dim / 2) => (B, seq_len, H, head_dim / 2)\n",
        "4. x_real # # Convert complex number back to the real number: additional 2 in the final dim is for real from imag\n",
        "# (B, seq_len, H, head_dim / 2) => (B, seq_len, H * head_dim / 2, 2)\n",
        "5. x_out # Flatten the last two dimensions back into 2nd last dimension\n",
        "# (B, seq_len, H * head_dim / 2, 2) => (B, seq_len, H * head_dim)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "RxMG9f5k1rcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RollingBufferKVCache\n",
        "\n",
        "```python\n",
        "1. initialize cache_k,cache_v with zeros with shape (max_batch_size, attn_window, n_kv_heads, head_dim)\n",
        "\n",
        "\n",
        "update_cache\n",
        "----------------------------------------------------------------------\n",
        "1. cache_position = start_pos % attn_window # position wraps around within the attn_window size\n",
        "# circular buffer, meaning it can overwrite old entries when the window's limit is reached.\n",
        "2. cache_k[:batch_size, cache_position:cache_position + 1] = xk\n",
        "   cache_v[:batch_size, cache_position:cache_position + 1] = xv\n",
        "# fill (:B, idx) part of the (max_B, max_seq_len, n_kv_heads, Head_Dim) cache with (B, 1, n_kv_heads, Head_Dim)\n",
        "# shape of xk and xv: (batch_size, 1, n_kv_heads, head_dim)\n",
        "# only the active sequences in the batch are updated.Since the sequence length is 1\n",
        "# as indicated by the shape (batch_size, 1, n_kv_heads, head_dim), we update just a single position.\n",
        "\n",
        "Example:\n",
        "\n",
        "attn_window = 5\n",
        "start_pos = 7\n",
        "cache_position = 7 % 5 = 2\n",
        "This means the new key-value pair will be placed at position 2 of the cache (for the specified batch).\n",
        "\n",
        "update_cache_multiple\n",
        "-----------------------------------------------------------------------\n",
        "for i in range(seq_len):\n",
        "    update_cache(xk[:, i:i+1, :, :], xv[:, i:i+1, :, :], batch_size, start_pos + i)\n",
        "\n",
        "# updating the cache used when seq_len > 1, yet in inference we only care about the seq_len = 1 case\n",
        "# can be optimized in the future to support Mistral's pre-fill and chunking (to handle prompts)\n",
        "# For each token i in the sequence, it slices the tensors xk and xv to extract the keys and values corresponding to that token, and then calls update_cache with these values and the adjusted starting position (start_pos + i).\n",
        "\n",
        "Example:\n",
        "seq_len = 3\n",
        "start_pos = 7\n",
        "For each i in the range [0, 2]:\n",
        "    The method extracts xk[:, i:i+1, :, :] and xv[:, i:i+1, :, :], corresponding to the keys and values for the\n",
        "    i-th token in the sequence.\n",
        "    It then calls update_cache with these values and updates the cache at positions 7, 8, and 9 (% attn_window).\n",
        "\n",
        "\n",
        "retrieve_cache\n",
        "------------------------------------------------------------------------\n",
        "# calculate the effective start position considering the rolling buffer's nature\n",
        "# NOTE: start_pos should be updated to be start_pos + seq_len when called after update_cache\n",
        "1. effective_start_pos = start_pos % attn_window\n",
        "# retrieve KV from the cache, split into 2 parts to handle the wrap-around\n",
        "2. keys = torch.cat([cache_k[:batch_size, effective_start_pos:, :, :],\n",
        "                  cache_k[:batch_size, :effective_start_pos, :, :]], dim=1) # same for values\n",
        "# select the last seq_len tokens from the concatenated keys and values (to handle when < attn_window)\n",
        "3. keys = keys[:, -start_pos:, :, :] # same for values\n",
        "4. return keys, values\n",
        "\n",
        "\n",
        "Example:\n",
        "attn_window = 5\n",
        "batch_size = 2\n",
        "start_pos = 7\n",
        "effective_start_pos = 7 % 5 = 2\n",
        "\n",
        "cache_k for batch 0: [K0, K1, K2, K3, K4]\n",
        "cache_k for batch 1: [L0, L1, L2, L3, L4]\n",
        "After splitting:\n",
        "\n",
        "First Part (batch 0): [K2, K3, K4]\n",
        "Second Part (batch 0): [K0, K1]\n",
        "After concatenation:\n",
        "\n",
        "Combined for batch 0: [K2, K3, K4, K0, K1]\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "e7Nud6pq1rck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Tq4zfECC1rcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Transformer\n",
        "==================================\n",
        "1. nn.Embedding(vocab,dim) : token_embeddings\n",
        "2. nn.ModuleList(TransformerBlock(args) X n_layers) # different LLM architecture implemented\n",
        "3. RMSNorm(dim,eps)\n",
        "4. nn.Linear(dim,vocab) : output\n",
        "5. precompute_freqs_pos_frequencis(dim//n_heads,max_seq*2,device,rope_theta) : freqs_complex # per attention head\n",
        "# i.e dim // n_heads = head_dim , for Llama rope_theta is not passed\n",
        "\n",
        "@property # helpful for training-inference device switch error\n",
        "dtype() : return data type of parameter of the model\n",
        "device() : device on which model parameters are stored\n",
        "\n",
        "\n",
        "forward()\n",
        "----------------------------------------------\n",
        "\"\"\"\n",
        "note that with the KV Cache, only need the latest tokens, no need all tokens: info about previous tokens are saved in the cache\n",
        "NOTE: this is only for inference, not training (in training there's no KV cache)\n",
        "\"\"\"\n",
        "1. get hidden_state = token_embeddings(tokens) # (B,seq) -> (B,seq,dim) [1 token at a time --> map to dim]\n",
        "2. retrive pairs(m,theta) corresponding to position [start_pos, start_pos + seq_len] from freqs_complex\n",
        "3. hidden_states = layer(hidden_states,start_pos,freqs_complex) for layer in layers\n",
        "# apply precomputed frequencies to the encoding layers for positional encoding , each layer is (Nx transformer blocks)\n",
        "4. apply RMSNorm on combined hidden_states from step 3\n",
        "5. pass through output\n",
        "\n",
        "\n",
        "TransformerBlock\n",
        "===============================================================\n",
        "\"\"\" a single transformer block (different for Llama & Mistral) \"\"\"\n",
        "1. head_dim = dim//n_heads\n",
        "2. RMSNorm(dim,eps) : rms_norm # before attention & feed_forwardd\n",
        "2. SelfAttention(args) : attention # Decoder only with causal attention (only work for inference)\n",
        "3. MOE(experts=[FeedForward(args) X moe.n_experts,gate=nn.Linear(dim,moe.n_experts),args.moe]\n",
        "# Feed Forward Layer (with MoE support) otherwise FeedForward(args) : feed_forwad\n",
        "\n",
        "\n",
        "forward()\n",
        "-----------------------------------------------------------------\n",
        "1. hidden_states = x + attention.forward(rms_norm(x),start_pos,freqs_complex)\n",
        "# (B, seq_Len, dim) + (B, seq_Len, dim) => (B, seq_Len, dim) we're dealing\n",
        "# with 1 token at a time start_pos : current token we're dealing\n",
        "2. out = hidden_states + feed_forward(rms_norm(hidden_states)) # (B, seq_Len, dim) + (B, seq_Len, dim) => (B, seq_Len, dim)\n",
        "3. return out\n",
        "\n",
        "\n",
        "\n",
        "RMSNorm\n",
        "========================================================================\n",
        "\"\"\"same for both\"\"\"\n",
        "1. nn.Parameter(torch.ones(dim)) : weight\n",
        "# gamma(g) parameter trainable to perform rescaling on the norm\n",
        "\n",
        "_norm()\n",
        "-------------------------------------------------------------------------\n",
        "# RMSNorm stat : (B, Seq_Len, Dim) * (B, Seq_Len, 1) = (B, Seq_Len, Dim)\n",
        "1. return x * 1/rms\n",
        "\n",
        "forward()\n",
        "-------------------------------------------------------------------------\n",
        "# (Dim) * (B, Seq_Len, Dim) = (B, Seq_Len, Dim)\n",
        "# auto-broadcasting expands (Dim) to (1, 1, Dim) to multiplied to the last dimension of (B, Seq_Len, Dim)\n",
        "# recall: Automatic broadcasting in PyTorch occurs when dimensions match or are broadcastable starting from the trailing dimensions (i.e., from right to left)\n",
        "1. weight * _norm(x) # typecast to float & verify if it's same type as x\n",
        "\n",
        "\n",
        "FeedForward\n",
        "==========================================================================\n",
        "\"\"\"same for both but Llama uses multiple_of parameter for hidden_dim\"\"\"\n",
        "1. hidden_dim = multiple_of * ((int(hidden_dim) + multiple_of - 1) // multiple_of\n",
        "# round hidden_dim to the nearest multiple of the args.multiple_of parameter (bigger or equal)\n",
        "# just a design choice to look cool :)\n",
        "1. gate : w1(dim,hidden_dim) , up : w2(hidden_dim,dim), down : w3(dim,hidden_dim)\n",
        "\n",
        "forward()\n",
        "---------------------------------------------------------------\n",
        "1. xw1 = w1(x)\n",
        "# (S(XW1) * XV)XW2 = (ss)XW2; goal shape: (B, seq_len, Dim) => (B, seq_len, Dim)\n",
        "# (B, seq_len, Dim) w1=> (B, seq_len, Hidden_Dim)\n",
        "2. sxw1 = silu(xw1)\n",
        "# (B, seq_len, Hidden_Dim) => (B, seq_len, Hidden_Dim) # silu\n",
        "3. xv = w3(x)\n",
        "# (B, seq_len, Dim) w3=> (B, seq_len, Hidden_Dim)\n",
        "4. sxw1xv = sxw1 * xv\n",
        "# (B, seq_len, Hidden_Dim) * (B, seq_len, Hidden_Dim) = (B, seq_len, Hidden_Dim) = element wise multiplication\n",
        "5. return w2(sxw1xv)\n",
        "# (B, seq_len, Hidden_Dim) w2=> (B, seq_len, Dim)\n",
        "\n",
        "\n",
        "MOE\n",
        "================================================================================\n",
        "\"\"\"the only difference in Mixtral MOE: after attention, instead of RMS=>MLP, it has RMS=>MOE\"\"\"\n",
        "1. nn.ModuleList(experts) : experts\n",
        "2. gate , moe_args\n",
        "\n",
        "forward()\n",
        "------------------------------------------------------------------------------------\n",
        "# NOTE: in the mistral paper, all input/output size used are (B * seq_len, Dim) instead of (B, seq_len, Dim)\n",
        "# goal shape: (B, seq_len, Dim) = > (B, seq_len, Dim)\n",
        "1. flat_x = reshape input shape to (B * seq_len, Dim)\n",
        "2. gate_logic = gate(flat_x)  # recall gate is linear with (Dim, n_experts)\n",
        "# (B * seq_len, Dim) gate=> (B * seq_len, n_experts) for each input token\n",
        "3. weights, selected_experts = torch.topk(gate_logits, self.moe_args.n_experts_per_tok)\n",
        "# Get the top k experts for each input token, using torch.topk\n",
        "# weights=logits, selected_experts=indices\n",
        "# (B * seq_len, n_experts) => (B * seq_len, n_experts_per_tok)\n",
        "4. weights = F.softmax(weights, dim=1, dtype=torch.float).to(x.dtype)\n",
        "# # Normalize the weights with softmax, to get the selected top k experts' weights on the tokens\n",
        "5. results = torch.zeros_like(flat_x)\n",
        "# init results: (B * seq_len, Dim)\n",
        "6. # Iterate over each expert to compute the weighted sum of the outputs from each selected top k experts,\n",
        "for i, expert in enumerate(self.experts):\n",
        "    # for each expert: retrieves only the batch_idx & selected_exp_idx this expert is responsible for\n",
        "    batch_idx, selected_exp_idx = torch.where(selected_experts == i)\n",
        "    # (K, Dim) => (K, Dim), where K is how many tokens this expert is responsible for\n",
        "    expert_out = expert(flat_x[batch_idx])  # recall expert is FFN with Dim=>Dim\n",
        "    # (K, 1), where K is how many tokens this expert is responsible for\n",
        "    expert_w = weights[batch_idx, selected_exp_idx, None]\n",
        "    # add the experts' weighted sum output to the corresponding tokens\n",
        "    # expert_w * expert_out: (K, 1) * (K, Dim) => (K, Dim)\n",
        "    # results: still (B * seq_len, Dim), where the corresponding tokens are updated\n",
        "    results[batch_idx] += expert_w * expert_out\n",
        "7. results = results.view(B, seq_len, dim)    \n",
        "# reshape results: (B * seq_len, Dim) => (batch_size, seq_len, dim)\n",
        "8. return results\n"
      ],
      "metadata": {
        "id": "KEK8hLUs1rcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T06:38:55.126141Z",
          "iopub.execute_input": "2024-08-31T06:38:55.126522Z",
          "iopub.status.idle": "2024-08-31T06:38:55.439774Z",
          "shell.execute_reply.started": "2024-08-31T06:38:55.126457Z",
          "shell.execute_reply": "2024-08-31T06:38:55.43886Z"
        },
        "trusted": true,
        "id": "y_fObERM1rco",
        "outputId": "8c399ab8-b768-4554-f3b4-c3ebab079b12",
        "colab": {
          "referenced_widgets": [
            "ab17fade7dad480d97e8dca8552d0cd6"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab17fade7dad480d97e8dca8552d0cd6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "torch.backends.cuda.enable_flash_sdp(True)\n",
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T06:39:03.851921Z",
          "iopub.execute_input": "2024-08-31T06:39:03.852295Z",
          "iopub.status.idle": "2024-08-31T06:39:07.11376Z",
          "shell.execute_reply.started": "2024-08-31T06:39:03.852258Z",
          "shell.execute_reply": "2024-08-31T06:39:07.112839Z"
        },
        "trusted": true,
        "id": "0X2IaTJo1rcp",
        "outputId": "8aa54675-0c21-438c-b1ee-4e48ca160a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda', index=0)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming we have the following data\n",
        "data = {\n",
        "    'transaction_id': ['T1001', 'T1002', 'T1003', 'T1004', 'T1005'],\n",
        "    'customer_id': ['C001', 'C002', 'C003', 'C002', 'C001'],\n",
        "    'payment_amount': [125.50, 89.99, 120.00, 54.30, 210.20],\n",
        "    'payment_date': ['2021-10-05', '2021-10-06', '2021-10-07', '2021-10-05', '2021-10-08'],\n",
        "    'payment_status': ['Paid', 'Unpaid', 'Paid', 'Paid', 'Pending']\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T06:39:13.685358Z",
          "iopub.execute_input": "2024-08-31T06:39:13.685838Z",
          "iopub.status.idle": "2024-08-31T06:39:14.103105Z",
          "shell.execute_reply.started": "2024-08-31T06:39:13.685802Z",
          "shell.execute_reply": "2024-08-31T06:39:14.102253Z"
        },
        "trusted": true,
        "id": "j1p5fBcV1rcq",
        "outputId": "23036c77-0eb9-4b95-80d9-c899d1e171f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  transaction_id customer_id  payment_amount payment_date payment_status\n0          T1001        C001          125.50   2021-10-05           Paid\n1          T1002        C002           89.99   2021-10-06         Unpaid\n2          T1003        C003          120.00   2021-10-07           Paid\n3          T1004        C002           54.30   2021-10-05           Paid\n4          T1005        C001          210.20   2021-10-08        Pending",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>transaction_id</th>\n      <th>customer_id</th>\n      <th>payment_amount</th>\n      <th>payment_date</th>\n      <th>payment_status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>T1001</td>\n      <td>C001</td>\n      <td>125.50</td>\n      <td>2021-10-05</td>\n      <td>Paid</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>T1002</td>\n      <td>C002</td>\n      <td>89.99</td>\n      <td>2021-10-06</td>\n      <td>Unpaid</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>T1003</td>\n      <td>C003</td>\n      <td>120.00</td>\n      <td>2021-10-07</td>\n      <td>Paid</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>T1004</td>\n      <td>C002</td>\n      <td>54.30</td>\n      <td>2021-10-05</td>\n      <td>Paid</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>T1005</td>\n      <td>C001</td>\n      <td>210.20</td>\n      <td>2021-10-08</td>\n      <td>Pending</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_payment_status(df: data, transaction_id: str) -> str:\n",
        "    if transaction_id in df.transaction_id.values:\n",
        "        return json.dumps({'status': df[df.transaction_id == transaction_id].payment_status.item()})\n",
        "    return json.dumps({'error': 'transaction id not found.'})\n",
        "\n",
        "def retrieve_payment_date(df: data, transaction_id: str) -> str:\n",
        "    if transaction_id in df.transaction_id.values:\n",
        "        return json.dumps({'date': df[df.transaction_id == transaction_id].payment_date.item()})\n",
        "    return json.dumps({'error': 'transaction id not found.'})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T06:39:17.476729Z",
          "iopub.execute_input": "2024-08-31T06:39:17.477306Z",
          "iopub.status.idle": "2024-08-31T06:39:17.484987Z",
          "shell.execute_reply.started": "2024-08-31T06:39:17.477258Z",
          "shell.execute_reply": "2024-08-31T06:39:17.483815Z"
        },
        "trusted": true,
        "id": "wwvhBRbX1rcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"retrieve_payment_status\",\n",
        "            \"description\": \"Get payment status of a transaction\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"transaction_id\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The transaction id.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"transaction_id\"],\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"retrieve_payment_date\",\n",
        "            \"description\": \"Get payment date of a transaction\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"transaction_id\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The transaction id.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"transaction_id\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "names_to_functions = {\n",
        "  'retrieve_payment_status': functools.partial(retrieve_payment_status, df=df),\n",
        "  'retrieve_payment_date': functools.partial(retrieve_payment_date, df=df)\n",
        "}\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"What's the status of my transaction T1001?\"}]\n",
        "\n",
        "B_FUNC, E_FUNC = \"You have access to the following functions. Use them if required:\\n\\n\", \"\\n\\n\"\n",
        "B_INST, E_INST = \"GPT4 Correct User: \", \"<|end_of_turn|>GPT4 Correct Assistant:\\n\\n\" #OpenChat style\n",
        "prompt = f\"{B_INST}{B_FUNC}{names_to_functions}{E_FUNC}{messages}{E_INST}\\n\\n\"\n",
        "prompt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T06:39:21.938987Z",
          "iopub.execute_input": "2024-08-31T06:39:21.939347Z",
          "iopub.status.idle": "2024-08-31T06:39:21.955347Z",
          "shell.execute_reply.started": "2024-08-31T06:39:21.939311Z",
          "shell.execute_reply": "2024-08-31T06:39:21.954404Z"
        },
        "trusted": true,
        "id": "piHyGOvm1rct",
        "outputId": "1f9f38b3-cd66-4a96-e353-155865e5e9f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'GPT4 Correct User: You have access to the following functions. Use them if required:\\n\\n{\\'retrieve_payment_status\\': functools.partial(<function retrieve_payment_status at 0x7f238ff1d1b0>, df=  transaction_id customer_id  payment_amount payment_date payment_status\\n0          T1001        C001          125.50   2021-10-05           Paid\\n1          T1002        C002           89.99   2021-10-06         Unpaid\\n2          T1003        C003          120.00   2021-10-07           Paid\\n3          T1004        C002           54.30   2021-10-05           Paid\\n4          T1005        C001          210.20   2021-10-08        Pending), \\'retrieve_payment_date\\': functools.partial(<function retrieve_payment_date at 0x7f238ff1d240>, df=  transaction_id customer_id  payment_amount payment_date payment_status\\n0          T1001        C001          125.50   2021-10-05           Paid\\n1          T1002        C002           89.99   2021-10-06         Unpaid\\n2          T1003        C003          120.00   2021-10-07           Paid\\n3          T1004        C002           54.30   2021-10-05           Paid\\n4          T1005        C001          210.20   2021-10-08        Pending)}\\n\\n[{\\'role\\': \\'user\\', \\'content\\': \"What\\'s the status of my transaction T1001?\"}]<|end_of_turn|>GPT4 Correct Assistant:\\n\\n\\n\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,device=\"auto\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,trust_remote_code=True,torch_dtype=torch.bfloat16,\n",
        "                                      low_cpu_mem_usage=True,device_map=\"auto\")\n",
        "\n",
        "# text = \"Hello my name is\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-31T06:41:06.552962Z",
          "iopub.execute_input": "2024-08-31T06:41:06.553327Z"
        },
        "trusted": true,
        "id": "z0lYJkaU1rcu",
        "outputId": "423cea40-d072-4886-93e3-412e619fee64",
        "colab": {
          "referenced_widgets": [
            "2b0dff2d277447849d57811ccaaafe9c",
            "a962b1e3b1be4312b58b6eb07050bd59",
            "dd07ba0228034027adbd339654e5b2c9",
            "92117ed0225c4a3cb1be90f0ce3d86bb",
            "1b0f6d7f95ed42a0858a9a48b8431ac5",
            "3415a333695c450f9065cd8c8f436ac2",
            "56901dba57d440aab5d6ede4d89cd548",
            "277de0875f5947dcb617afefcfdf794f",
            "9d546f335d114e57b5f497a1700efc1e"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b0dff2d277447849d57811ccaaafe9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00019.safetensors:  61%|######1   | 3.00G/4.89G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a962b1e3b1be4312b58b6eb07050bd59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd07ba0228034027adbd339654e5b2c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92117ed0225c4a3cb1be90f0ce3d86bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b0f6d7f95ed42a0858a9a48b8431ac5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3415a333695c450f9065cd8c8f436ac2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56901dba57d440aab5d6ede4d89cd548"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "277de0875f5947dcb617afefcfdf794f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d546f335d114e57b5f497a1700efc1e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = tokenizer.apply_chat_template(prompt,  return_dict=True, return_tensors=\"pt\", add_generation_prompt=True,tokenize=False)\n",
        "# prompt"
      ],
      "metadata": {
        "id": "hiRFzslB1rcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and get model outputs\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ZL4kE-b81rcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract function calls from the model output\n",
        "def extract_function_call(response):\n",
        "    # Example extraction logic; modify according to actual response format\n",
        "    match = re.search(r'Call function (\\w+)\\((.*)\\)', response)\n",
        "    if match:\n",
        "        func_name = match.group(1)\n",
        "        params = json.loads(f\"{{{match.group(2)}}}\")\n",
        "        return func_name, params\n",
        "    return None, None\n",
        "\n",
        "# Extract function call and parameters\n",
        "func_name, params = extract_function_call(response)\n",
        "\n",
        "# Call the function if it exists\n",
        "if func_name in names_to_functions:\n",
        "    result = names_to_functions[func_name](**params)\n",
        "    print(f\"Function call result: {result}\")\n",
        "else:\n",
        "    print(\"No valid function call found.\")\n"
      ],
      "metadata": {
        "id": "zHe1GUwX1rcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Expert, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, input_dim, num_experts):\n",
        "        super(Router, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return probs\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts, top_k=1):\n",
        "        super(MoELayer, self).__init__()\n",
        "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)])\n",
        "        self.router = Router(input_dim, num_experts)\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k if top_k <= num_experts else num_experts  # Ensure top_k does not exceed num_experts\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "        # Routing\n",
        "        probs = self.router(x)  # Shape: [batch_size, seq_length, num_experts]\n",
        "\n",
        "        # Get top-k experts for each token\n",
        "        topk_probs, topk_indices = torch.topk(probs, self.top_k, dim=-1)  # Shape: [batch_size, seq_length, top_k]\n",
        "\n",
        "        # Prepare outputs\n",
        "        outputs = torch.zeros(batch_size, seq_length, x.size(-1)).to(x.device)\n",
        "\n",
        "        # Iterate over available experts (up to top_k)\n",
        "        for i in range(self.top_k):\n",
        "            expert_output = self.experts[i](x)  # Shape: [batch_size, seq_length, output_dim]\n",
        "\n",
        "            # Create mask for current expert i\n",
        "            mask = topk_indices[:, :, i]  # Shape: [batch_size, seq_length]\n",
        "            mask = mask.unsqueeze(-1).expand(-1, -1, expert_output.size(-1))  # Shape: [batch_size, seq_length, output_dim]\n",
        "\n",
        "            # Gather corresponding probabilities\n",
        "            masked_probs = topk_probs[:, :, i]  # Shape: [batch_size, seq_length]\n",
        "\n",
        "            # Apply mask to expert outputs\n",
        "            masked_output = expert_output * masked_probs.unsqueeze(-1)  # Shape: [batch_size, seq_length, output_dim]\n",
        "\n",
        "            # Sum over top-k dimension\n",
        "            outputs += masked_output\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts, top_k=1):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=8)\n",
        "        self.moelayer = MoELayer(input_dim, hidden_dim, output_dim, num_experts, top_k)\n",
        "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        # Self-Attention\n",
        "        attn_output, attn_output_weights = self.self_attention(x, x, x)\n",
        "        x = self.layer_norm1(x + self.dropout(attn_output)) # residual connection\n",
        "\n",
        "        # MoE Layer\n",
        "        moe_output = self.moelayer(x)\n",
        "        x = self.layer_norm2(x + self.dropout(moe_output))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, num_experts, top_k=1):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(input_dim, hidden_dim, output_dim, num_experts, top_k)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory)\n",
        "        return x\n",
        "\n",
        "# Usage\n",
        "input_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = 128\n",
        "num_experts = 4\n",
        "top_k = 2\n",
        "num_layers = 6\n",
        "batch_size = 32\n",
        "seq_length = 10\n",
        "\n",
        "# Initialize the Transformer Decoder with MoE\n",
        "decoder = TransformerDecoder(num_layers, input_dim, hidden_dim, output_dim, num_experts, top_k)\n",
        "x = torch.randn(batch_size, seq_length, input_dim)\n",
        "memory = torch.randn(batch_size, seq_length, input_dim)  # For illustration, this could be encoder output in practice\n",
        "output = decoder(x, memory)\n",
        "print(x.shape)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "rEgVYDtD1rcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}