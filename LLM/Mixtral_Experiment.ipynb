{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 67121,
          "databundleVersionId": 7806901,
          "isSourceIdPinned": false,
          "sourceType": "competition"
        },
        {
          "sourceId": 10765944,
          "sourceType": "datasetVersion",
          "datasetId": 6678406
        },
        {
          "sourceId": 5994,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": false,
          "modelInstanceId": 4761,
          "modelId": 3108
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Mixtral-Experiment",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "wKEHQMdWQsrB"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "llm_prompt_recovery_path = kagglehub.competition_download('llm-prompt-recovery')\n",
        "tsr564_gemma_rewrite_nbroad_1_path = kagglehub.dataset_download('tsr564/gemma-rewrite-nbroad-1')\n",
        "mistral_ai_mixtral_pytorch_8x7b_instruct_v0_1_hf_1_path = kagglehub.model_download('mistral-ai/mixtral/PyTorch/8x7b-instruct-v0.1-hf/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "yz6WoCW3QsrJ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "d-rBuTH_QsrP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git  -U\n",
        "!pip install -q git+https://github.com/huggingface/accelerate.git  -U\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q git+https://github.com/huggingface/peft.git  -U"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:01:58.113341Z",
          "iopub.execute_input": "2025-03-28T15:01:58.11378Z",
          "iopub.status.idle": "2025-03-28T15:03:01.254685Z",
          "shell.execute_reply.started": "2025-03-28T15:01:58.113752Z",
          "shell.execute_reply": "2025-03-28T15:03:01.253784Z"
        },
        "id": "LXF8x9FOQsrR",
        "outputId": "c0371f82-bc86-445c-94b8-052983e15431"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "#https://github.com/Lightning-AI/lit-gpt/issues/327\n",
        "# torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "# torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:03:01.255821Z",
          "iopub.execute_input": "2025-03-28T15:03:01.256088Z",
          "iopub.status.idle": "2025-03-28T15:03:01.260719Z",
          "shell.execute_reply.started": "2025-03-28T15:03:01.256066Z",
          "shell.execute_reply": "2025-03-28T15:03:01.259775Z"
        },
        "id": "Vg9-yPhnQsrW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save and load a PyTorch model, follow these steps:\n",
        "\n",
        "### Saving the Model\n",
        "\n",
        "1. **Save the Entire Model**:\n",
        "   ```python\n",
        "   torch.save(model, 'model.pth')\n",
        "   ```\n",
        "\n",
        "2. **Save Only the Model State Dict**:\n",
        "   ```python\n",
        "   torch.save(model.state_dict(), 'model_state_dict.pth')\n",
        "   ```\n",
        "\n",
        "### Loading the Model\n",
        "\n",
        "1. **Load the Entire Model**:\n",
        "   ```python\n",
        "   model = torch.load('model.pth')\n",
        "   model.eval()  # Set the model to evaluation mode\n",
        "   ```\n",
        "\n",
        "2. **Load the Model State Dict**:\n",
        "   ```python\n",
        "   model = EnhancedRNN(...)  # Initialize the model architecture\n",
        "   model.load_state_dict(torch.load('model_state_dict.pth'))\n",
        "   model.eval()  # Set the model to evaluation mode\n",
        "   ```\n",
        "\n",
        "\n",
        "### Base Model Files Overview:\n",
        "\n",
        "1. **`config.json`**: Contains model architecture settings like hyperparameters and initialization details.\n",
        "2. **`generation_config.json`**: Includes text generation settings such as sequence length and sampling strategies.\n",
        "3. **`model.safetensors.index.json`**: Stores metadata for managing model weights in `safetensors` format.\n",
        "4. **`model-*.safetensors`**: Contains quantized model weights split across multiple files in the `safetensors` format.\n",
        "5. **`special_tokens_map.json`**: Maps special tokens to their respective identifiers.\n",
        "6. **`tokenizer.json`**: Includes the tokenizer’s vocabulary and configuration.\n",
        "7. **`tokenizer.model`**: The binary model used for tokenization.\n",
        "8. **`tokenizer_config.json`**: Configures how the tokenizer processes text.\n",
        "\n",
        "### Summary:\n",
        "- **Config Files**: Define model and tokenizer setup.\n",
        "- **Model Weights**: Contain trained and quantized weights.\n",
        "- **Tokenizer Files**: Used for text tokenization and detokenization, including vocabulary and special tokens.\n",
        "\n",
        "These files are needed to properly load the model and tokenizer, typically handled by libraries like `transformers`."
      ],
      "metadata": {
        "id": "riWBsTmyQsrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference with Alpaca style Prompt\n",
        "\n",
        "```python\n",
        "# {\n",
        "#     \"description\": \"Template used by Alpaca-LoRA.\",\n",
        "#     \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
        "#     \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
        "#     \"response_split\": \"### Response:\"    \n",
        "# }\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def format_prompt(sample):\n",
        "    instructions=sample[\"instruction\"] # here system_prompt\n",
        "    inputs = sample[\"input\"]           # here user_prompt\n",
        "    responses = sample[\"output\"]        # here \"\" preset but will be in training dataset\n",
        "    texts = []\n",
        "    for instruction,input,response in zip(instructions,inputs,responses):\n",
        "        text = alpaca_prompt.format(instruction,input,response)+EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\":texts,} # add data in 1 column for SFTTrainer\n",
        "    \n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\",split=\"train\")\n",
        "dataset = dataset.map(format_prompt,batched=True)\n",
        "\n",
        "\n",
        "def prepare_for_peft(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False  # freeze the model - train adapters later\n",
        "        if param.dim() == 1:\n",
        "            # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "            param.data = param.data.to(torch.float32)\n",
        "\n",
        "    model.config.gradient_checkpointing = True  # enable gradient checkpointing\n",
        "    model.config.use_cache = False  # disable cache for memory efficiency\n",
        "    model.config.output_hidden_states = True  # set to True if you want hidden states\n",
        "    model.config.output_attentions = True  # set to True if you want attention weights\n",
        "\n",
        "    # No need to define a separate class, we can use nn.Sequential directly\n",
        "    model.lm_head = nn.Sequential(nn.Linear(model.config.hidden_size, model.config.vocab_size))\n",
        "    return model\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "0-ciAp6jQsrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this can help speed up inference\n",
        "max_new_tokens = 30\n",
        "\n",
        "#output test is trimmed according to this\n",
        "max_sentences_in_response = 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:03:01.262491Z",
          "iopub.execute_input": "2025-03-28T15:03:01.262803Z",
          "iopub.status.idle": "2025-03-28T15:03:01.303727Z",
          "shell.execute_reply.started": "2025-03-28T15:03:01.262783Z",
          "shell.execute_reply": "2025-03-28T15:03:01.303091Z"
        },
        "id": "-V6sokY1Qsrd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
        "\n",
        "\n",
        "model_name = '/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    model_max_length=512,  # Reduce maximum sequence length\n",
        ")\n",
        "\n",
        "# Load base model(Mistral 7B)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload= True\n",
        ")\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.gradient_checkpointing = True\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16,\n",
        "        attn_implementation = \"eager\",\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        config=config,\n",
        "        # max_memory={0: \"8GiB\",1:\"13GiB\"},  # Limit GPU memory usage\n",
        "        offload_folder=\"offload\",  # Specify offload directory\n",
        "        offload_state_dict=True  # Enable state dict offloading\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:03:01.30512Z",
          "iopub.execute_input": "2025-03-28T15:03:01.305406Z",
          "iopub.status.idle": "2025-03-28T15:10:23.298958Z",
          "shell.execute_reply.started": "2025-03-28T15:03:01.305376Z",
          "shell.execute_reply": "2025-03-28T15:10:23.29804Z"
        },
        "id": "-qpyvlVFQsre",
        "outputId": "4fb539ed-4ad8-4aa8-bd5e-67cf4e5068f8",
        "colab": {
          "referenced_widgets": [
            "d0f3d068dd0d48e797a3069419935089"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0f3d068dd0d48e797a3069419935089"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # LoRA config\n",
        "# from peft import LoraConfig,PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "# peft_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        "# )\n",
        "# model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:10:23.300025Z",
          "iopub.execute_input": "2025-03-28T15:10:23.300603Z",
          "iopub.status.idle": "2025-03-28T15:10:23.304063Z",
          "shell.execute_reply.started": "2025-03-28T15:10:23.30056Z",
          "shell.execute_reply": "2025-03-28T15:10:23.303344Z"
        },
        "id": "twOdg0SLQsrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:10:23.305174Z",
          "iopub.execute_input": "2025-03-28T15:10:23.305478Z",
          "iopub.status.idle": "2025-03-28T15:10:23.633062Z",
          "shell.execute_reply.started": "2025-03-28T15:10:23.30545Z",
          "shell.execute_reply": "2025-03-28T15:10:23.632235Z"
        },
        "id": "O9Zk6xXaQsri",
        "outputId": "72eed9aa-daf4-4314-cb6a-4af745e21a5d"
      },
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "111"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model"
      ],
      "metadata": {
        "id": "DCP9o9u2Qsrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:10:23.633894Z",
          "iopub.execute_input": "2025-03-28T15:10:23.634194Z",
          "iopub.status.idle": "2025-03-28T15:10:23.649897Z",
          "shell.execute_reply.started": "2025-03-28T15:10:23.63417Z",
          "shell.execute_reply": "2025-03-28T15:10:23.649218Z"
        },
        "id": "ZGktUxKhQsrl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison\"\"\"\n",
        "\n",
        "tokens = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "# input_ids = tokens.input_ids.to(\"cuda\")\n",
        "# attention_masks = tokens.attention_mask.to(\"cuda\")\n",
        "print(f\" input_ids : {tokens.input_ids.shape}\")\n",
        "\n",
        "for idx, token_id in enumerate(tokens.input_ids[0]):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(token_id) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )\n",
        "\n",
        "generation_output = model.generate(**tokens,max_new_tokens=100)\n",
        "print(\"generated output(max_tokens + input_ids) : \",generation_output[0].shape) #\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:10:23.652448Z",
          "iopub.execute_input": "2025-03-28T15:10:23.652674Z",
          "iopub.status.idle": "2025-03-28T15:10:53.644403Z",
          "shell.execute_reply.started": "2025-03-28T15:10:23.652656Z",
          "shell.execute_reply": "2025-03-28T15:10:53.643481Z"
        },
        "id": "LWFJLvhJQsrm",
        "outputId": "3be6cad4-5610-4c87-a8ef-a90323a89289"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " input_ids : torch.Size([1, 29])\n\u001b[0;30;48;2;102;194;165m<s>\u001b[0m \u001b[0;30;48;2;252;141;98mQ\u001b[0m \u001b[0;30;48;2;141;160;203mwen\u001b[0m \u001b[0;30;48;2;231;138;195m2\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m5\u001b[0m \u001b[0;30;48;2;102;194;165mvs\u001b[0m \u001b[0;30;48;2;252;141;98mPh\u001b[0m \u001b[0;30;48;2;141;160;203mi\u001b[0m \u001b[0;30;48;2;231;138;195m-\u001b[0m \u001b[0;30;48;2;166;216;84m3\u001b[0m \u001b[0;30;48;2;255;217;47m-\u001b[0m \u001b[0;30;48;2;102;194;165mmin\u001b[0m \u001b[0;30;48;2;252;141;98mi\u001b[0m \u001b[0;30;48;2;141;160;203mwhich\u001b[0m \u001b[0;30;48;2;231;138;195mone\u001b[0m \u001b[0;30;48;2;166;216;84mis\u001b[0m \u001b[0;30;48;2;255;217;47mbetter\u001b[0m \u001b[0;30;48;2;102;194;165mfor\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203mpurpose\u001b[0m \u001b[0;30;48;2;231;138;195m?\u001b[0m \u001b[0;30;48;2;166;216;84mTell\u001b[0m \u001b[0;30;48;2;255;217;47mme\u001b[0m \u001b[0;30;48;2;102;194;165mtechnical\u001b[0m \u001b[0;30;48;2;252;141;98muse\u001b[0m \u001b[0;30;48;2;141;160;203mcase\u001b[0m \u001b[0;30;48;2;231;138;195mcomparison\u001b[0m \u001b[0;30;48;2;166;216;84m</s>\u001b[0m generated output(max_tokens + input_ids) :  torch.Size([129])\n<s> Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison</s>R: Both Qwen2.5 and Phi-3-mini are single-board computers (SBCs) that can be used for various purposes, such as prototyping, IoT projects, and education. Here is a comparison of their technical specifications and use cases:\n\nQwen2.5:\n\n* CPU: Allwinner H3 quad-core Cortex-A7\n* GPU: Mali-400MP2\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward pass"
      ],
      "metadata": {
        "id": "jYcybQkaQsro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### logits & past_key_values will have same shape , here we're passing the tokens through (model.model + model.lm_head)"
      ],
      "metadata": {
        "id": "xI2TdWwhQsrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**tokens)\n",
        "#\n",
        "print(\"logits shape ([batch_sz,seq_len,vocab_sz]) : \",output.logits.shape)\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(f\"No. of Decoder : {len(output.past_key_values)}\")\n",
        "print(f\"Cached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : {len(output.past_key_values[0])}\")\n",
        "print(f\"Key(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {output.past_key_values[0][0].shape}\")\n",
        "print(f\"Value(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {output.past_key_values[0][1].shape}\")\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(\"Predicted token after the forward pass\")\n",
        "for idx,logits_value in enumerate(output.logits[0]):\n",
        "    token_id = torch.argmax(logits_value,dim=-1)\n",
        "    print(f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +tokenizer.decode(token_id) +'\\x1b[0m',end=' ')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:10:53.646247Z",
          "iopub.execute_input": "2025-03-28T15:10:53.646521Z",
          "iopub.status.idle": "2025-03-28T15:10:55.230269Z",
          "shell.execute_reply.started": "2025-03-28T15:10:53.646501Z",
          "shell.execute_reply": "2025-03-28T15:10:55.229139Z"
        },
        "id": "iVUJcldiQsrp",
        "outputId": "f037fb4a-4b8c-49f2-f361-d18652e5e91b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "logits shape ([batch_sz,seq_len,vocab_sz]) :  torch.Size([1, 29, 32000])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nNo. of Decoder : 32\nCached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : 2\nKey(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\nValue(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nPredicted token after the forward pass\n\u001b[0;30;48;2;102;194;165mQ\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203mch\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m\n\u001b[0m \u001b[0;30;48;2;102;194;165mQ\u001b[0m \u001b[0;30;48;2;252;141;98mp\u001b[0m \u001b[0;30;48;2;141;160;203m2\u001b[0m \u001b[0;30;48;2;231;138;195m1\u001b[0m \u001b[0;30;48;2;166;216;84m\n\u001b[0m \u001b[0;30;48;2;255;217;47m2\u001b[0m \u001b[0;30;48;2;102;194;165mi\u001b[0m \u001b[0;30;48;2;252;141;98m\n\u001b[0m \u001b[0;30;48;2;141;160;203mone\u001b[0m \u001b[0;30;48;2;231;138;195mis\u001b[0m \u001b[0;30;48;2;166;216;84mbetter\u001b[0m \u001b[0;30;48;2;255;217;47m?\u001b[0m \u001b[0;30;48;2;102;194;165mme\u001b[0m \u001b[0;30;48;2;252;141;98m?\u001b[0m \u001b[0;30;48;2;141;160;203m?\u001b[0m \u001b[0;30;48;2;231;138;195m\n\u001b[0m \u001b[0;30;48;2;166;216;84mme\u001b[0m \u001b[0;30;48;2;255;217;47mthe\u001b[0m \u001b[0;30;48;2;102;194;165mdetails\u001b[0m \u001b[0;30;48;2;252;141;98mof\u001b[0m \u001b[0;30;48;2;141;160;203m\n\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84mR\u001b[0m ",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward pass of input through model except the lm_head then passing through lm_head"
      ],
      "metadata": {
        "id": "rqtmIUBOQsrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = model.model(tokens.input_ids)\n",
        "\n",
        "print(\"last_hidden_state shape ([batch_sz,seq_len,hidden_sz]) : \",model_output[0].shape)\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(f\"No. of Decoder : {len(model_output.past_key_values)}\")\n",
        "print(f\"Cached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : {len(model_output.past_key_values[0])}\")\n",
        "print(f\"Key(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {model_output.past_key_values[0][0].shape}\")\n",
        "print(f\"Value(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {model_output.past_key_values[0][1].shape}\")\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "lm_head_output = model.lm_head(model_output[0])\n",
        "print(\"lm_head/logits shape ([batch_sz,seq_len,vocab_sz]) : \",lm_head_output.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:10:55.231184Z",
          "iopub.execute_input": "2025-03-28T15:10:55.23148Z",
          "iopub.status.idle": "2025-03-28T15:10:56.632269Z",
          "shell.execute_reply.started": "2025-03-28T15:10:55.231457Z",
          "shell.execute_reply": "2025-03-28T15:10:56.631498Z"
        },
        "id": "UDzWY9APQsrs",
        "outputId": "e2b81978-6ef2-4c4d-a9a8-2b61cd22910c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "last_hidden_state shape ([batch_sz,seq_len,hidden_sz]) :  torch.Size([1, 29, 4096])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nNo. of Decoder : 32\nCached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : 2\nKey(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\nValue(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nlm_head/logits shape ([batch_sz,seq_len,vocab_sz]) :  torch.Size([1, 29, 32000])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Theoretically, both should match but\n",
        "    If False, then small numerical differences exist due to normalization, dropout, or caching.\n",
        "\n",
        "\n",
        "\n",
        "##### **Why Are the Values Different?**\n",
        "1. **Dropout (During Training Mode)**\n",
        "   - If the model is in **training mode (`model.train()`)**, dropout is applied, leading to slightly different hidden states.\n",
        "   - Try running in **evaluation mode**:\n",
        "     ```python\n",
        "     model.eval()\n",
        "     ```\n",
        "\n",
        "2. **LayerNorm / Residual Connection Effects**\n",
        "   - Some architectures (e.g., GPT) apply **Layer Normalization and residual connections differently** inside `forward()`.\n",
        "   - If you extract `model.model(input_ids)`, it **may not be identical** to the forward method.\n",
        "\n",
        "3. **Past Key-Value Caching (for Decoding)**\n",
        "   - When using `model(**tokens)`, **past key-value states** are handled inside the model, which can slightly alter values.\n",
        "   - Try disabling caching:\n",
        "     ```python\n",
        "     output = model(**tokens, use_cache=False)\n",
        "     ```\n"
      ],
      "metadata": {
        "id": "en8_biqnQsru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual inference"
      ],
      "metadata": {
        "id": "bVZr6T1IQsrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "num_tokens_to_generate = 50  # Number of tokens to predict\n",
        "\n",
        "tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "input_ids = tokens.input_ids  # Initial tokenized input\n",
        "\n",
        "for token_idx in range(num_tokens_to_generate):\n",
        "    with torch.no_grad():  # No need to compute gradients during inference\n",
        "        model_output = model.model(input_ids)  # Get hidden states\n",
        "        lm_head_output = model.lm_head(model_output[0]).to(\"cuda:0\")  # Get logits\n",
        "\n",
        "    # Get the predicted token ID (argmax over vocabulary)\n",
        "    next_token_id = lm_head_output[:, -1, :].argmax(dim=-1)\n",
        "    if token_idx==35:\n",
        "        print(f\"Generated Token no : {token_idx}\")\n",
        "        print(f\"lm_head_output [batch_sz,updated_seq_len,vocab_sz] : {lm_head_output.shape}\")\n",
        "        print(f\"lm_head_output[:, -1, :] = [batch_sz,vocab_sz] : { lm_head_output[:, -1, :].shape}\")\n",
        "        print(f\"next_token_id [predicted_seq_len] : {next_token_id.shape}\")\n",
        "        print(f\"next_token_id.unsqueeze(-1) [batch_sz,predicted_seq_len] : {next_token_id.unsqueeze(-1).shape}\")\n",
        "        print(\"-\"*100)\n",
        "        print(\"-\"*100)\n",
        "\n",
        "    # Append predicted token to input_ids\n",
        "    input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "# Decode the generated sequence\n",
        "predicted_text = tokenizer.decode(input_ids[0])\n",
        "\n",
        "print(\"Generated Text:\", predicted_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:10:56.633157Z",
          "iopub.execute_input": "2025-03-28T15:10:56.633415Z",
          "iopub.status.idle": "2025-03-28T15:12:13.551463Z",
          "shell.execute_reply.started": "2025-03-28T15:10:56.633395Z",
          "shell.execute_reply": "2025-03-28T15:12:13.550488Z"
        },
        "id": "7BjNylf-Qsrx",
        "outputId": "e939a9fd-12f5-4b23-992d-1e0bd88a77c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generated Token no : 35\nlm_head_output [batch_sz,updated_seq_len,vocab_sz] : torch.Size([1, 64, 32000])\nlm_head_output[:, -1, :] = [batch_sz,vocab_sz] : torch.Size([1, 32000])\nnext_token_id [predicted_seq_len] : torch.Size([1])\nnext_token_id.unsqueeze(-1) [batch_sz,predicted_seq_len] : torch.Size([1, 1])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nGenerated Text: <s> Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison</s>R: Both Qwen2.5 and Phi-3-mini are single-board computers (SBCs) that can be used for various purposes, such as prototyping, IoT projects, and education. Here is\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:13.552482Z",
          "iopub.execute_input": "2025-03-28T15:12:13.552865Z",
          "iopub.status.idle": "2025-03-28T15:12:13.896416Z",
          "shell.execute_reply.started": "2025-03-28T15:12:13.55283Z",
          "shell.execute_reply": "2025-03-28T15:12:13.895451Z"
        },
        "id": "Yprl58ZkQsry",
        "outputId": "8a1c0acb-694b-4536-ec93-f675c537e822"
      },
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward pass output\n",
        "\n",
        "##### **Why No `loss.backward()` or `optimizer.step()` in Token Prediction?**  \n",
        "\n",
        "The key reason is that we are **only performing inference (prediction)** and **not training**.  \n",
        "Since we are only **generating tokens**, we don't need loss computation or gradient updates.\n",
        "\n",
        "    The sequence length is 1 less because we shift the logits for next-token prediction alignment.\n",
        "    The first token does not have a previous token, so it’s ignored for loss computation.\n",
        "    This is expected behavior in causal language modeling (like GPT).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FMdkMx5SQsrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "model_output = model.model(input_ids)  # Hidden states before lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])  # Logits from the language model head\n",
        "\n",
        "# Shift input_ids and logits for loss calculation\n",
        "shift_logits = lm_head_output[:, :-1, :].contiguous().to(\"cuda:0\") # [batch_sz,seq_len-1,vocab_sz]\n",
        "shift_labels = input_ids[:, 1:].contiguous().to(\"cuda:0\")          # [batch_sz,seq_len-1]\n",
        "\n",
        "# Compute loss\n",
        "# pred :  shift_logits.view(-1, vocab_sz) = [seq_len-1,vocab_sz]\n",
        "# actual : shift_labels.view(-1) = [seq_len-1]\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "# Backward pass(single) - retain_graph=True of you want to run multiple backward pass (it might raise CUDA OutOfMemory)\n",
        "loss.backward(retain_graph=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:13.897448Z",
          "iopub.execute_input": "2025-03-28T15:12:13.897783Z",
          "iopub.status.idle": "2025-03-28T15:12:17.345579Z",
          "shell.execute_reply.started": "2025-03-28T15:12:13.89775Z",
          "shell.execute_reply": "2025-03-28T15:12:17.344864Z"
        },
        "id": "XQjIiyJaQsr1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check gradients\n",
        "print(\"Gradients computed for model parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"layer : {name} --> grad : {param.grad.norm().item()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:17.346394Z",
          "iopub.execute_input": "2025-03-28T15:12:17.346728Z",
          "iopub.status.idle": "2025-03-28T15:12:17.43573Z",
          "shell.execute_reply.started": "2025-03-28T15:12:17.346703Z",
          "shell.execute_reply": "2025-03-28T15:12:17.434988Z"
        },
        "id": "ezbjNElpQsr2",
        "outputId": "9e03fa60-c6aa-486b-cd55-5fbdaf3062cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Gradients computed for model parameters:\nlayer : model.embed_tokens.weight --> grad : 41.375\nlayer : model.layers.0.input_layernorm.weight --> grad : 0.43701171875\nlayer : model.layers.0.post_attention_layernorm.weight --> grad : 0.2274169921875\nlayer : model.layers.1.input_layernorm.weight --> grad : 0.155029296875\nlayer : model.layers.1.post_attention_layernorm.weight --> grad : 0.1331787109375\nlayer : model.layers.2.input_layernorm.weight --> grad : 0.1600341796875\nlayer : model.layers.2.post_attention_layernorm.weight --> grad : 0.1279296875\nlayer : model.layers.3.input_layernorm.weight --> grad : 0.0419921875\nlayer : model.layers.3.post_attention_layernorm.weight --> grad : 0.09185791015625\nlayer : model.layers.4.input_layernorm.weight --> grad : 0.13671875\nlayer : model.layers.4.post_attention_layernorm.weight --> grad : 0.08013916015625\nlayer : model.layers.5.input_layernorm.weight --> grad : 0.08551025390625\nlayer : model.layers.5.post_attention_layernorm.weight --> grad : 0.057037353515625\nlayer : model.layers.6.input_layernorm.weight --> grad : 0.09747314453125\nlayer : model.layers.6.post_attention_layernorm.weight --> grad : 0.0748291015625\nlayer : model.layers.7.input_layernorm.weight --> grad : 0.074462890625\nlayer : model.layers.7.post_attention_layernorm.weight --> grad : 0.03973388671875\nlayer : model.layers.8.input_layernorm.weight --> grad : 0.0175933837890625\nlayer : model.layers.8.post_attention_layernorm.weight --> grad : 0.1483154296875\nlayer : model.layers.9.input_layernorm.weight --> grad : 0.059814453125\nlayer : model.layers.9.post_attention_layernorm.weight --> grad : 0.076171875\nlayer : model.layers.10.input_layernorm.weight --> grad : 0.1373291015625\nlayer : model.layers.10.post_attention_layernorm.weight --> grad : 0.14501953125\nlayer : model.layers.11.input_layernorm.weight --> grad : 0.0614013671875\nlayer : model.layers.11.post_attention_layernorm.weight --> grad : 0.05352783203125\nlayer : model.layers.12.input_layernorm.weight --> grad : 0.06756591796875\nlayer : model.layers.12.post_attention_layernorm.weight --> grad : 0.0875244140625\nlayer : model.layers.13.input_layernorm.weight --> grad : 0.0909423828125\nlayer : model.layers.13.post_attention_layernorm.weight --> grad : 0.0289764404296875\nlayer : model.layers.14.input_layernorm.weight --> grad : 0.041107177734375\nlayer : model.layers.14.post_attention_layernorm.weight --> grad : 0.030029296875\nlayer : model.layers.15.input_layernorm.weight --> grad : 0.0276031494140625\nlayer : model.layers.15.post_attention_layernorm.weight --> grad : 0.029083251953125\nlayer : model.layers.16.input_layernorm.weight --> grad : 0.053314208984375\nlayer : model.layers.16.post_attention_layernorm.weight --> grad : 0.038482666015625\nlayer : model.layers.17.input_layernorm.weight --> grad : 0.0233154296875\nlayer : model.layers.17.post_attention_layernorm.weight --> grad : 0.028961181640625\nlayer : model.layers.18.input_layernorm.weight --> grad : 0.060516357421875\nlayer : model.layers.18.post_attention_layernorm.weight --> grad : 0.0287017822265625\nlayer : model.layers.19.input_layernorm.weight --> grad : 0.036285400390625\nlayer : model.layers.19.post_attention_layernorm.weight --> grad : 0.031646728515625\nlayer : model.layers.20.input_layernorm.weight --> grad : 0.046051025390625\nlayer : model.layers.20.post_attention_layernorm.weight --> grad : 0.0273590087890625\nlayer : model.layers.21.input_layernorm.weight --> grad : 0.0089263916015625\nlayer : model.layers.21.post_attention_layernorm.weight --> grad : 0.021270751953125\nlayer : model.layers.22.input_layernorm.weight --> grad : 0.0146942138671875\nlayer : model.layers.22.post_attention_layernorm.weight --> grad : 0.025787353515625\nlayer : model.layers.23.input_layernorm.weight --> grad : 0.0184478759765625\nlayer : model.layers.23.post_attention_layernorm.weight --> grad : 0.0123138427734375\nlayer : model.layers.24.input_layernorm.weight --> grad : 0.061737060546875\nlayer : model.layers.24.post_attention_layernorm.weight --> grad : 0.0158233642578125\nlayer : model.layers.25.input_layernorm.weight --> grad : 0.01343536376953125\nlayer : model.layers.25.post_attention_layernorm.weight --> grad : 0.0209503173828125\nlayer : model.layers.26.input_layernorm.weight --> grad : 0.015228271484375\nlayer : model.layers.26.post_attention_layernorm.weight --> grad : 0.0166778564453125\nlayer : model.layers.27.input_layernorm.weight --> grad : 0.022857666015625\nlayer : model.layers.27.post_attention_layernorm.weight --> grad : 0.02191162109375\nlayer : model.layers.28.input_layernorm.weight --> grad : 0.008544921875\nlayer : model.layers.28.post_attention_layernorm.weight --> grad : 0.0174102783203125\nlayer : model.layers.29.input_layernorm.weight --> grad : 0.01131439208984375\nlayer : model.layers.29.post_attention_layernorm.weight --> grad : 0.0224761962890625\nlayer : model.layers.30.input_layernorm.weight --> grad : 0.0098876953125\nlayer : model.layers.30.post_attention_layernorm.weight --> grad : 0.029144287109375\nlayer : model.layers.31.input_layernorm.weight --> grad : 0.037811279296875\nlayer : model.layers.31.post_attention_layernorm.weight --> grad : 0.0615234375\nlayer : model.norm.weight --> grad : 0.1239013671875\nlayer : lm_head.weight --> grad : 26.265625\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:17.436557Z",
          "iopub.execute_input": "2025-03-28T15:12:17.436872Z",
          "iopub.status.idle": "2025-03-28T15:12:17.787681Z",
          "shell.execute_reply.started": "2025-03-28T15:12:17.436842Z",
          "shell.execute_reply": "2025-03-28T15:12:17.786987Z"
        },
        "id": "eEXnhEhnQsr4",
        "outputId": "74b9ffff-159b-4a94-99de-cc76c0c086e6"
      },
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral Model Architecture\n",
        "\n",
        "```python\n",
        "\n",
        "MixtralForCausalLM(\n",
        "  (model): MixtralModel(\n",
        "    (embed_tokens): Embedding(32000, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-31): 32 x MixtralDecoderLayer(\n",
        "        (self_attn): MixtralSdpaAttention(\n",
        "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
        "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
        "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): MixtralRotaryEmbedding()\n",
        "        )\n",
        "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
        "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
        "          (experts): ModuleList(\n",
        "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
        "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
        "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
        "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "          )\n",
        "        )\n",
        "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        ")\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ORWpypgpQsr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Config\n",
        "\n",
        "```python\n",
        "\n",
        "MixtralConfig {\n",
        "  \"_name_or_path\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
        "  \"architectures\": [\n",
        "    \"MixtralForCausalLM\"\n",
        "  ],\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 1,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 14336,\n",
        "  \"max_position_embeddings\": 32768,\n",
        "  \"model_type\": \"mixtral\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_experts_per_tok\": 2,\n",
        "  \"num_hidden_layers\": 32,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"num_local_experts\": 8,\n",
        "  \"output_router_logits\": false,\n",
        "  \"rms_norm_eps\": 1e-05,\n",
        "  \"rope_theta\": 1000000.0,\n",
        "  \"router_aux_loss_coef\": 0.02,\n",
        "  \"router_jitter_noise\": 0.0,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.45.1\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 32000\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "OgSBC614Qsr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the most Important Parts of the Model\n",
        "\n",
        "\n",
        "1. **Embedding Layer**: This converts token IDs to embeddings.\n",
        "2. **Self-Attention Layer**: This performs the self-attention mechanism.\n",
        "3. **Block Sparse MoE Experts**: This applies the Mixture of Experts (MoE) mechanism.\n",
        "4. **Post-Attention LayerNorm**: This normalizes the output after the attention mechanism.\n",
        "5. **Final Norm Layer**: This normalizes the final output of the model.\n",
        "6. **Language Model Head**: This converts the final hidden states to logits.\n"
      ],
      "metadata": {
        "id": "mknvf45_Qsr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params_billions = total_params / 1e9  # Convert to billions\n",
        "trainable_params_billions = trainable_params / 1e9\n",
        "non_trainable_params_billions = (total_params - trainable_params) / 1e9\n",
        "\n",
        "print(f\"Total Parameters: {total_params_billions:.2f}B\")  # Format to 2 decimal places\n",
        "print(f\"Trainable Parameters: {trainable_params_billions:.2f}B\")\n",
        "print(f\"Non-Trainable Parameters: {non_trainable_params_billions:.2f}B\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:17.788533Z",
          "iopub.execute_input": "2025-03-28T15:12:17.788859Z",
          "iopub.status.idle": "2025-03-28T15:12:17.802426Z",
          "shell.execute_reply.started": "2025-03-28T15:12:17.78882Z",
          "shell.execute_reply": "2025-03-28T15:12:17.8016Z"
        },
        "id": "xBO9mVWnQsr7",
        "outputId": "fd99167a-9ad4-4bdd-802f-9d308bd08d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total Parameters: 23.48B\nTrainable Parameters: 0.26B\nNon-Trainable Parameters: 23.22B\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a dictionary to store the outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vo_vywSrQsr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outputs = {\n",
        "    \"embed_tokens\": None,\n",
        "    \"self_attn_layer_1\": None,\n",
        "    \"block_sparse_moe_experts\": None,\n",
        "    \"post_attention_layernorm\": None,\n",
        "    \"norm\": None,\n",
        "    \"lm_head\": None,\n",
        "    \"input_layernorm\": None,  # Adding hook for input layernorm\n",
        "    \"self_attn_q_proj\": None,  # Adding hook for q_proj in self_attn\n",
        "    \"self_attn_k_proj\": None,  # Adding hook for k_proj in self_attn\n",
        "    \"self_attn_v_proj\": None,  # Adding hook for v_proj in self_attn\n",
        "    \"self_attn_o_proj\": None,  # Adding hook for o_proj in self_attn\n",
        "    \"block_sparse_moe_gate\": None,  # Adding hook for gate in block_sparse_moe\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:17.803144Z",
          "iopub.execute_input": "2025-03-28T15:12:17.803362Z",
          "iopub.status.idle": "2025-03-28T15:12:17.81963Z",
          "shell.execute_reply.started": "2025-03-28T15:12:17.803336Z",
          "shell.execute_reply": "2025-03-28T15:12:17.818827Z"
        },
        "id": "PV6Ki0fBQsr9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define & Register hooks"
      ],
      "metadata": {
        "id": "aoLQf2v5Qsr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hook functions\n",
        "def hook_fn(name):\n",
        "    def hook(module, input, output):\n",
        "        outputs[name] = output\n",
        "    return hook\n",
        "\n",
        "# Register hooks\n",
        "model.model.embed_tokens.register_forward_hook(hook_fn(\"embed_tokens\"))\n",
        "model.model.layers[0].self_attn.register_forward_hook(hook_fn(\"self_attn_layer_1\"))\n",
        "model.model.layers[0].block_sparse_moe.experts[0].register_forward_hook(hook_fn(\"block_sparse_moe_experts\"))\n",
        "model.model.layers[0].post_attention_layernorm.register_forward_hook(hook_fn(\"post_attention_layernorm\"))\n",
        "model.model.norm.register_forward_hook(hook_fn(\"norm\"))\n",
        "model.lm_head.register_forward_hook(hook_fn(\"lm_head\"))\n",
        "\n",
        "# Additional hooks\n",
        "model.model.layers[0].input_layernorm.register_forward_hook(hook_fn(\"input_layernorm\"))\n",
        "model.model.layers[0].self_attn.q_proj.register_forward_hook(hook_fn(\"self_attn_q_proj\"))\n",
        "model.model.layers[0].self_attn.k_proj.register_forward_hook(hook_fn(\"self_attn_k_proj\"))\n",
        "model.model.layers[0].self_attn.v_proj.register_forward_hook(hook_fn(\"self_attn_v_proj\"))\n",
        "model.model.layers[0].self_attn.o_proj.register_forward_hook(hook_fn(\"self_attn_o_proj\"))\n",
        "model.model.layers[0].block_sparse_moe.gate.register_forward_hook(hook_fn(\"block_sparse_moe_gate\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:17.820535Z",
          "iopub.execute_input": "2025-03-28T15:12:17.820839Z",
          "iopub.status.idle": "2025-03-28T15:12:17.84765Z",
          "shell.execute_reply.started": "2025-03-28T15:12:17.820802Z",
          "shell.execute_reply": "2025-03-28T15:12:17.847022Z"
        },
        "id": "w8UCkyBcQsr-",
        "outputId": "0d3cd240-3f6b-4b1b-a553-4eab0ebd548c"
      },
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<torch.utils.hooks.RemovableHandle at 0x7f5af66b7eb0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Pass"
      ],
      "metadata": {
        "id": "X3Q6yuAxQsr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The quick brown fox jumps over the lazy dog !\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "print(\"Tokenized inputs {'input_ids','attention_mask'} - \",inputs)\n",
        "print(\"Decoded tokens : \",tokenizer.decode(inputs['input_ids'][0]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:17.848483Z",
          "iopub.execute_input": "2025-03-28T15:12:17.848776Z",
          "iopub.status.idle": "2025-03-28T15:12:17.87154Z",
          "shell.execute_reply.started": "2025-03-28T15:12:17.848749Z",
          "shell.execute_reply": "2025-03-28T15:12:17.870963Z"
        },
        "id": "1A-IWz5KQsr_",
        "outputId": "adf30423-9b6a-4460-ad4f-dfb9f3e4f974"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Tokenized inputs {'input_ids','attention_mask'} -  {'input_ids': tensor([[    1,   415,  2936,  9060,   285,  1142,   461, 10575,   754,   272,\n         17898,  3914,   918,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\nDecoded tokens :  <s> The quick brown fox jumps over the lazy dog !</s>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    model_output = model(**inputs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:17.872418Z",
          "iopub.execute_input": "2025-03-28T15:12:17.872705Z",
          "iopub.status.idle": "2025-03-28T15:12:19.232242Z",
          "shell.execute_reply.started": "2025-03-28T15:12:17.872679Z",
          "shell.execute_reply": "2025-03-28T15:12:19.23126Z"
        },
        "id": "3T30tA5UQssA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# for layer, output in outputs.items():\n",
        "#     print(f\"Output at {layer}: \")\n",
        "#     if isinstance(output, torch.Tensor):\n",
        "#         print(output.shape, type(output))\n",
        "#     elif isinstance(output, tuple):\n",
        "#         for i, o in enumerate(output):\n",
        "#             print(f\"Output {i}: {o.shape if isinstance(o, torch.Tensor) else type(o)}\")\n",
        "#     else:\n",
        "#         print(type(output))\n",
        "#     print(\"-\" * 100)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:19.233171Z",
          "iopub.execute_input": "2025-03-28T15:12:19.233397Z",
          "iopub.status.idle": "2025-03-28T15:12:19.236839Z",
          "shell.execute_reply.started": "2025-03-28T15:12:19.233379Z",
          "shell.execute_reply": "2025-03-28T15:12:19.235961Z"
        },
        "id": "J1EWHLEaQssA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| Layer Name                   | Shape Format                      | Dimensions | Notes                                                      |\n",
        "|------------------------------|------------------------------------|------------|------------------------------------------------------------|\n",
        "| `embed_tokens`               | `(batch_size, seq_len, embed_dim)` | `[1, 13, 4096]` | Embedding tokens from vocabulary.                           |\n",
        "| `self_attn_layer_1`           | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Output of first attention layer.                            |\n",
        "| `block_sparse_moe_experts`    | `(num_experts, expert_embed_dim)`  | `[3, 4096]` | Expert outputs in MoE block.                                |\n",
        "| `post_attention_layernorm`    | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Layer norm after attention.                                 |\n",
        "| `norm`                       | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Final normalization layer.                                  |\n",
        "| `lm_head`                    | `(batch_size, seq_len, vocab_size)`| `[1, 13, 32000]` | Logits for each token over the vocabulary.                  |\n",
        "| `input_layernorm`             | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Input layer normalization.                                  |\n",
        "| `self_attn_q_proj`            | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Query projection in self-attention.                         |\n",
        "| `self_attn_k_proj`            | `(batch_size, seq_len, key_dim)`   | `[1, 13, 1024]` | Key projection in self-attention.                           |\n",
        "| `self_attn_v_proj`            | `(batch_size, seq_len, value_dim)` | `[1, 13, 1024]` | Value projection in self-attention.                         |\n",
        "| `self_attn_o_proj`            | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Output projection after attention.                          |\n",
        "| `block_sparse_moe_gate`       | `(seq_len, num_experts)`           | `[13, 8]`   | Gating decisions for the mixture of experts.                |\n",
        "\n"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "mHZNok7zQssN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-28T15:12:19.237515Z",
          "iopub.execute_input": "2025-03-28T15:12:19.237701Z",
          "iopub.status.idle": "2025-03-28T15:12:19.253774Z",
          "shell.execute_reply.started": "2025-03-28T15:12:19.237685Z",
          "shell.execute_reply": "2025-03-28T15:12:19.252951Z"
        },
        "id": "jMFFtzoLQssO"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}