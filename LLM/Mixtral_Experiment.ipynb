{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9692313,
          "sourceType": "datasetVersion",
          "datasetId": 5925699
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Mixtral-Experiment",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "KooZreKGqQET"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "tsr564_json_gbnf_path = kagglehub.dataset_download('tsr564/json-gbnf')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "0Ikg2t92qQEU"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Fetch Hugging Face username and token from Colab secrets\n",
        "HF_USERNAME = \"\"\n",
        "HF_TOKEN = \"\"\n",
        "\n",
        "# Login to Hugging Face\n",
        "try:\n",
        "  login(token=HF_TOKEN)\n",
        "except ValueError:\n",
        "  # If token is not valid or found, login with username and token\n",
        "  # (likely requires manual authorization)\n",
        "  login(username=HF_USERNAME, token=HF_TOKEN)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:20:43.271188Z",
          "iopub.execute_input": "2024-10-18T18:20:43.271841Z",
          "iopub.status.idle": "2024-10-18T18:20:43.353056Z",
          "shell.execute_reply.started": "2024-10-18T18:20:43.271801Z",
          "shell.execute_reply": "2024-10-18T18:20:43.35218Z"
        },
        "id": "tIuyd_5vqQEV",
        "outputId": "ad95cd9c-6cd8-4db1-d6e7-0429f809d618"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,device=\"auto\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,trust_remote_code=True,\n",
        "                                             torch_dtype=torch.bfloat16,\n",
        "                                      low_cpu_mem_usage=True,device_map=\"auto\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:20:43.354757Z",
          "iopub.execute_input": "2024-10-18T18:20:43.355493Z",
          "iopub.status.idle": "2024-10-18T18:30:40.651163Z",
          "shell.execute_reply.started": "2024-10-18T18:20:43.355448Z",
          "shell.execute_reply": "2024-10-18T18:30:40.650377Z"
        },
        "id": "kvfb4yLTqQEW",
        "outputId": "2fc5cfd1-cf69-4e76-c33b-e711f184bea8",
        "colab": {
          "referenced_widgets": [
            "fa5c2b7f05bc412993098a3731e72989",
            "e64a4b6de34d4f40b88305ce507e3658",
            "4675ed906a964735b4334458935ab4b9",
            "e4f24bafae8f4397b76818a34ca9d6e4",
            "3621e97c28544d34ab3953c22d227cd0",
            "dd02aa16c10b4ab78373aa3dae939489",
            "44e75ecc95b74f03a7a58e6ea21165c1",
            "6d26de44c0334077b6c14104747a48ad",
            "57c7fa8051a94bcb96c0309651ab8298",
            "b736720173fd4ba5bbe54cbcc1177423",
            "368fe041fff84949ac30d3d45ac78a0d",
            "79ff492b16e946c8a6238d31b181ffc8",
            "2a12b5905b434c11beaaceaf7e1a6394",
            "9f16b85fde7148b7931c30fb024c87d5",
            "f0bae3fc9925442e82d58ecd7a305808",
            "2181a83c39114bc78b1e4859b3ccdfed",
            "14ad494e78084d8983bc6c0751f9d941",
            "280600190e10484db98261256542f236",
            "562e9f5c0d0d4228b218553019e483b6",
            "cc6675e71cea4018b6adff29d60f0a82",
            "39633f760e104265b1ddc2bcb3e4961d",
            "64288ea1c3074a528339b9d0f9729d18",
            "584114fa6b554a1495f6aa14011e0cc6",
            "2756416bfbcf474c94c1ca2ab4b7d8e3",
            "8c6e4f33682040feb42c1385c66b7ba2",
            "68cc9722525c46328cf963c2a4f2740a",
            "06367bbf0c094ba1bc7d481fb1bfc3f9",
            "1434b26ed3b4449b8fd6a76e0f1e5c97"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa5c2b7f05bc412993098a3731e72989"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e64a4b6de34d4f40b88305ce507e3658"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4675ed906a964735b4334458935ab4b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4f24bafae8f4397b76818a34ca9d6e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3621e97c28544d34ab3953c22d227cd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd02aa16c10b4ab78373aa3dae939489"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44e75ecc95b74f03a7a58e6ea21165c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d26de44c0334077b6c14104747a48ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57c7fa8051a94bcb96c0309651ab8298"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b736720173fd4ba5bbe54cbcc1177423"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "368fe041fff84949ac30d3d45ac78a0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79ff492b16e946c8a6238d31b181ffc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a12b5905b434c11beaaceaf7e1a6394"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f16b85fde7148b7931c30fb024c87d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0bae3fc9925442e82d58ecd7a305808"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2181a83c39114bc78b1e4859b3ccdfed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14ad494e78084d8983bc6c0751f9d941"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "280600190e10484db98261256542f236"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "562e9f5c0d0d4228b218553019e483b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc6675e71cea4018b6adff29d60f0a82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39633f760e104265b1ddc2bcb3e4961d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64288ea1c3074a528339b9d0f9729d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "584114fa6b554a1495f6aa14011e0cc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2756416bfbcf474c94c1ca2ab4b7d8e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c6e4f33682040feb42c1385c66b7ba2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68cc9722525c46328cf963c2a4f2740a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06367bbf0c094ba1bc7d481fb1bfc3f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1434b26ed3b4449b8fd6a76e0f1e5c97"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral Model Architecture\n",
        "\n",
        "```python\n",
        "\n",
        "MixtralForCausalLM(\n",
        "  (model): MixtralModel(\n",
        "    (embed_tokens): Embedding(32000, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-31): 32 x MixtralDecoderLayer(\n",
        "        (self_attn): MixtralSdpaAttention(\n",
        "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
        "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
        "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): MixtralRotaryEmbedding()\n",
        "        )\n",
        "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
        "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
        "          (experts): ModuleList(\n",
        "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
        "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
        "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
        "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "          )\n",
        "        )\n",
        "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        ")\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "RLzAzD5EqQEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Config\n",
        "\n",
        "```python\n",
        "\n",
        "MixtralConfig {\n",
        "  \"_name_or_path\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
        "  \"architectures\": [\n",
        "    \"MixtralForCausalLM\"\n",
        "  ],\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 1,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 14336,\n",
        "  \"max_position_embeddings\": 32768,\n",
        "  \"model_type\": \"mixtral\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_experts_per_tok\": 2,\n",
        "  \"num_hidden_layers\": 32,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"num_local_experts\": 8,\n",
        "  \"output_router_logits\": false,\n",
        "  \"rms_norm_eps\": 1e-05,\n",
        "  \"rope_theta\": 1000000.0,\n",
        "  \"router_aux_loss_coef\": 0.02,\n",
        "  \"router_jitter_noise\": 0.0,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.45.1\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 32000\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "XTHotUSQqQEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the most Important Parts of the Model\n",
        "\n",
        "\n",
        "1. **Embedding Layer**: This converts token IDs to embeddings.\n",
        "2. **Self-Attention Layer**: This performs the self-attention mechanism.\n",
        "3. **Block Sparse MoE Experts**: This applies the Mixture of Experts (MoE) mechanism.\n",
        "4. **Post-Attention LayerNorm**: This normalizes the output after the attention mechanism.\n",
        "5. **Final Norm Layer**: This normalizes the final output of the model.\n",
        "6. **Language Model Head**: This converts the final hidden states to logits.\n"
      ],
      "metadata": {
        "id": "MBrbvIg7qQEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")\n",
        "print(f\"Non-Trainable Parameters: {total_params - trainable_params}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.652378Z",
          "iopub.execute_input": "2024-10-18T18:30:40.652918Z",
          "iopub.status.idle": "2024-10-18T18:30:40.673546Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.652882Z",
          "shell.execute_reply": "2024-10-18T18:30:40.672605Z"
        },
        "id": "gGKkBp1sqQEa",
        "outputId": "2b3b761c-7c77-4073-a50f-b9b206327091"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total Parameters: 46702792704\nTrainable Parameters: 46702792704\nNon-Trainable Parameters: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a dictionary to store the outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nJ2CtDrgqQEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outputs = {\n",
        "    \"embed_tokens\": None,\n",
        "    \"self_attn_layer_1\": None,\n",
        "    \"block_sparse_moe_experts\": None,\n",
        "    \"post_attention_layernorm\": None,\n",
        "    \"norm\": None,\n",
        "    \"lm_head\": None,\n",
        "    \"input_layernorm\": None,  # Adding hook for input layernorm\n",
        "    \"self_attn_q_proj\": None,  # Adding hook for q_proj in self_attn\n",
        "    \"self_attn_k_proj\": None,  # Adding hook for k_proj in self_attn\n",
        "    \"self_attn_v_proj\": None,  # Adding hook for v_proj in self_attn\n",
        "    \"self_attn_o_proj\": None,  # Adding hook for o_proj in self_attn\n",
        "    \"block_sparse_moe_gate\": None,  # Adding hook for gate in block_sparse_moe\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.675424Z",
          "iopub.execute_input": "2024-10-18T18:30:40.675923Z",
          "iopub.status.idle": "2024-10-18T18:30:40.686803Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.675885Z",
          "shell.execute_reply": "2024-10-18T18:30:40.685864Z"
        },
        "id": "vzJBiqzYqQEb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define & Register hooks"
      ],
      "metadata": {
        "id": "XBc16RonqQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hook functions\n",
        "def hook_fn(name):\n",
        "    def hook(module, input, output):\n",
        "        outputs[name] = output\n",
        "    return hook\n",
        "\n",
        "# Register hooks\n",
        "model.model.embed_tokens.register_forward_hook(hook_fn(\"embed_tokens\"))\n",
        "model.model.layers[0].self_attn.register_forward_hook(hook_fn(\"self_attn_layer_1\"))\n",
        "model.model.layers[0].block_sparse_moe.experts[0].register_forward_hook(hook_fn(\"block_sparse_moe_experts\"))\n",
        "model.model.layers[0].post_attention_layernorm.register_forward_hook(hook_fn(\"post_attention_layernorm\"))\n",
        "model.model.norm.register_forward_hook(hook_fn(\"norm\"))\n",
        "model.lm_head.register_forward_hook(hook_fn(\"lm_head\"))\n",
        "\n",
        "# Additional hooks\n",
        "model.model.layers[0].input_layernorm.register_forward_hook(hook_fn(\"input_layernorm\"))\n",
        "model.model.layers[0].self_attn.q_proj.register_forward_hook(hook_fn(\"self_attn_q_proj\"))\n",
        "model.model.layers[0].self_attn.k_proj.register_forward_hook(hook_fn(\"self_attn_k_proj\"))\n",
        "model.model.layers[0].self_attn.v_proj.register_forward_hook(hook_fn(\"self_attn_v_proj\"))\n",
        "model.model.layers[0].self_attn.o_proj.register_forward_hook(hook_fn(\"self_attn_o_proj\"))\n",
        "model.model.layers[0].block_sparse_moe.gate.register_forward_hook(hook_fn(\"block_sparse_moe_gate\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.68794Z",
          "iopub.execute_input": "2024-10-18T18:30:40.688277Z",
          "iopub.status.idle": "2024-10-18T18:30:40.705334Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.688245Z",
          "shell.execute_reply": "2024-10-18T18:30:40.704462Z"
        },
        "id": "jMsW04MjqQEc",
        "outputId": "73d6f142-0bc3-4590-d4dd-2c65a52a5e60"
      },
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<torch.utils.hooks.RemovableHandle at 0x7d29af68c820>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Pass"
      ],
      "metadata": {
        "id": "jDNGySckqQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The quick brown fox jumps over the lazy dog !\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "print(\"Tokenized inputs {'input_ids','attention_mask'} - \",inputs)\n",
        "print(\"Decoded tokens : \",tokenizer.decode(inputs['input_ids'][0]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.706383Z",
          "iopub.execute_input": "2024-10-18T18:30:40.706895Z",
          "iopub.status.idle": "2024-10-18T18:30:40.728093Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.706863Z",
          "shell.execute_reply": "2024-10-18T18:30:40.727243Z"
        },
        "id": "WyJNqLauqQEd",
        "outputId": "bcdf5acd-15dd-4088-a367-68106fd7d3f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Tokenized inputs {'input_ids','attention_mask'} -  {'input_ids': tensor([[    1,   415,  2936,  9060,   285,  1142,   461, 10575,   754,   272,\n         17898,  3914,   918]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\nDecoded tokens :  <s> The quick brown fox jumps over the lazy dog !\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    model_output = model(**inputs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.729287Z",
          "iopub.execute_input": "2024-10-18T18:30:40.729872Z",
          "iopub.status.idle": "2024-10-18T18:36:43.660892Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.72983Z",
          "shell.execute_reply": "2024-10-18T18:36:43.660087Z"
        },
        "id": "zv4QtX0EqQEd",
        "outputId": "25f82a96-c032-4519-808f-b7cee06c14d2"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for layer, output in outputs.items():\n",
        "    print(f\"Output at {layer}: \")\n",
        "    if isinstance(output, torch.Tensor):\n",
        "        print(output.shape, type(output))\n",
        "    elif isinstance(output, tuple):\n",
        "        for i, o in enumerate(output):\n",
        "            print(f\"Output {i}: {o.shape if isinstance(o, torch.Tensor) else type(o)}\")\n",
        "    else:\n",
        "        print(type(output))\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:56:11.28238Z",
          "iopub.execute_input": "2024-10-18T18:56:11.283252Z",
          "iopub.status.idle": "2024-10-18T18:56:11.291437Z",
          "shell.execute_reply.started": "2024-10-18T18:56:11.283214Z",
          "shell.execute_reply": "2024-10-18T18:56:11.290478Z"
        },
        "id": "oisICFqsqQEd",
        "outputId": "9f6f7477-747c-4098-a047-201e4fec58ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Output at embed_tokens: \ntorch.Size([1, 13, 4096]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at self_attn_layer_1: \nOutput 0: torch.Size([1, 13, 4096])\nOutput 1: <class 'NoneType'>\nOutput 2: <class 'transformers.cache_utils.DynamicCache'>\n----------------------------------------------------------------------------------------------------\nOutput at block_sparse_moe_experts: \ntorch.Size([3, 4096]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at post_attention_layernorm: \ntorch.Size([1, 13, 4096]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at norm: \ntorch.Size([1, 13, 4096]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at lm_head: \ntorch.Size([1, 13, 32000]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at input_layernorm: \ntorch.Size([1, 13, 4096]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at self_attn_q_proj: \ntorch.Size([1, 13, 4096]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at self_attn_k_proj: \ntorch.Size([1, 13, 1024]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at self_attn_v_proj: \ntorch.Size([1, 13, 1024]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at self_attn_o_proj: \ntorch.Size([1, 13, 4096]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\nOutput at block_sparse_moe_gate: \ntorch.Size([13, 8]) <class 'torch.Tensor'>\n----------------------------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Explanation of Shapes:\n",
        "\n",
        "### 1. **embed_tokens**\n",
        "- **Shape:** `torch.Size([1, 13, 4096])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size (number of sequences in this batch, here it's 1 sequence).\n",
        "  - `13`: Sequence length (the number of tokens in the input, here 13 tokens).\n",
        "  - `4096`: Embedding size (each token is mapped to a 4096-dimensional vector).\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, embed_dim)`\n",
        "\n",
        "### 2. **self_attn_layer_1**\n",
        "- **Shape (Output 0):** `torch.Size([1, 13, 4096])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `4096`: Hidden size (output of the attention mechanism).\n",
        "\n",
        "  **Shape Format:** `(batch_size, seq_len, hidden_dim)`\n",
        "\n",
        "### 3. **block_sparse_moe_experts**\n",
        "- **Shape:** `torch.Size([3, 4096])`\n",
        "- **Explanation:**\n",
        "  - `3`: Number of activated experts for the MoE (Mixture of Experts) layer. In the `block_sparse_moe`, two experts are chosen per token (2 tokens in the batch may activate the same experts, hence the 3).\n",
        "  - `4096`: Expert embedding size (the dimensionality of the expert's output).\n",
        "  \n",
        "  **Shape Format:** `(num_experts, expert_embed_dim)`\n",
        "\n",
        "### 4. **post_attention_layernorm**\n",
        "- **Shape:** `torch.Size([1, 13, 4096])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `4096`: Output dimension after the post-attention normalization step.\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, hidden_dim)`\n",
        "\n",
        "### 5. **norm**\n",
        "- **Shape:** `torch.Size([1, 13, 4096])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `4096`: Output dimension after applying the final normalization layer.\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, hidden_dim)`\n",
        "\n",
        "### 6. **lm_head**\n",
        "- **Shape:** `torch.Size([1, 13, 32000])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `32000`: Vocabulary size (logits over the vocabulary for each token in the sequence).\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, vocab_size)`\n",
        "\n",
        "### 7. **input_layernorm**\n",
        "- **Shape:** `torch.Size([1, 13, 4096])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `4096`: Output dimension after the input layer normalization step.\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, hidden_dim)`\n",
        "\n",
        "### 8. **self_attn_q_proj**\n",
        "- **Shape:** `torch.Size([1, 13, 4096])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `4096`: Query projection size (the hidden state is projected to the query vector space).\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, hidden_dim)`\n",
        "\n",
        "### 9. **self_attn_k_proj**\n",
        "- **Shape:** `torch.Size([1, 13, 1024])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `1024`: Key projection size (here, the key is projected to a smaller dimensional space compared to queries/values).\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, key_dim)`\n",
        "\n",
        "### 10. **self_attn_v_proj**\n",
        "- **Shape:** `torch.Size([1, 13, 1024])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `1024`: Value projection size (the values are also projected to the same size as the keys).\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, value_dim)`\n",
        "\n",
        "### 11. **self_attn_o_proj**\n",
        "- **Shape:** `torch.Size([1, 13, 4096])`\n",
        "- **Explanation:**\n",
        "  - `1`: Batch size.\n",
        "  - `13`: Sequence length.\n",
        "  - `4096`: Output projection size (the final result after the attention mechanism is projected back to the original hidden dimension).\n",
        "  \n",
        "  **Shape Format:** `(batch_size, seq_len, hidden_dim)`\n",
        "\n",
        "### 12. **block_sparse_moe_gate**\n",
        "- **Shape:** `torch.Size([13, 8])`\n",
        "- **Explanation:**\n",
        "  - `13`: Sequence length (the gate operates per token).\n",
        "  - `8`: Number of experts (gating decisions are made over all available experts).\n",
        "  \n",
        "  **Shape Format:** `(seq_len, num_experts)`\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| Layer Name                   | Shape Format                      | Dimensions | Notes                                                      |\n",
        "|------------------------------|------------------------------------|------------|------------------------------------------------------------|\n",
        "| `embed_tokens`               | `(batch_size, seq_len, embed_dim)` | `[1, 13, 4096]` | Embedding tokens from vocabulary.                           |\n",
        "| `self_attn_layer_1`           | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Output of first attention layer.                            |\n",
        "| `block_sparse_moe_experts`    | `(num_experts, expert_embed_dim)`  | `[3, 4096]` | Expert outputs in MoE block.                                |\n",
        "| `post_attention_layernorm`    | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Layer norm after attention.                                 |\n",
        "| `norm`                       | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Final normalization layer.                                  |\n",
        "| `lm_head`                    | `(batch_size, seq_len, vocab_size)`| `[1, 13, 32000]` | Logits for each token over the vocabulary.                  |\n",
        "| `input_layernorm`             | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Input layer normalization.                                  |\n",
        "| `self_attn_q_proj`            | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Query projection in self-attention.                         |\n",
        "| `self_attn_k_proj`            | `(batch_size, seq_len, key_dim)`   | `[1, 13, 1024]` | Key projection in self-attention.                           |\n",
        "| `self_attn_v_proj`            | `(batch_size, seq_len, value_dim)` | `[1, 13, 1024]` | Value projection in self-attention.                         |\n",
        "| `self_attn_o_proj`            | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Output projection after attention.                          |\n",
        "| `block_sparse_moe_gate`       | `(seq_len, num_experts)`           | `[13, 8]`   | Gating decisions for the mixture of experts.                |\n",
        "\n"
      ],
      "metadata": {
        "id": "ex5YBu38qQEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function calling with Mistral"
      ],
      "metadata": {
        "id": "iZrB3cybqQEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -q llama-cpp-python --no-index --find-links=file:///kaggle/input/llama-cpp-wheels/llamacpp"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-22T12:08:17.065439Z",
          "iopub.execute_input": "2024-10-22T12:08:17.066378Z",
          "iopub.status.idle": "2024-10-22T12:08:19.621011Z",
          "shell.execute_reply.started": "2024-10-22T12:08:17.066325Z",
          "shell.execute_reply": "2024-10-22T12:08:19.619798Z"
        },
        "id": "H8nI4L3TqQEf",
        "outputId": "eccb6547-06d3-42be-c79d-2cd1fb32d78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Location 'file:///kaggle/input/llama-cpp-wheels/llamacpp' is ignored: it is neither a file nor a directory.\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement llama-cpp-python (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for llama-cpp-python\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-cpp-python"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-22T12:08:42.123037Z",
          "iopub.execute_input": "2024-10-22T12:08:42.123797Z",
          "iopub.status.idle": "2024-10-22T12:10:38.854943Z",
          "shell.execute_reply.started": "2024-10-22T12:08:42.123751Z",
          "shell.execute_reply": "2024-10-22T12:10:38.853652Z"
        },
        "id": "_1prusbOqQEf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Fastest and smallest model\n",
        "!curl -L \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q2_K.gguf?download=true\" -o ./mistral-7b-instruct-v0.2.Q2_K.gguf\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-22T12:10:54.981039Z",
          "iopub.execute_input": "2024-10-22T12:10:54.981435Z",
          "iopub.status.idle": "2024-10-22T12:11:12.272234Z",
          "shell.execute_reply.started": "2024-10-22T12:10:54.981394Z",
          "shell.execute_reply": "2024-10-22T12:11:12.271109Z"
        },
        "id": "lF4YVKFmqQEf",
        "outputId": "55897474-0e84-47b8-d2fc-6afa4050843a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1156  100  1156    0     0  10658      0 --:--:-- --:--:-- --:--:-- 10703\n100 2940M  100 2940M    0     0   180M      0  0:00:16  0:00:16 --:--:--  150M\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# !python llama.cpp/convert.py /kaggle/input/codellama/pytorch/7b-python-hf/1 \\\n",
        "#   --outfile codellama-7b.gguf \\\n",
        "#   --outtype q8_0"
      ],
      "metadata": {
        "trusted": true,
        "id": "0F5Ay7v6qQEf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from typing import Any, Callable, Dict\n",
        "import json\n",
        "\n",
        "import llama_cpp\n",
        "from llama_cpp import LlamaGrammar\n",
        "\n",
        "class MethodCaller:\n",
        "\n",
        "    def __init__(self, model: Any):\n",
        "        self.grammar = LlamaGrammar.from_file(\"/kaggle/input/json-gbnf/json.gbnf\")\n",
        "        self.model = model\n",
        "\n",
        "    def get_schema(self, item: Callable) -> Dict[str, Any]:\n",
        "        schema = {\n",
        "            \"name\": item.__name__,\n",
        "            \"description\": str(inspect.getdoc(item)),\n",
        "            \"signature\": str(inspect.signature(item)),\n",
        "            \"output\": str(inspect.signature(item).return_annotation),\n",
        "        }\n",
        "        return schema\n",
        "\n",
        "    def run(self, query: str, function: Callable):\n",
        "\n",
        "        schema = self.get_schema(function)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "            You are a helpful assistant designed to output JSON.\n",
        "            Given the following function schema\n",
        "            << {schema} >>\n",
        "            and query\n",
        "            << {query} >>\n",
        "            extract the parameters values from the query, in a valid JSON format.\n",
        "            Example:\n",
        "            Input:\n",
        "            query: \"How is the weather in Hawaii right now in International units?\"\n",
        "            schema:\n",
        "            {{\n",
        "                \"name\": \"get_weather\",\n",
        "                \"description\": \"Useful to get the weather in a specific location\",\n",
        "                \"signature\": \"(location: str, degree: str) -> str\",\n",
        "                \"output\": \"<class 'str'>\",\n",
        "            }}\n",
        "\n",
        "            Result: {{\n",
        "                \"location\": \"London\",\n",
        "                \"degree\": \"Celsius\",\n",
        "            }}\n",
        "\n",
        "            Input:\n",
        "            query: {query}\n",
        "            schema: {schema}\n",
        "            Result:\n",
        "        \"\"\"\n",
        "\n",
        "        completion = self.model.create_chat_completion(\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }],\n",
        "            temperature=0.1,\n",
        "            max_tokens=500,\n",
        "            grammar=self.grammar,\n",
        "            stream=False,\n",
        "        )\n",
        "\n",
        "        output = completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "        output = output.replace(\"'\", '\"').strip().rstrip(\",\")\n",
        "\n",
        "        function_inputs = json.loads(output)\n",
        "        print( function_inputs )\n",
        "        return  function(**function_inputs)\n",
        "\n",
        "\n",
        "# Function to be called described using the Python docstring format\n",
        "def add_two_numbers( first_number: int, second_number: int ) -> str:\n",
        "    \"\"\"\n",
        "    Adds two numbers together.\n",
        "\n",
        "    :param first_number: The first number to add.\n",
        "    :type first_number: int\n",
        "\n",
        "    :param second_number: The second number to add.\n",
        "    :type second_number: int\n",
        "\n",
        "    :return: The sum of the two numbers.\n",
        "    \"\"\"\n",
        "    return first_number + second_number\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-22T12:11:19.599398Z",
          "iopub.execute_input": "2024-10-22T12:11:19.600133Z",
          "iopub.status.idle": "2024-10-22T12:11:19.74256Z",
          "shell.execute_reply.started": "2024-10-22T12:11:19.600086Z",
          "shell.execute_reply": "2024-10-22T12:11:19.741797Z"
        },
        "id": "JHqknKVSqQEg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "\n",
        "\n",
        "model = llama_cpp.Llama(\n",
        "    model_path=\"/kaggle/working/mistral-7b-instruct-v0.2.Q2_K.gguf\",\n",
        "    n_gpu_layers=-1, # Load all layers into VRAM of the GPU\n",
        "    n_threads=4,     # CPU cores used\n",
        "    n_ctx=2048,      # Max tokens for in + out\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "fun = MethodCaller( model )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-22T12:11:29.401794Z",
          "iopub.execute_input": "2024-10-22T12:11:29.402263Z",
          "iopub.status.idle": "2024-10-22T12:11:29.800884Z",
          "shell.execute_reply.started": "2024-10-22T12:11:29.402226Z",
          "shell.execute_reply": "2024-10-22T12:11:29.799857Z"
        },
        "id": "EG-MIjYFqQEg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "result = fun.run(\"Add 66 and seventy\", add_two_numbers)\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-22T12:11:42.229583Z",
          "iopub.execute_input": "2024-10-22T12:11:42.230571Z",
          "iopub.status.idle": "2024-10-22T12:13:16.213749Z",
          "shell.execute_reply.started": "2024-10-22T12:11:42.230503Z",
          "shell.execute_reply": "2024-10-22T12:13:16.212934Z"
        },
        "id": "5GUQh3xiqQEg",
        "outputId": "6857a963-82b5-4a99-dffd-23b65420f88e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'first_number': 66, 'second_number': 70}\n136\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VnJiJNWAqQEh"
      }
    }
  ]
}