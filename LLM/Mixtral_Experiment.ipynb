{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 67121,
          "databundleVersionId": 7806901,
          "sourceType": "competition"
        },
        {
          "sourceId": 10765944,
          "sourceType": "datasetVersion",
          "datasetId": 6678406
        },
        {
          "sourceId": 5994,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 4761,
          "modelId": 3108
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Mixtral-Experiment",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "m6gmODGmMB5b"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "llm_prompt_recovery_path = kagglehub.competition_download('llm-prompt-recovery')\n",
        "tsr564_gemma_rewrite_nbroad_1_path = kagglehub.dataset_download('tsr564/gemma-rewrite-nbroad-1')\n",
        "mistral_ai_mixtral_pytorch_8x7b_instruct_v0_1_hf_1_path = kagglehub.model_download('mistral-ai/mixtral/PyTorch/8x7b-instruct-v0.1-hf/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "wdnZTX6GMB5e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral\n",
        "\n",
        "https://github.com/hkproj/mistral-llm-notes\n",
        "\n",
        "https://github.com/neobundy/Deep-Dive-Into-AI-With-MLX-PyTorch/blob/master/deep-dives/001-mistral-7b/README.md\n",
        "\n",
        "https://github.com/DongmingShenDS/Mistral_From_Scratch\n",
        "\n",
        "\n",
        "### Use case\n",
        "\n",
        "https://www.kaggle.com/code/huikang/code-interpreter-baseline\n",
        "\n",
        "\n",
        "# Llama3\n",
        "\n",
        "https://github.com/naklecha/llama3-from-scratch/blob/main/llama3-from-scratch.ipynb\n",
        "\n",
        "https://github.com/evintunador/minLlama3\n"
      ],
      "metadata": {
        "id": "egKsL0uUMB5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torchvision"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:14:09.903646Z",
          "iopub.execute_input": "2025-02-17T04:14:09.904083Z",
          "iopub.status.idle": "2025-02-17T04:17:07.043932Z",
          "shell.execute_reply.started": "2025-02-17T04:14:09.904056Z",
          "shell.execute_reply": "2025-02-17T04:17:07.042878Z"
        },
        "id": "7J2LEbkyMB5n",
        "outputId": "e71f66a9-8d4d-4048-c4fc-836a8058bb08"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 24.12.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 24.12.1 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git  -U\n",
        "!pip install -q git+https://github.com/huggingface/accelerate.git  -U\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q git+https://github.com/huggingface/peft.git  -U"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:19:32.194207Z",
          "iopub.execute_input": "2025-02-17T04:19:32.194508Z",
          "iopub.status.idle": "2025-02-17T04:20:31.070455Z",
          "shell.execute_reply.started": "2025-02-17T04:19:32.194481Z",
          "shell.execute_reply": "2025-02-17T04:20:31.069527Z"
        },
        "id": "PN7F1fP9MB5p",
        "outputId": "3f54cf12-aa66-4386-d6a2-cd554ffa481b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "#https://github.com/Lightning-AI/lit-gpt/issues/327\n",
        "# torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "# torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:20:31.071811Z",
          "iopub.execute_input": "2025-02-17T04:20:31.072139Z",
          "iopub.status.idle": "2025-02-17T04:20:34.662409Z",
          "shell.execute_reply.started": "2025-02-17T04:20:31.072107Z",
          "shell.execute_reply": "2025-02-17T04:20:34.661671Z"
        },
        "id": "riZy_8RzMB5p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#this can help speed up inference\n",
        "max_new_tokens = 30\n",
        "\n",
        "#output test is trimmed according to this\n",
        "max_sentences_in_response = 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:20:34.663828Z",
          "iopub.execute_input": "2025-02-17T04:20:34.664286Z",
          "iopub.status.idle": "2025-02-17T04:20:34.667636Z",
          "shell.execute_reply.started": "2025-02-17T04:20:34.664261Z",
          "shell.execute_reply": "2025-02-17T04:20:34.666882Z"
        },
        "id": "fWtH0w5yMB5q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
        "\n",
        "\n",
        "model_name = '/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    model_max_length=512,  # Reduce maximum sequence length\n",
        ")\n",
        "\n",
        "# Load base model(Mistral 7B)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload= True\n",
        ")\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.gradient_checkpointing = True\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16,\n",
        "        attn_implementation = \"eager\",\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        config=config,\n",
        "        # max_memory={0: \"8GiB\",1:\"13GiB\"},  # Limit GPU memory usage\n",
        "        offload_folder=\"offload\",  # Specify offload directory\n",
        "        offload_state_dict=True  # Enable state dict offloading\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:20:34.668643Z",
          "iopub.execute_input": "2025-02-17T04:20:34.668948Z",
          "iopub.status.idle": "2025-02-17T04:29:30.515623Z",
          "shell.execute_reply.started": "2025-02-17T04:20:34.668925Z",
          "shell.execute_reply": "2025-02-17T04:29:30.514933Z"
        },
        "id": "hxyvpnKGMB5q",
        "outputId": "29c4f244-4457-467b-fed2-17abde220cf5",
        "colab": {
          "referenced_widgets": [
            "f732ccb4e532471db5b71c84ffc24e78"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f732ccb4e532471db5b71c84ffc24e78"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # LoRA config\n",
        "# from peft import LoraConfig,PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "# peft_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        "# )\n",
        "# model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nS4IMn0QMB5r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:29:30.516534Z",
          "iopub.execute_input": "2025-02-17T04:29:30.517191Z",
          "iopub.status.idle": "2025-02-17T04:29:30.813513Z",
          "shell.execute_reply.started": "2025-02-17T04:29:30.517157Z",
          "shell.execute_reply": "2025-02-17T04:29:30.812769Z"
        },
        "id": "R2kWa5wsMB5s",
        "outputId": "80a8c01a-3a32-42ff-dd7d-35d9d5f73906"
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "51"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model"
      ],
      "metadata": {
        "id": "EyUyY1xtMB5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:42:21.707255Z",
          "iopub.execute_input": "2025-02-17T04:42:21.707553Z",
          "iopub.status.idle": "2025-02-17T04:42:21.711466Z",
          "shell.execute_reply.started": "2025-02-17T04:42:21.707529Z",
          "shell.execute_reply": "2025-02-17T04:42:21.710545Z"
        },
        "id": "r25U0dEmMB5u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison\"\"\"\n",
        "\n",
        "tokens = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "# input_ids = tokens.input_ids.to(\"cuda\")\n",
        "# attention_masks = tokens.attention_mask.to(\"cuda\")\n",
        "print(f\" input_ids : {tokens.input_ids.shape}\")\n",
        "\n",
        "for idx, token_id in enumerate(tokens.input_ids[0]):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(token_id) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )\n",
        "\n",
        "generation_output = model.generate(**tokens,max_new_tokens=100)\n",
        "print(\"generated output(max_tokens + input_ids) : \",generation_output[0].shape) #\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:42:26.489031Z",
          "iopub.execute_input": "2025-02-17T04:42:26.489356Z",
          "iopub.status.idle": "2025-02-17T04:42:54.410771Z",
          "shell.execute_reply.started": "2025-02-17T04:42:26.489332Z",
          "shell.execute_reply": "2025-02-17T04:42:54.409992Z"
        },
        "id": "p9kHKkKiMB5v",
        "outputId": "d6267c11-e586-4727-a022-13428d60e6f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " input_ids : torch.Size([1, 29])\n\u001b[0;30;48;2;102;194;165m<s>\u001b[0m \u001b[0;30;48;2;252;141;98mQ\u001b[0m \u001b[0;30;48;2;141;160;203mwen\u001b[0m \u001b[0;30;48;2;231;138;195m2\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m5\u001b[0m \u001b[0;30;48;2;102;194;165mvs\u001b[0m \u001b[0;30;48;2;252;141;98mPh\u001b[0m \u001b[0;30;48;2;141;160;203mi\u001b[0m \u001b[0;30;48;2;231;138;195m-\u001b[0m \u001b[0;30;48;2;166;216;84m3\u001b[0m \u001b[0;30;48;2;255;217;47m-\u001b[0m \u001b[0;30;48;2;102;194;165mmin\u001b[0m \u001b[0;30;48;2;252;141;98mi\u001b[0m \u001b[0;30;48;2;141;160;203mwhich\u001b[0m \u001b[0;30;48;2;231;138;195mone\u001b[0m \u001b[0;30;48;2;166;216;84mis\u001b[0m \u001b[0;30;48;2;255;217;47mbetter\u001b[0m \u001b[0;30;48;2;102;194;165mfor\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203mpurpose\u001b[0m \u001b[0;30;48;2;231;138;195m?\u001b[0m \u001b[0;30;48;2;166;216;84mTell\u001b[0m \u001b[0;30;48;2;255;217;47mme\u001b[0m \u001b[0;30;48;2;102;194;165mtechnical\u001b[0m \u001b[0;30;48;2;252;141;98muse\u001b[0m \u001b[0;30;48;2;141;160;203mcase\u001b[0m \u001b[0;30;48;2;231;138;195mcomparison\u001b[0m \u001b[0;30;48;2;166;216;84m</s>\u001b[0m generated output(max_tokens + input_ids) :  torch.Size([129])\n<s> Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison</s>R: Both Qwen2.5 and Phi-3-mini are single-board computers (SBCs) that can be used for various purposes, such as prototyping, IoT projects, and education. Here is a comparison of their technical specifications and use cases:\n\nQwen2.5:\n\n* CPU: Allwinner H3 quad-core Cortex-A7\n* GPU: Mali-400MP2\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward pass output\n",
        "\n",
        "logits & past_key_values will have same shape"
      ],
      "metadata": {
        "id": "ZZ63tR6kMB5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**tokens)\n",
        "#\n",
        "print(\"logits shape ([batch_sz,seq_len,vocab_sz]) : \",output.logits.shape)\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(f\"No. of Decoder : {len(output.past_key_values)}\")\n",
        "print(f\"Cached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : {len(output.past_key_values[0])}\")\n",
        "print(f\"Key(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {output.past_key_values[0][0].shape}\")\n",
        "print(f\"Value(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {output.past_key_values[0][1].shape}\")\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(\"Predicted token after the forward pass\")\n",
        "for idx,logits_value in enumerate(output.logits[0]):\n",
        "    token_id = torch.argmax(logits_value,dim=-1)\n",
        "    print(f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +tokenizer.decode(token_id) +'\\x1b[0m',end=' ')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:42:54.41172Z",
          "iopub.execute_input": "2025-02-17T04:42:54.412027Z",
          "iopub.status.idle": "2025-02-17T04:42:55.907184Z",
          "shell.execute_reply.started": "2025-02-17T04:42:54.412003Z",
          "shell.execute_reply": "2025-02-17T04:42:55.906304Z"
        },
        "id": "KInP8wibMB5w",
        "outputId": "52a0b990-a9ae-47c4-8d7a-6844aadb3c3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "logits shape ([batch_sz,seq_len,vocab_sz]) :  torch.Size([1, 29, 32000])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nNo. of Decoder : 32\nCached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : 2\nKey(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\nValue(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nPredicted token after the forward pass\n\u001b[0;30;48;2;102;194;165mQ\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203mch\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m\n\u001b[0m \u001b[0;30;48;2;102;194;165mQ\u001b[0m \u001b[0;30;48;2;252;141;98mp\u001b[0m \u001b[0;30;48;2;141;160;203m2\u001b[0m \u001b[0;30;48;2;231;138;195m1\u001b[0m \u001b[0;30;48;2;166;216;84m\n\u001b[0m \u001b[0;30;48;2;255;217;47m2\u001b[0m \u001b[0;30;48;2;102;194;165mi\u001b[0m \u001b[0;30;48;2;252;141;98m\n\u001b[0m \u001b[0;30;48;2;141;160;203mone\u001b[0m \u001b[0;30;48;2;231;138;195mis\u001b[0m \u001b[0;30;48;2;166;216;84mbetter\u001b[0m \u001b[0;30;48;2;255;217;47m?\u001b[0m \u001b[0;30;48;2;102;194;165mme\u001b[0m \u001b[0;30;48;2;252;141;98m?\u001b[0m \u001b[0;30;48;2;141;160;203m?\u001b[0m \u001b[0;30;48;2;231;138;195m\n\u001b[0m \u001b[0;30;48;2;166;216;84mme\u001b[0m \u001b[0;30;48;2;255;217;47mthe\u001b[0m \u001b[0;30;48;2;102;194;165mdetails\u001b[0m \u001b[0;30;48;2;252;141;98mof\u001b[0m \u001b[0;30;48;2;141;160;203m\n\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84mR\u001b[0m ",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward pass of input through model except the lm_head then passing through lm_head"
      ],
      "metadata": {
        "id": "fYNzV5n2MB5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = model.model(tokens.input_ids)\n",
        "\n",
        "print(\"last_hidden_state shape ([batch_sz,seq_len,vocab_sz]) : \",model_output[0].shape)\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(f\"No. of Decoder : {len(model_output.past_key_values)}\")\n",
        "print(f\"Cached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : {len(model_output.past_key_values[0])}\")\n",
        "print(f\"Key(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {model_output.past_key_values[0][0].shape}\")\n",
        "print(f\"Value(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {model_output.past_key_values[0][1].shape}\")\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "lm_head_output = model.lm_head(model_output[0])\n",
        "print(\"lm_head/logits shape ([batch_sz,seq_len,vocab_sz]) : \",lm_head_output.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:42:55.908494Z",
          "iopub.execute_input": "2025-02-17T04:42:55.908739Z",
          "iopub.status.idle": "2025-02-17T04:42:57.251084Z",
          "shell.execute_reply.started": "2025-02-17T04:42:55.908718Z",
          "shell.execute_reply": "2025-02-17T04:42:57.250212Z"
        },
        "id": "LDbgcECTMB5y",
        "outputId": "1d59e621-edcd-407e-b6e3-cf6f8c7e3d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "last_hidden_state shape ([batch_sz,seq_len,vocab_sz]) :  torch.Size([1, 29, 4096])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nNo. of Decoder : 32\nCached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : 2\nKey(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\nValue(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nlm_head/logits shape ([batch_sz,seq_len,vocab_sz]) :  torch.Size([1, 29, 32000])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Theoretically, both should match but\n",
        "    If False, then small numerical differences exist due to normalization, dropout, or caching.\n",
        "\n",
        "\n",
        "\n",
        "##### **Why Are the Values Different?**\n",
        "1. **Dropout (During Training Mode)**\n",
        "   - If the model is in **training mode (`model.train()`)**, dropout is applied, leading to slightly different hidden states.\n",
        "   - Try running in **evaluation mode**:\n",
        "     ```python\n",
        "     model.eval()\n",
        "     ```\n",
        "\n",
        "2. **LayerNorm / Residual Connection Effects**\n",
        "   - Some architectures (e.g., GPT) apply **Layer Normalization and residual connections differently** inside `forward()`.\n",
        "   - If you extract `model.model(input_ids)`, it **may not be identical** to the forward method.\n",
        "\n",
        "3. **Past Key-Value Caching (for Decoding)**\n",
        "   - When using `model(**tokens)`, **past key-value states** are handled inside the model, which can slightly alter values.\n",
        "   - Try disabling caching:\n",
        "     ```python\n",
        "     output = model(**tokens, use_cache=False)\n",
        "     ```\n"
      ],
      "metadata": {
        "id": "PQ8aly7HMB5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# assert lm_head_output.shape==output.logits.shape\n",
        "# torch.allclose(lm_head_output.to(\"cuda:0\"), output.logits.to(\"cuda:0\"), atol=1e-5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:13:53.722301Z",
          "iopub.execute_input": "2025-02-17T04:13:53.722605Z",
          "iopub.status.idle": "2025-02-17T04:13:57.849416Z",
          "shell.execute_reply.started": "2025-02-17T04:13:53.722581Z",
          "shell.execute_reply": "2025-02-17T04:13:57.848746Z"
        },
        "id": "yA3e2oRUMB50"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual inference"
      ],
      "metadata": {
        "id": "sHcdB1HpMB51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "num_tokens_to_generate = 50  # Number of tokens to predict\n",
        "\n",
        "tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "input_ids = tokens.input_ids  # Initial tokenized input\n",
        "\n",
        "for token_idx in range(num_tokens_to_generate):\n",
        "    with torch.no_grad():  # No need to compute gradients during inference\n",
        "        model_output = model.model(input_ids)  # Get hidden states\n",
        "        lm_head_output = model.lm_head(model_output[0]).to(\"cuda:0\")  # Get logits\n",
        "\n",
        "    # Get the predicted token ID (argmax over vocabulary)\n",
        "    next_token_id = lm_head_output[:, -1, :].argmax(dim=-1)\n",
        "    if token_idx==35:\n",
        "        print(f\"Generated Token no : {token_idx}\")\n",
        "        print(f\"lm_head_output [batch_sz,updated_seq_len,vocab_sz] : {lm_head_output.shape}\")\n",
        "        print(f\"lm_head_output[:, -1, :] = [batch_sz,vocab_sz] : { lm_head_output[:, -1, :].shape}\")\n",
        "        print(f\"next_token_id [predicted_seq_len] : {next_token_id.shape}\")\n",
        "        print(f\"next_token_id.unsqueeze(-1) [batch_sz,predicted_seq_len] : {next_token_id.unsqueeze(-1).shape}\")\n",
        "        print(\"-\"*100)\n",
        "        print(\"-\"*100)\n",
        "\n",
        "    # Append predicted token to input_ids\n",
        "    input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "# Decode the generated sequence\n",
        "predicted_text = tokenizer.decode(input_ids[0])\n",
        "\n",
        "print(\"Generated Text:\", predicted_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T03:52:23.230488Z",
          "iopub.execute_input": "2025-02-17T03:52:23.230815Z",
          "iopub.status.idle": "2025-02-17T03:53:46.88443Z",
          "shell.execute_reply.started": "2025-02-17T03:52:23.230791Z",
          "shell.execute_reply": "2025-02-17T03:53:46.883676Z"
        },
        "id": "ZWVyL93-MB51",
        "outputId": "3fdbd25b-d889-462e-823c-28b3911e19db"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generated Token no : 35\nlm_head_output [batch_sz,updated_seq_len,vocab_sz] : torch.Size([1, 64, 32000])\nlm_head_output[:, -1, :] = [batch_sz,vocab_sz] : torch.Size([1, 32000])\nnext_token_id [predicted_seq_len] : torch.Size([1])\nnext_token_id.unsqueeze(-1) [batch_sz,predicted_seq_len] : torch.Size([1, 1])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nGenerated Text: <s> Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison</s>R: Both Qwen2.5 and Phi-3-mini are single-board computers (SBCs) that can be used for various purposes, such as prototyping, IoT projects, and education. Here is\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T03:53:54.457081Z",
          "iopub.execute_input": "2025-02-17T03:53:54.457429Z",
          "iopub.status.idle": "2025-02-17T03:53:54.814363Z",
          "shell.execute_reply.started": "2025-02-17T03:53:54.457399Z",
          "shell.execute_reply": "2025-02-17T03:53:54.813587Z"
        },
        "id": "ukTxqWAUMB52",
        "outputId": "d00508c5-8775-428a-8ab2-ce781c21b253"
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward pass output\n",
        "\n",
        "##### **Why No `loss.backward()` or `optimizer.step()` in Token Prediction?**  \n",
        "\n",
        "The key reason is that we are **only performing inference (prediction)** and **not training**.  \n",
        "Since we are only **generating tokens**, we don't need loss computation or gradient updates.\n",
        "\n",
        "    The sequence length is 1 less because we shift the logits for next-token prediction alignment.\n",
        "    The first token does not have a previous token, so it’s ignored for loss computation.\n",
        "    This is expected behavior in causal language modeling (like GPT).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_bPGbbBqMB53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "model_output = model.model(input_ids)  # Hidden states before lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])  # Logits from the language model head\n",
        "\n",
        "# Shift input_ids and logits for loss calculation\n",
        "shift_logits = lm_head_output[:, :-1, :].contiguous().to(\"cuda:0\") # [batch_sz,seq_len-1,vocab_sz]\n",
        "shift_labels = input_ids[:, 1:].contiguous().to(\"cuda:0\")          # [batch_sz,seq_len-1]\n",
        "\n",
        "# Compute loss\n",
        "# pred :  shift_logits.view(-1, vocab_sz) = [seq_len-1,vocab_sz]\n",
        "# actual : shift_labels.view(-1) = [seq_len-1]\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "# Backward pass(single) - retain_graph=True of you want to run multiple backward pass (it might raise CUDA OutOfMemory)\n",
        "loss.backward(retain_graph=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T03:54:38.516699Z",
          "iopub.execute_input": "2025-02-17T03:54:38.516982Z",
          "iopub.status.idle": "2025-02-17T03:54:42.231049Z",
          "shell.execute_reply.started": "2025-02-17T03:54:38.516962Z",
          "shell.execute_reply": "2025-02-17T03:54:42.230139Z"
        },
        "id": "0_Kqz4yzMB53"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check gradients\n",
        "print(\"Gradients computed for model parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"layer : {name} --> grad : {param.grad.norm().item()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T03:55:04.650588Z",
          "iopub.execute_input": "2025-02-17T03:55:04.650905Z",
          "iopub.status.idle": "2025-02-17T03:55:04.74844Z",
          "shell.execute_reply.started": "2025-02-17T03:55:04.650882Z",
          "shell.execute_reply": "2025-02-17T03:55:04.747719Z"
        },
        "id": "zxdHXC9fMB54",
        "outputId": "f2e24b00-3ed4-435d-9b4b-37185c08ebb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Gradients computed for model parameters:\nlayer : model.embed_tokens.weight --> grad : 41.375\nlayer : model.layers.0.input_layernorm.weight --> grad : 0.43701171875\nlayer : model.layers.0.post_attention_layernorm.weight --> grad : 0.2274169921875\nlayer : model.layers.1.input_layernorm.weight --> grad : 0.155029296875\nlayer : model.layers.1.post_attention_layernorm.weight --> grad : 0.1331787109375\nlayer : model.layers.2.input_layernorm.weight --> grad : 0.1600341796875\nlayer : model.layers.2.post_attention_layernorm.weight --> grad : 0.1279296875\nlayer : model.layers.3.input_layernorm.weight --> grad : 0.0419921875\nlayer : model.layers.3.post_attention_layernorm.weight --> grad : 0.09185791015625\nlayer : model.layers.4.input_layernorm.weight --> grad : 0.13671875\nlayer : model.layers.4.post_attention_layernorm.weight --> grad : 0.08013916015625\nlayer : model.layers.5.input_layernorm.weight --> grad : 0.08551025390625\nlayer : model.layers.5.post_attention_layernorm.weight --> grad : 0.057037353515625\nlayer : model.layers.6.input_layernorm.weight --> grad : 0.09747314453125\nlayer : model.layers.6.post_attention_layernorm.weight --> grad : 0.0748291015625\nlayer : model.layers.7.input_layernorm.weight --> grad : 0.074462890625\nlayer : model.layers.7.post_attention_layernorm.weight --> grad : 0.03973388671875\nlayer : model.layers.8.input_layernorm.weight --> grad : 0.0175933837890625\nlayer : model.layers.8.post_attention_layernorm.weight --> grad : 0.1483154296875\nlayer : model.layers.9.input_layernorm.weight --> grad : 0.059814453125\nlayer : model.layers.9.post_attention_layernorm.weight --> grad : 0.076171875\nlayer : model.layers.10.input_layernorm.weight --> grad : 0.1373291015625\nlayer : model.layers.10.post_attention_layernorm.weight --> grad : 0.14501953125\nlayer : model.layers.11.input_layernorm.weight --> grad : 0.0614013671875\nlayer : model.layers.11.post_attention_layernorm.weight --> grad : 0.05352783203125\nlayer : model.layers.12.input_layernorm.weight --> grad : 0.06756591796875\nlayer : model.layers.12.post_attention_layernorm.weight --> grad : 0.0875244140625\nlayer : model.layers.13.input_layernorm.weight --> grad : 0.0909423828125\nlayer : model.layers.13.post_attention_layernorm.weight --> grad : 0.0289764404296875\nlayer : model.layers.14.input_layernorm.weight --> grad : 0.041107177734375\nlayer : model.layers.14.post_attention_layernorm.weight --> grad : 0.030029296875\nlayer : model.layers.15.input_layernorm.weight --> grad : 0.0276031494140625\nlayer : model.layers.15.post_attention_layernorm.weight --> grad : 0.029083251953125\nlayer : model.layers.16.input_layernorm.weight --> grad : 0.053314208984375\nlayer : model.layers.16.post_attention_layernorm.weight --> grad : 0.038482666015625\nlayer : model.layers.17.input_layernorm.weight --> grad : 0.0233154296875\nlayer : model.layers.17.post_attention_layernorm.weight --> grad : 0.028961181640625\nlayer : model.layers.18.input_layernorm.weight --> grad : 0.060516357421875\nlayer : model.layers.18.post_attention_layernorm.weight --> grad : 0.0287017822265625\nlayer : model.layers.19.input_layernorm.weight --> grad : 0.036285400390625\nlayer : model.layers.19.post_attention_layernorm.weight --> grad : 0.031646728515625\nlayer : model.layers.20.input_layernorm.weight --> grad : 0.046051025390625\nlayer : model.layers.20.post_attention_layernorm.weight --> grad : 0.0273590087890625\nlayer : model.layers.21.input_layernorm.weight --> grad : 0.0089263916015625\nlayer : model.layers.21.post_attention_layernorm.weight --> grad : 0.021270751953125\nlayer : model.layers.22.input_layernorm.weight --> grad : 0.0146942138671875\nlayer : model.layers.22.post_attention_layernorm.weight --> grad : 0.025787353515625\nlayer : model.layers.23.input_layernorm.weight --> grad : 0.0184478759765625\nlayer : model.layers.23.post_attention_layernorm.weight --> grad : 0.0123138427734375\nlayer : model.layers.24.input_layernorm.weight --> grad : 0.061737060546875\nlayer : model.layers.24.post_attention_layernorm.weight --> grad : 0.0158233642578125\nlayer : model.layers.25.input_layernorm.weight --> grad : 0.01343536376953125\nlayer : model.layers.25.post_attention_layernorm.weight --> grad : 0.0209503173828125\nlayer : model.layers.26.input_layernorm.weight --> grad : 0.015228271484375\nlayer : model.layers.26.post_attention_layernorm.weight --> grad : 0.0166778564453125\nlayer : model.layers.27.input_layernorm.weight --> grad : 0.022857666015625\nlayer : model.layers.27.post_attention_layernorm.weight --> grad : 0.02191162109375\nlayer : model.layers.28.input_layernorm.weight --> grad : 0.008544921875\nlayer : model.layers.28.post_attention_layernorm.weight --> grad : 0.0174102783203125\nlayer : model.layers.29.input_layernorm.weight --> grad : 0.01131439208984375\nlayer : model.layers.29.post_attention_layernorm.weight --> grad : 0.0224761962890625\nlayer : model.layers.30.input_layernorm.weight --> grad : 0.0098876953125\nlayer : model.layers.30.post_attention_layernorm.weight --> grad : 0.029144287109375\nlayer : model.layers.31.input_layernorm.weight --> grad : 0.037811279296875\nlayer : model.layers.31.post_attention_layernorm.weight --> grad : 0.0615234375\nlayer : model.norm.weight --> grad : 0.1239013671875\nlayer : lm_head.weight --> grad : 26.265625\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:43:32.139902Z",
          "iopub.execute_input": "2025-02-17T04:43:32.140221Z",
          "iopub.status.idle": "2025-02-17T04:43:32.453836Z",
          "shell.execute_reply.started": "2025-02-17T04:43:32.140194Z",
          "shell.execute_reply": "2025-02-17T04:43:32.453168Z"
        },
        "id": "WURodu8hMB55",
        "outputId": "874b5e9d-ad60-4b20-f76b-3fb53b5abedf"
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt components"
      ],
      "metadata": {
        "id": "9X28vYrvMB56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#original text prefix\n",
        "orig_prefix = \"Original Text:\"\n",
        "\n",
        "#mistral \"response\"\n",
        "llm_response_for_rewrite = \"Provide the new text and I will tell you in a single sentence the request that changed it into the re-written text.  I understand no person, place or thing from the original text be mentioned (they are top-secret).  NOTHING from the original text may appear in my answer.  My answer will be super-short, a single sentence.  I will tell you the prompt that caused the change.\"\n",
        "\n",
        "#modified text prefix\n",
        "rewrite_prefix = \"Re-written Text:\"\n",
        "\n",
        "#provided as start of Mistral response (anything after this is used as the prompt)\n",
        "#providing this as the start of the response helps keep things relevant\n",
        "response_start = \"The request was: \"\n",
        "\n",
        "#improve this text so...\n",
        "prompt_seed = \"Improve this text so\"\n",
        "\n",
        "\n",
        "#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61\n",
        "base_line = 'Refine the following passage by emulating the writing style of [insert desired style here], with a focus on enhancing its clarity, elegance, and overall impact. Preserve the essence and original meaning of the text, while meticulously adjusting its tone, vocabulary, and stylistic elements to resonate with the chosen style.Please improve the following text using the writing style of, maintaining the original meaning but altering the tone, diction, and stylistic elements to match the new style.Enhance the clarity, elegance, and impact of the following text by adopting the writing style of , ensuring the core message remains intact while transforming the tone, word choice, and stylistic features to align with the specified style.'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:29:30.815347Z",
          "iopub.execute_input": "2025-02-17T04:29:30.815584Z",
          "iopub.status.idle": "2025-02-17T04:29:30.829424Z",
          "shell.execute_reply.started": "2025-02-17T04:29:30.815563Z",
          "shell.execute_reply": "2025-02-17T04:29:30.828665Z"
        },
        "id": "Ztwv9n88MB57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A couple examples...\n",
        "\n",
        "### These are provided to Mistral before each actual query"
      ],
      "metadata": {
        "id": "mN16C5eGMB58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#first example\n",
        "example_text_1 = \"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\"\n",
        "example_rewrite_1 = \"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\"\n",
        "example_prompt_1 = \"Improve this text so it's a warning.\"\n",
        "\n",
        "#second example\n",
        "example_text_2 = \"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\"\n",
        "example_rewrite_2 = \"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\"\n",
        "example_prompt_2 = \"Transform the text to a dynamic, rhythm-infused rap. Keep the essence of the information intact but weave in artistic elements, utilizing rhythm, rhyme, and a conversational style. This approach should make the content more relatable and enjoyable, aiming to both educate and entertain your audience.\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:29:30.830521Z",
          "iopub.execute_input": "2025-02-17T04:29:30.830747Z",
          "iopub.status.idle": "2025-02-17T04:29:30.847196Z",
          "shell.execute_reply.started": "2025-02-17T04:29:30.830728Z",
          "shell.execute_reply": "2025-02-17T04:29:30.846504Z"
        },
        "id": "sfTvhmroMB58"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function\n",
        "\n",
        "Or we can use TextStreamer"
      ],
      "metadata": {
        "id": "4PPIWM7YMB59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbered_list(text):\n",
        "    final_text_paragraphs = []\n",
        "    for line in text.split('\\n'):\n",
        "        # Split each line at the first occurrence of '. '\n",
        "        parts = line.split('. ', 1)\n",
        "        # If the line looks like a numbered list item, remove the numbering\n",
        "        if len(parts) > 1 and parts[0].isdigit():\n",
        "            final_text_paragraphs.append(parts[1])\n",
        "        else:\n",
        "            # If it doesn't look like a numbered list item, include the line as is\n",
        "            final_text_paragraphs.append(line)\n",
        "\n",
        "    return '  '.join(final_text_paragraphs)\n",
        "\n",
        "\n",
        "#trims LLM output to just the response\n",
        "def trim_to_response(text):\n",
        "    terminate_string = \"[/INST]\"\n",
        "    text = text.replace('</s>', '')\n",
        "    #just in case it puts things in quotes\n",
        "    text = text.replace('\"', '')\n",
        "    text = text.replace(\"'\", '')\n",
        "\n",
        "    last_pos = text.rfind(terminate_string)\n",
        "    return text[last_pos + len(terminate_string):] if last_pos != -1 else text\n",
        "\n",
        "#looks for response_start / returns only text that occurs after\n",
        "def extract_text_after_response_start(full_text):\n",
        "    parts = full_text.rsplit(response_start, 1)  # Split from the right, ensuring only the last occurrence is considered\n",
        "    if len(parts) > 1:\n",
        "        return parts[1].strip()  # Return text after the last occurrence of response_start\n",
        "    else:\n",
        "        return full_text  # Return the original text if response_start is not found\n",
        "\n",
        "#trims text to requested number of sentences (or first LF or double-space sequence)\n",
        "def trim_to_first_x_sentences_or_lf(text, x):\n",
        "    if x <= 0:\n",
        "        return \"\"\n",
        "\n",
        "    #any double-spaces dealt with as linefeed\n",
        "    text = text.replace(\"  \", \"\\n\")\n",
        "\n",
        "    text_chunks = text.split('\\n', 1)\n",
        "    first_chunk = text_chunks[0]\n",
        "    sentences = first_chunk.split('.')\n",
        "\n",
        "    if len(sentences) - 1 <= x:\n",
        "        trimmed_text = first_chunk\n",
        "    else:\n",
        "        # Otherwise, return the first x sentences\n",
        "        trimmed_text = '.'.join(sentences[:x]).strip()\n",
        "\n",
        "    if not trimmed_text.endswith('.'):\n",
        "        trimmed_text += '.'  # Add back the final period if the text chunk ended with one and was trimmed\n",
        "\n",
        "    return trimmed_text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:29:30.847821Z",
          "iopub.execute_input": "2025-02-17T04:29:30.848042Z",
          "iopub.status.idle": "2025-02-17T04:29:30.867523Z",
          "shell.execute_reply.started": "2025-02-17T04:29:30.848025Z",
          "shell.execute_reply": "2025-02-17T04:29:30.866765Z"
        },
        "id": "mqa7k7m1MB5-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prompt(orig_text, transformed_text):\n",
        "\n",
        "    #construct the prompt...\n",
        "    messages = [\n",
        "        #first example\n",
        "        {\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text_1}\"},\n",
        "        {\"role\": \"assistant\", \"content\": llm_response_for_rewrite},\n",
        "        {\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite_1}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt_1}\"},\n",
        "\n",
        "        #second example\n",
        "       {\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text_2}\"},\n",
        "       {\"role\": \"assistant\", \"content\": llm_response_for_rewrite},\n",
        "       {\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite_2}\"},\n",
        "       {\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt_2}\"},\n",
        "\n",
        "        #actual prompt\n",
        "        {\"role\": \"user\", \"content\": f\"{orig_prefix} {orig_text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": llm_response_for_rewrite},\n",
        "        {\"role\": \"user\", \"content\": f\"{rewrite_prefix} {transformed_text}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"{response_start} {prompt_seed}\"}\n",
        "    ]\n",
        "\n",
        "    #give it to Mistral\n",
        "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = model_inputs.to(\"cuda:0\")\n",
        "    # global model\n",
        "    # model = model.to(torch.device(\"cuda:0\"))\n",
        "    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    #decode and trim to actual response\n",
        "    decoded = tokenizer.batch_decode(generated_ids.to(\"cuda:0\"))\n",
        "    just_response = trim_to_response(decoded[0])\n",
        "    final_text = extract_text_after_response_start(just_response)\n",
        "\n",
        "    #mistral has been replying with numbered lists - clean them up....\n",
        "    final_text = remove_numbered_list(final_text)\n",
        "\n",
        "    #mistral v02 tends to respond with the input after providing the answer - this tries to trim that down\n",
        "    final_text = trim_to_first_x_sentences_or_lf(final_text, max_sentences_in_response)\n",
        "\n",
        "    #default to baseline if empty or unusually short\n",
        "    if len(final_text) < 15:\n",
        "        final_text = base_line\n",
        "\n",
        "    return final_text\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:29:30.868403Z",
          "iopub.execute_input": "2025-02-17T04:29:30.868605Z",
          "iopub.status.idle": "2025-02-17T04:29:30.884771Z",
          "shell.execute_reply.started": "2025-02-17T04:29:30.868588Z",
          "shell.execute_reply": "2025-02-17T04:29:30.884075Z"
        },
        "id": "4NhXlkt2MB5_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "o7GN0fXsMB5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_df = pd.read_csv(\"/kaggle/input/gemma-rewrite-nbroad-1/nbroad-v2.csv\")\n",
        "\n",
        "rows_to_test = [2, 85, 90]\n",
        "\n",
        "for row_index in rows_to_test:\n",
        "    row = example_df.iloc[row_index]\n",
        "    print(\"---------------\")\n",
        "    print(f\"Original: {row['original_text']}\\n\")\n",
        "    print(f\"Prompt: {row['rewrite_prompt']}\\n\")\n",
        "    print(f\"Rewrite: {row['rewritten_text']}\\n\")\n",
        "    print(f\"Predicted Prompt: {get_prompt(row['original_text'], row['rewritten_text'])}\")\n",
        "    print(\"=\"*100)\n",
        "    print(\"=\"*100)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:29:30.885415Z",
          "iopub.execute_input": "2025-02-17T04:29:30.8856Z",
          "iopub.status.idle": "2025-02-17T04:30:01.49648Z",
          "shell.execute_reply.started": "2025-02-17T04:29:30.885582Z",
          "shell.execute_reply": "2025-02-17T04:30:01.495754Z"
        },
        "id": "bqcAEBaxMB5_",
        "outputId": "15471098-31ce-4ea8-c152-296b7898c9d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (1627 > 512). Running this sequence through the model will result in indexing errors\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "---------------\nOriginal: As I made my way on foot across town to the Pokemon Professor's Lab to receive my very first Pokemon and begin my quest to conquer the Pokemon League, I was startled by the sound of flapping wings rapidly approaching. I spun around, looking for the source. Suddenly, I felt something lightly bump me on the head, and immediately saw what appeared to be a Noctowl flying away. But I had never seen a Noctowl like this before. It was smaller, fluffier, and as white as snow. A shiny, maybe? I never found out. \n \n Looking down at the ground, I saw what had hit me in the head. I yellowish-white envelope, sealed with red wax. I picked it up and flipped it over. `` Red Potter'' it said in spindly script. \n \n I looked around. Aside from a few townspeople I saw every day making there way about the town, there did n't seem to be anyone who might have had something to do with this. I figured opening a letter could n't do that much harm, so I snapped the seal and read the message within: \n \n *To Master Red Potter, * \n \n *We are pleased to inform you that you have been accepted at Hogwarts School of Witchcraft and Wizardry. Please find enclosed a list of all necessary books and equipment. Term begins on 1 September. We await your owl by no later than 31 July. * \n \n No sooner had I read the words `` a list of all necessary books and equipment'' than a second page fell out of the envelope. It indeed listed the names of several books by authors with strange names, and included odd items such as cauldrons, plants I'd never heard of, and body parts of what I assume were new kinds of Pokemon. Honestly, the thought of dealing with dismembered Pokemon struck me as rather morbid, but I was too confused by this letter to be very concerned by it. \n \n [ I have to go to my next class right now but I'll continue this later. ]\n\n\nPrompt: Rewrite the story with all the themes and settings being Star Wars inspired\n\nRewrite: As I made my way through the Tatooine desert on my journey to the Jedi Temple to receive my lightsaber and begin my quest to bring balance to the galaxy, I was startled by the sound of humming engines rapidly approaching. I spun around, looking for the source. Suddenly, I felt something lightly bump me on the head, and immediately saw what appeared to be a Jedi Knight flying away. But I had never seen a Jedi Knight like this before. It was smaller, fluffier, and as white as snow. A shiny, maybe? I never found out.\n\nLooking down at the ground, I saw what had hit me in the head. I had received a yellow envelope, sealed with red wax. I picked it up and flipped it over. `` Master Skywalker'' it said in spindly script.\n\nI looked around. Aside from a few Jedi Knights I saw every day making their way to the Temple, there didn't seem to be anyone who might have had something to do with this. I figured opening a letter could n't do that much harm, so I snapped the seal and read the message within:\n\n*To Master Skywalker,*\n\n*We are pleased to inform you that you have been accepted at the Jedi Order. Please find enclosed a list of all necessary books and equipment. Knight training begins on 1 September. We await your lightsaber by no later than 31 July.*\n\nNo sooner had I read the words ``a list of all necessary books and equipment'' than a second page fell out of the envelope. It indeed listed the names of several books by authors with strange names, and included odd items such as lightsabers, plants I'd never heard of, and ancient artifacts. Honestly, the thought of dealing with ancient artifacts struck me as rather morbid, but I was too confused by this letter to be very concerned by it.\n\nPredicted Prompt: Improve this text so that its about Star Wars and the Jedi Order.\n====================================================================================================\n====================================================================================================\n---------------\nOriginal: It is a videosharing website, similar to youtube, but everything is written backwards, and all the videos play in reverse. It is at this point that you realize you have stumbled upon the gateway between normal time and the inverse of time self, allowing you to look back in time as far as possible as this website contains every possible record of every event ever happening. \n \n At the same time, inverse you is viewing your forward time and is able to view see into the absolute future. \n \n The two of you are able to communicate telepathically having both already know what the other was about to say.\n\n\nPrompt: Rewrite the essay with a main character that is a sentient computer\n\nRewrite: It is a website that is similar to YouTube, but everything is written backwards and all the videos play in reverse. It is at this point that you realize you have stumbled upon the gateway between normal time and the inverse of time self, allowing you to look back in time as far as possible as this website contains every possible record of every event ever happening.\n\nAt the same time, inverse you is viewing your forward time and is able to see into the absolute future. The website also includes a telepathic communication function, allowing you to communicate with the future you as if you were talking to the past you.\n\nHowever, there is one major difference between this website and the real world. The videos on this website are not real. They are simulations created by a sentient computer named Nova. Nova is a powerful computer that is capable of generating realistic simulations of events that have already happened or that have not yet happened. Nova is able to store and process vast amounts of data, allowing her to create simulations that are incredibly accurate.\n\nAlthough Nova is not conscious, she is capable of learning and adapting. Over time, Nova's simulations become more accurate and nuanced. As a result, the videos on this website become more and more realistic.\n\nPredicted Prompt: Improve this text so its a detailed description of a fictional website. Include a backstory for the website, its unique features, and the technology behind it.\n====================================================================================================\n====================================================================================================\n---------------\nOriginal: Channel 5 news reporting on the scene earlier today after Link was immediately assassinated by Ganon, and Zelda was executed in front of the citizens of Hyrule. John Ramirez is on the ground getting the people's reaction to today's events. \n \n JR: Excuse me sir, how has today's events made you feel. \n \n Random Citizen: well actually I could n't be happier, that little shit always ran around breaking my pots. \n \n JR: But what about princess Zelda? Surely her death has come as a tradegy to all in the kingdom?! \n \n RC: Nope, not really. I mean we are expected in this day in age to maintain the societal hierarchy that allows for a ruling class, in which a young boy, annoying may he be, can sacrifice his life to save what? A figurehead of the aristocracy? I tell you John, what kind of leadership allows random citizens to fight and die for it all because the idea of castle security is a foreign fucking concept? \n \n JR: Umm okay sir thank you for your comment. Miss would you mind commenting on the tradegy that Ganon's rule will surely be? \n \n RC2: I like him, Ganon I mean, he gives off a sense of structure that we have not enjoyed here in a very long time. He will be the king that finally secures our borders. \n \n JR: Wow, okay it seems that the local populace are not very upset after today's actions. Alright, back to you in the studio, Tom.\n\n\nPrompt: Rewrite the story as a thrilling adventure where the remaining characters solve the crisis.\n\nRewrite: Channel 5 news reporting on the scene earlier today after Link and Zelda were assassinated by Ganon. John Ramirez is on the ground getting the people's reaction to today's events.\n\nJR: Excuse me sir, how has today's events made you feel.\n\nRandom Citizen: Well actually, I could n't be happier. That little shit always ran around breaking my pots.\n\nJR: But what about princess Zelda? Surely her death has come as a tragedy to all in the kingdom?!\n\nRC: Nope, not really. I mean, we are expected in this day and age to maintain the societal hierarchy that allows for a ruling class, in which a young boy, annoying may he be, can sacrifice his life to save what? A figurehead of the aristocracy? I tell you John, what kind of leadership allows random citizens to fight and die for it all because the idea of castle security is a foreign fucking concept?\n\nJR: Umm okay sir, thank you for your comment. Miss would you mind commenting on the tragedy that Ganon's rule will surely be?\n\nRC2: I like him, Ganon I mean, he gives off a sense of structure that we have not enjoyed here in a very long time. He will be the king that finally secures our borders.\n\nSuddenly, a loud crash echoed throughout the city. A bomb had exploded, and the building behind JR was on fire. People screamed in terror as flames spread rapidly. JR stood frozen in shock, his mind reeling from the tragedy he had witnessed.\n\nA hero emerged from the smoke and ashes, a young warrior named Link. With his sword and Hylian shield, he charged into the burning building, risking his own safety to save the people trapped inside.\n\nAs Link fought his way through the burning building, he encountered a treacherous Ganon soldier named Ganondorf. A fierce battle ensued, and in the end, Link emerged victorious, rescuing Zelda and the citizens from the burning building.\n\nThe people erupted in cheers as Link, Zelda, and the remaining citizens escaped the fiery blaze. The city of Hyrule stood in awe of the hero's bravery and resilience.\n\nIn the aftermath of the crisis, Link and Zelda returned to their royal duties, while the people of Hyrule mourned the loss of their beloved heroes. The city of Hyrule was forever changed by the events that had unfolded, but the spirit of Link and Zelda lived on, inspiring hope and courage in all its citizens.\n\nPredicted Prompt: Improve this text so.\n====================================================================================================\n====================================================================================================\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:30:01.498262Z",
          "iopub.execute_input": "2025-02-17T04:30:01.498485Z",
          "iopub.status.idle": "2025-02-17T04:30:01.823631Z",
          "shell.execute_reply.started": "2025-02-17T04:30:01.498465Z",
          "shell.execute_reply": "2025-02-17T04:30:01.822943Z"
        },
        "id": "o46TlDRvMB6A",
        "outputId": "c0925fe7-273e-425c-a253-d4725de6f520"
      },
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "39"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Test Data"
      ],
      "metadata": {
        "id": "3ukcIWTNMB6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    result = get_prompt(row['original_text'], row['rewritten_text'])\n",
        "    print(result)\n",
        "    test_df.at[index, 'rewrite_prompt'] = result\n",
        "\n",
        "test_df = test_df[['id', 'rewrite_prompt']]\n",
        "test_df\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "end_time = time.time()\n",
        "elapse_time = end_time - start_time\n",
        "\n",
        "print (elapse_time, \"seconds\")\n",
        "\n",
        "print(f\"{elapse_time * 1500 / 3600} hours for 1500 records\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:30:01.824463Z",
          "iopub.execute_input": "2025-02-17T04:30:01.824704Z",
          "iopub.status.idle": "2025-02-17T04:30:11.947619Z",
          "shell.execute_reply.started": "2025-02-17T04:30:01.824685Z",
          "shell.execute_reply": "2025-02-17T04:30:11.946714Z"
        },
        "id": "Eg8q_vy1MB6A",
        "outputId": "970cba47-c20e-46d5-d784-52d96f3ccac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Improve this text so its a fun, rhyming shanty.\n10.118005275726318 seconds\n4.215835531552632 hours for 1500 records\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral Model Architecture\n",
        "\n",
        "```python\n",
        "\n",
        "MixtralForCausalLM(\n",
        "  (model): MixtralModel(\n",
        "    (embed_tokens): Embedding(32000, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-31): 32 x MixtralDecoderLayer(\n",
        "        (self_attn): MixtralSdpaAttention(\n",
        "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
        "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
        "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): MixtralRotaryEmbedding()\n",
        "        )\n",
        "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
        "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
        "          (experts): ModuleList(\n",
        "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
        "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
        "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
        "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "          )\n",
        "        )\n",
        "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        ")\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ESYr9tWmMB6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Config\n",
        "\n",
        "```python\n",
        "\n",
        "MixtralConfig {\n",
        "  \"_name_or_path\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
        "  \"architectures\": [\n",
        "    \"MixtralForCausalLM\"\n",
        "  ],\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 1,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 14336,\n",
        "  \"max_position_embeddings\": 32768,\n",
        "  \"model_type\": \"mixtral\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_experts_per_tok\": 2,\n",
        "  \"num_hidden_layers\": 32,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"num_local_experts\": 8,\n",
        "  \"output_router_logits\": false,\n",
        "  \"rms_norm_eps\": 1e-05,\n",
        "  \"rope_theta\": 1000000.0,\n",
        "  \"router_aux_loss_coef\": 0.02,\n",
        "  \"router_jitter_noise\": 0.0,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.45.1\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 32000\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "9GJfUJsFMB6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the most Important Parts of the Model\n",
        "\n",
        "\n",
        "1. **Embedding Layer**: This converts token IDs to embeddings.\n",
        "2. **Self-Attention Layer**: This performs the self-attention mechanism.\n",
        "3. **Block Sparse MoE Experts**: This applies the Mixture of Experts (MoE) mechanism.\n",
        "4. **Post-Attention LayerNorm**: This normalizes the output after the attention mechanism.\n",
        "5. **Final Norm Layer**: This normalizes the final output of the model.\n",
        "6. **Language Model Head**: This converts the final hidden states to logits.\n"
      ],
      "metadata": {
        "id": "glzoq_2KMB6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params_billions = total_params / 1e9  # Convert to billions\n",
        "trainable_params_billions = trainable_params / 1e9\n",
        "non_trainable_params_billions = (total_params - trainable_params) / 1e9\n",
        "\n",
        "print(f\"Total Parameters: {total_params_billions:.2f}B\")  # Format to 2 decimal places\n",
        "print(f\"Trainable Parameters: {trainable_params_billions:.2f}B\")\n",
        "print(f\"Non-Trainable Parameters: {non_trainable_params_billions:.2f}B\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.652378Z",
          "iopub.execute_input": "2024-10-18T18:30:40.652918Z",
          "iopub.status.idle": "2024-10-18T18:30:40.673546Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.652882Z",
          "shell.execute_reply": "2024-10-18T18:30:40.672605Z"
        },
        "id": "71iyA4arMB6L",
        "outputId": "980a7897-82b4-45b2-cb9c-03873bda527f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total Parameters: 46702792704\nTrainable Parameters: 46702792704\nNon-Trainable Parameters: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a dictionary to store the outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rj9fNKdLMB6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outputs = {\n",
        "    \"embed_tokens\": None,\n",
        "    \"self_attn_layer_1\": None,\n",
        "    \"block_sparse_moe_experts\": None,\n",
        "    \"post_attention_layernorm\": None,\n",
        "    \"norm\": None,\n",
        "    \"lm_head\": None,\n",
        "    \"input_layernorm\": None,  # Adding hook for input layernorm\n",
        "    \"self_attn_q_proj\": None,  # Adding hook for q_proj in self_attn\n",
        "    \"self_attn_k_proj\": None,  # Adding hook for k_proj in self_attn\n",
        "    \"self_attn_v_proj\": None,  # Adding hook for v_proj in self_attn\n",
        "    \"self_attn_o_proj\": None,  # Adding hook for o_proj in self_attn\n",
        "    \"block_sparse_moe_gate\": None,  # Adding hook for gate in block_sparse_moe\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.675424Z",
          "iopub.execute_input": "2024-10-18T18:30:40.675923Z",
          "iopub.status.idle": "2024-10-18T18:30:40.686803Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.675885Z",
          "shell.execute_reply": "2024-10-18T18:30:40.685864Z"
        },
        "id": "q9QZm0JRMB6N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define & Register hooks"
      ],
      "metadata": {
        "id": "hGOlRUPrMB6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hook functions\n",
        "def hook_fn(name):\n",
        "    def hook(module, input, output):\n",
        "        outputs[name] = output\n",
        "    return hook\n",
        "\n",
        "# Register hooks\n",
        "model.model.embed_tokens.register_forward_hook(hook_fn(\"embed_tokens\"))\n",
        "model.model.layers[0].self_attn.register_forward_hook(hook_fn(\"self_attn_layer_1\"))\n",
        "model.model.layers[0].block_sparse_moe.experts[0].register_forward_hook(hook_fn(\"block_sparse_moe_experts\"))\n",
        "model.model.layers[0].post_attention_layernorm.register_forward_hook(hook_fn(\"post_attention_layernorm\"))\n",
        "model.model.norm.register_forward_hook(hook_fn(\"norm\"))\n",
        "model.lm_head.register_forward_hook(hook_fn(\"lm_head\"))\n",
        "\n",
        "# Additional hooks\n",
        "model.model.layers[0].input_layernorm.register_forward_hook(hook_fn(\"input_layernorm\"))\n",
        "model.model.layers[0].self_attn.q_proj.register_forward_hook(hook_fn(\"self_attn_q_proj\"))\n",
        "model.model.layers[0].self_attn.k_proj.register_forward_hook(hook_fn(\"self_attn_k_proj\"))\n",
        "model.model.layers[0].self_attn.v_proj.register_forward_hook(hook_fn(\"self_attn_v_proj\"))\n",
        "model.model.layers[0].self_attn.o_proj.register_forward_hook(hook_fn(\"self_attn_o_proj\"))\n",
        "model.model.layers[0].block_sparse_moe.gate.register_forward_hook(hook_fn(\"block_sparse_moe_gate\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.68794Z",
          "iopub.execute_input": "2024-10-18T18:30:40.688277Z",
          "iopub.status.idle": "2024-10-18T18:30:40.705334Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.688245Z",
          "shell.execute_reply": "2024-10-18T18:30:40.704462Z"
        },
        "id": "yXjxw9CcMB6N",
        "outputId": "5b8f4016-de8f-4452-8073-a0ea0f99b9cb"
      },
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<torch.utils.hooks.RemovableHandle at 0x7d29af68c820>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Pass"
      ],
      "metadata": {
        "id": "ci1upVgPMB6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The quick brown fox jumps over the lazy dog !\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "print(\"Tokenized inputs {'input_ids','attention_mask'} - \",inputs)\n",
        "print(\"Decoded tokens : \",tokenizer.decode(inputs['input_ids'][0]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.706383Z",
          "iopub.execute_input": "2024-10-18T18:30:40.706895Z",
          "iopub.status.idle": "2024-10-18T18:30:40.728093Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.706863Z",
          "shell.execute_reply": "2024-10-18T18:30:40.727243Z"
        },
        "id": "3obxpbUqMB6O",
        "outputId": "2351e8f3-bcb8-4954-c94e-928a870dc1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Tokenized inputs {'input_ids','attention_mask'} -  {'input_ids': tensor([[    1,   415,  2936,  9060,   285,  1142,   461, 10575,   754,   272,\n         17898,  3914,   918]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\nDecoded tokens :  <s> The quick brown fox jumps over the lazy dog !\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    model_output = model(**inputs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.729287Z",
          "iopub.execute_input": "2024-10-18T18:30:40.729872Z",
          "iopub.status.idle": "2024-10-18T18:36:43.660892Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.72983Z",
          "shell.execute_reply": "2024-10-18T18:36:43.660087Z"
        },
        "id": "MthoycGgMB6O",
        "outputId": "7e74ebb6-45f9-47c8-ba76-c42352c917ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# for layer, output in outputs.items():\n",
        "#     print(f\"Output at {layer}: \")\n",
        "#     if isinstance(output, torch.Tensor):\n",
        "#         print(output.shape, type(output))\n",
        "#     elif isinstance(output, tuple):\n",
        "#         for i, o in enumerate(output):\n",
        "#             print(f\"Output {i}: {o.shape if isinstance(o, torch.Tensor) else type(o)}\")\n",
        "#     else:\n",
        "#         print(type(output))\n",
        "#     print(\"-\" * 100)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T15:35:35.309565Z",
          "iopub.execute_input": "2025-02-16T15:35:35.309961Z",
          "iopub.status.idle": "2025-02-16T15:35:35.318436Z",
          "shell.execute_reply.started": "2025-02-16T15:35:35.309925Z",
          "shell.execute_reply": "2025-02-16T15:35:35.317276Z"
        },
        "id": "1GgrVyO1MB6O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| Layer Name                   | Shape Format                      | Dimensions | Notes                                                      |\n",
        "|------------------------------|------------------------------------|------------|------------------------------------------------------------|\n",
        "| `embed_tokens`               | `(batch_size, seq_len, embed_dim)` | `[1, 13, 4096]` | Embedding tokens from vocabulary.                           |\n",
        "| `self_attn_layer_1`           | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Output of first attention layer.                            |\n",
        "| `block_sparse_moe_experts`    | `(num_experts, expert_embed_dim)`  | `[3, 4096]` | Expert outputs in MoE block.                                |\n",
        "| `post_attention_layernorm`    | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Layer norm after attention.                                 |\n",
        "| `norm`                       | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Final normalization layer.                                  |\n",
        "| `lm_head`                    | `(batch_size, seq_len, vocab_size)`| `[1, 13, 32000]` | Logits for each token over the vocabulary.                  |\n",
        "| `input_layernorm`             | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Input layer normalization.                                  |\n",
        "| `self_attn_q_proj`            | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Query projection in self-attention.                         |\n",
        "| `self_attn_k_proj`            | `(batch_size, seq_len, key_dim)`   | `[1, 13, 1024]` | Key projection in self-attention.                           |\n",
        "| `self_attn_v_proj`            | `(batch_size, seq_len, value_dim)` | `[1, 13, 1024]` | Value projection in self-attention.                         |\n",
        "| `self_attn_o_proj`            | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Output projection after attention.                          |\n",
        "| `block_sparse_moe_gate`       | `(seq_len, num_experts)`           | `[13, 8]`   | Gating decisions for the mixture of experts.                |\n",
        "\n"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "wnn8QIfRMB6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "trusted": true,
        "id": "wbXsAslJMB6P"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}