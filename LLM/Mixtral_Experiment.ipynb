{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 67121,
          "databundleVersionId": 7806901,
          "isSourceIdPinned": false,
          "sourceType": "competition"
        },
        {
          "sourceId": 10765944,
          "sourceType": "datasetVersion",
          "datasetId": 6678406
        },
        {
          "sourceId": 5994,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": false,
          "modelInstanceId": 4761,
          "modelId": 3108
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Mixtral-Experiment",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "XOzFWhMgMamu"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "llm_prompt_recovery_path = kagglehub.competition_download('llm-prompt-recovery')\n",
        "tsr564_gemma_rewrite_nbroad_1_path = kagglehub.dataset_download('tsr564/gemma-rewrite-nbroad-1')\n",
        "mistral_ai_mixtral_pytorch_8x7b_instruct_v0_1_hf_1_path = kagglehub.model_download('mistral-ai/mixtral/PyTorch/8x7b-instruct-v0.1-hf/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "4jtWKlJDMamz"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "rCyzFFavMam2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git  -U\n",
        "!pip install -q git+https://github.com/huggingface/accelerate.git  -U\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q git+https://github.com/huggingface/peft.git  -U"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:19:32.194207Z",
          "iopub.execute_input": "2025-02-17T04:19:32.194508Z",
          "iopub.status.idle": "2025-02-17T04:20:31.070455Z",
          "shell.execute_reply.started": "2025-02-17T04:19:32.194481Z",
          "shell.execute_reply": "2025-02-17T04:20:31.069527Z"
        },
        "id": "diMLm_-LMam5",
        "outputId": "3b228cd0-dde7-4bc9-a17c-bd4264892a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "#https://github.com/Lightning-AI/lit-gpt/issues/327\n",
        "# torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "# torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:20:31.071811Z",
          "iopub.execute_input": "2025-02-17T04:20:31.072139Z",
          "iopub.status.idle": "2025-02-17T04:20:34.662409Z",
          "shell.execute_reply.started": "2025-02-17T04:20:31.072107Z",
          "shell.execute_reply": "2025-02-17T04:20:34.661671Z"
        },
        "id": "PeLCO20_Mam7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#this can help speed up inference\n",
        "max_new_tokens = 30\n",
        "\n",
        "#output test is trimmed according to this\n",
        "max_sentences_in_response = 1"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:20:34.663828Z",
          "iopub.execute_input": "2025-02-17T04:20:34.664286Z",
          "iopub.status.idle": "2025-02-17T04:20:34.667636Z",
          "shell.execute_reply.started": "2025-02-17T04:20:34.664261Z",
          "shell.execute_reply": "2025-02-17T04:20:34.666882Z"
        },
        "id": "UFCjxClCMam7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
        "\n",
        "\n",
        "model_name = '/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    model_max_length=512,  # Reduce maximum sequence length\n",
        ")\n",
        "\n",
        "# Load base model(Mistral 7B)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload= True\n",
        ")\n",
        "\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.gradient_checkpointing = True\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16,\n",
        "        attn_implementation = \"eager\",\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        config=config,\n",
        "        # max_memory={0: \"8GiB\",1:\"13GiB\"},  # Limit GPU memory usage\n",
        "        offload_folder=\"offload\",  # Specify offload directory\n",
        "        offload_state_dict=True  # Enable state dict offloading\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:20:34.668643Z",
          "iopub.execute_input": "2025-02-17T04:20:34.668948Z",
          "iopub.status.idle": "2025-02-17T04:29:30.515623Z",
          "shell.execute_reply.started": "2025-02-17T04:20:34.668925Z",
          "shell.execute_reply": "2025-02-17T04:29:30.514933Z"
        },
        "id": "akd4QNvgMam8",
        "outputId": "8b235f09-a18d-44cb-d2e0-89cf72d86d9c",
        "colab": {
          "referenced_widgets": [
            "f732ccb4e532471db5b71c84ffc24e78"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f732ccb4e532471db5b71c84ffc24e78"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # LoRA config\n",
        "# from peft import LoraConfig,PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "# peft_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "#     target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        "# )\n",
        "# model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Iz9qB7hdMam9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:29:30.516534Z",
          "iopub.execute_input": "2025-02-17T04:29:30.517191Z",
          "iopub.status.idle": "2025-02-17T04:29:30.813513Z",
          "shell.execute_reply.started": "2025-02-17T04:29:30.517157Z",
          "shell.execute_reply": "2025-02-17T04:29:30.812769Z"
        },
        "id": "8N1zXKUlMam9",
        "outputId": "0d781860-0148-4bda-d8b4-61ef539c9235"
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "51"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model"
      ],
      "metadata": {
        "id": "cHy6SvjEMam-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:42:21.707255Z",
          "iopub.execute_input": "2025-02-17T04:42:21.707553Z",
          "iopub.status.idle": "2025-02-17T04:42:21.711466Z",
          "shell.execute_reply.started": "2025-02-17T04:42:21.707529Z",
          "shell.execute_reply": "2025-02-17T04:42:21.710545Z"
        },
        "id": "vq6FGluQMam_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison\"\"\"\n",
        "\n",
        "tokens = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
        "# input_ids = tokens.input_ids.to(\"cuda\")\n",
        "# attention_masks = tokens.attention_mask.to(\"cuda\")\n",
        "print(f\" input_ids : {tokens.input_ids.shape}\")\n",
        "\n",
        "for idx, token_id in enumerate(tokens.input_ids[0]):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(token_id) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )\n",
        "\n",
        "generation_output = model.generate(**tokens,max_new_tokens=100)\n",
        "print(\"generated output(max_tokens + input_ids) : \",generation_output[0].shape) #\n",
        "print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:42:26.489031Z",
          "iopub.execute_input": "2025-02-17T04:42:26.489356Z",
          "iopub.status.idle": "2025-02-17T04:42:54.410771Z",
          "shell.execute_reply.started": "2025-02-17T04:42:26.489332Z",
          "shell.execute_reply": "2025-02-17T04:42:54.409992Z"
        },
        "id": "nM9NWaRtMam_",
        "outputId": "71be41db-0aa5-4305-ada9-ab210d66bd37"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": " input_ids : torch.Size([1, 29])\n\u001b[0;30;48;2;102;194;165m<s>\u001b[0m \u001b[0;30;48;2;252;141;98mQ\u001b[0m \u001b[0;30;48;2;141;160;203mwen\u001b[0m \u001b[0;30;48;2;231;138;195m2\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m5\u001b[0m \u001b[0;30;48;2;102;194;165mvs\u001b[0m \u001b[0;30;48;2;252;141;98mPh\u001b[0m \u001b[0;30;48;2;141;160;203mi\u001b[0m \u001b[0;30;48;2;231;138;195m-\u001b[0m \u001b[0;30;48;2;166;216;84m3\u001b[0m \u001b[0;30;48;2;255;217;47m-\u001b[0m \u001b[0;30;48;2;102;194;165mmin\u001b[0m \u001b[0;30;48;2;252;141;98mi\u001b[0m \u001b[0;30;48;2;141;160;203mwhich\u001b[0m \u001b[0;30;48;2;231;138;195mone\u001b[0m \u001b[0;30;48;2;166;216;84mis\u001b[0m \u001b[0;30;48;2;255;217;47mbetter\u001b[0m \u001b[0;30;48;2;102;194;165mfor\u001b[0m \u001b[0;30;48;2;252;141;98mwhat\u001b[0m \u001b[0;30;48;2;141;160;203mpurpose\u001b[0m \u001b[0;30;48;2;231;138;195m?\u001b[0m \u001b[0;30;48;2;166;216;84mTell\u001b[0m \u001b[0;30;48;2;255;217;47mme\u001b[0m \u001b[0;30;48;2;102;194;165mtechnical\u001b[0m \u001b[0;30;48;2;252;141;98muse\u001b[0m \u001b[0;30;48;2;141;160;203mcase\u001b[0m \u001b[0;30;48;2;231;138;195mcomparison\u001b[0m \u001b[0;30;48;2;166;216;84m</s>\u001b[0m generated output(max_tokens + input_ids) :  torch.Size([129])\n<s> Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison</s>R: Both Qwen2.5 and Phi-3-mini are single-board computers (SBCs) that can be used for various purposes, such as prototyping, IoT projects, and education. Here is a comparison of their technical specifications and use cases:\n\nQwen2.5:\n\n* CPU: Allwinner H3 quad-core Cortex-A7\n* GPU: Mali-400MP2\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward pass output\n",
        "\n",
        "logits & past_key_values will have same shape"
      ],
      "metadata": {
        "id": "ikpkPrZyMam_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**tokens)\n",
        "#\n",
        "print(\"logits shape ([batch_sz,seq_len,vocab_sz]) : \",output.logits.shape)\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(f\"No. of Decoder : {len(output.past_key_values)}\")\n",
        "print(f\"Cached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : {len(output.past_key_values[0])}\")\n",
        "print(f\"Key(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {output.past_key_values[0][0].shape}\")\n",
        "print(f\"Value(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {output.past_key_values[0][1].shape}\")\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(\"Predicted token after the forward pass\")\n",
        "for idx,logits_value in enumerate(output.logits[0]):\n",
        "    token_id = torch.argmax(logits_value,dim=-1)\n",
        "    print(f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +tokenizer.decode(token_id) +'\\x1b[0m',end=' ')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:42:54.41172Z",
          "iopub.execute_input": "2025-02-17T04:42:54.412027Z",
          "iopub.status.idle": "2025-02-17T04:42:55.907184Z",
          "shell.execute_reply.started": "2025-02-17T04:42:54.412003Z",
          "shell.execute_reply": "2025-02-17T04:42:55.906304Z"
        },
        "id": "LbmqBi7eManA",
        "outputId": "2a4ac8e4-a1f1-4e04-82e3-92136464f036"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "logits shape ([batch_sz,seq_len,vocab_sz]) :  torch.Size([1, 29, 32000])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nNo. of Decoder : 32\nCached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : 2\nKey(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\nValue(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nPredicted token after the forward pass\n\u001b[0;30;48;2;102;194;165mQ\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203mch\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m\n\u001b[0m \u001b[0;30;48;2;102;194;165mQ\u001b[0m \u001b[0;30;48;2;252;141;98mp\u001b[0m \u001b[0;30;48;2;141;160;203m2\u001b[0m \u001b[0;30;48;2;231;138;195m1\u001b[0m \u001b[0;30;48;2;166;216;84m\n\u001b[0m \u001b[0;30;48;2;255;217;47m2\u001b[0m \u001b[0;30;48;2;102;194;165mi\u001b[0m \u001b[0;30;48;2;252;141;98m\n\u001b[0m \u001b[0;30;48;2;141;160;203mone\u001b[0m \u001b[0;30;48;2;231;138;195mis\u001b[0m \u001b[0;30;48;2;166;216;84mbetter\u001b[0m \u001b[0;30;48;2;255;217;47m?\u001b[0m \u001b[0;30;48;2;102;194;165mme\u001b[0m \u001b[0;30;48;2;252;141;98m?\u001b[0m \u001b[0;30;48;2;141;160;203m?\u001b[0m \u001b[0;30;48;2;231;138;195m\n\u001b[0m \u001b[0;30;48;2;166;216;84mme\u001b[0m \u001b[0;30;48;2;255;217;47mthe\u001b[0m \u001b[0;30;48;2;102;194;165mdetails\u001b[0m \u001b[0;30;48;2;252;141;98mof\u001b[0m \u001b[0;30;48;2;141;160;203m\n\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84mR\u001b[0m ",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward pass of input through model except the lm_head then passing through lm_head"
      ],
      "metadata": {
        "id": "l2FDOcvNManA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = model.model(tokens.input_ids)\n",
        "\n",
        "print(\"last_hidden_state shape ([batch_sz,seq_len,vocab_sz]) : \",model_output[0].shape)\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "print(f\"No. of Decoder : {len(model_output.past_key_values)}\")\n",
        "print(f\"Cached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : {len(model_output.past_key_values[0])}\")\n",
        "print(f\"Key(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {model_output.past_key_values[0][0].shape}\")\n",
        "print(f\"Value(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : {model_output.past_key_values[0][1].shape}\")\n",
        "############################\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)\n",
        "lm_head_output = model.lm_head(model_output[0])\n",
        "print(\"lm_head/logits shape ([batch_sz,seq_len,vocab_sz]) : \",lm_head_output.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:42:55.908494Z",
          "iopub.execute_input": "2025-02-17T04:42:55.908739Z",
          "iopub.status.idle": "2025-02-17T04:42:57.251084Z",
          "shell.execute_reply.started": "2025-02-17T04:42:55.908718Z",
          "shell.execute_reply": "2025-02-17T04:42:57.250212Z"
        },
        "id": "MwF6uC9TManA",
        "outputId": "a9c36974-74cd-422a-c112-06385b04bcfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "last_hidden_state shape ([batch_sz,seq_len,vocab_sz]) :  torch.Size([1, 29, 4096])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nNo. of Decoder : 32\nCached attention state of Key(K) & Value(V) for each attention head  : layers past_key_values[layer=0] : 2\nKey(K) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\nValue(V) [batch_size, num_attention_heads, seq_len, head_dim (hidden_size / num_attention_heads =  4096 / 32 = 128)] : torch.Size([1, 8, 29, 128])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nlm_head/logits shape ([batch_sz,seq_len,vocab_sz]) :  torch.Size([1, 29, 32000])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Theoretically, both should match but\n",
        "    If False, then small numerical differences exist due to normalization, dropout, or caching.\n",
        "\n",
        "\n",
        "\n",
        "##### **Why Are the Values Different?**\n",
        "1. **Dropout (During Training Mode)**\n",
        "   - If the model is in **training mode (`model.train()`)**, dropout is applied, leading to slightly different hidden states.\n",
        "   - Try running in **evaluation mode**:\n",
        "     ```python\n",
        "     model.eval()\n",
        "     ```\n",
        "\n",
        "2. **LayerNorm / Residual Connection Effects**\n",
        "   - Some architectures (e.g., GPT) apply **Layer Normalization and residual connections differently** inside `forward()`.\n",
        "   - If you extract `model.model(input_ids)`, it **may not be identical** to the forward method.\n",
        "\n",
        "3. **Past Key-Value Caching (for Decoding)**\n",
        "   - When using `model(**tokens)`, **past key-value states** are handled inside the model, which can slightly alter values.\n",
        "   - Try disabling caching:\n",
        "     ```python\n",
        "     output = model(**tokens, use_cache=False)\n",
        "     ```\n"
      ],
      "metadata": {
        "id": "fKh6e7YpManB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual inference"
      ],
      "metadata": {
        "id": "DEWq-zGDManB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "num_tokens_to_generate = 50  # Number of tokens to predict\n",
        "\n",
        "tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "input_ids = tokens.input_ids  # Initial tokenized input\n",
        "\n",
        "for token_idx in range(num_tokens_to_generate):\n",
        "    with torch.no_grad():  # No need to compute gradients during inference\n",
        "        model_output = model.model(input_ids)  # Get hidden states\n",
        "        lm_head_output = model.lm_head(model_output[0]).to(\"cuda:0\")  # Get logits\n",
        "\n",
        "    # Get the predicted token ID (argmax over vocabulary)\n",
        "    next_token_id = lm_head_output[:, -1, :].argmax(dim=-1)\n",
        "    if token_idx==35:\n",
        "        print(f\"Generated Token no : {token_idx}\")\n",
        "        print(f\"lm_head_output [batch_sz,updated_seq_len,vocab_sz] : {lm_head_output.shape}\")\n",
        "        print(f\"lm_head_output[:, -1, :] = [batch_sz,vocab_sz] : { lm_head_output[:, -1, :].shape}\")\n",
        "        print(f\"next_token_id [predicted_seq_len] : {next_token_id.shape}\")\n",
        "        print(f\"next_token_id.unsqueeze(-1) [batch_sz,predicted_seq_len] : {next_token_id.unsqueeze(-1).shape}\")\n",
        "        print(\"-\"*100)\n",
        "        print(\"-\"*100)\n",
        "\n",
        "    # Append predicted token to input_ids\n",
        "    input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "# Decode the generated sequence\n",
        "predicted_text = tokenizer.decode(input_ids[0])\n",
        "\n",
        "print(\"Generated Text:\", predicted_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T03:52:23.230488Z",
          "iopub.execute_input": "2025-02-17T03:52:23.230815Z",
          "iopub.status.idle": "2025-02-17T03:53:46.88443Z",
          "shell.execute_reply.started": "2025-02-17T03:52:23.230791Z",
          "shell.execute_reply": "2025-02-17T03:53:46.883676Z"
        },
        "id": "A9h6W5FcManB",
        "outputId": "6fc2469b-5e8a-4023-ea6c-8d2ac5b4f354"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generated Token no : 35\nlm_head_output [batch_sz,updated_seq_len,vocab_sz] : torch.Size([1, 64, 32000])\nlm_head_output[:, -1, :] = [batch_sz,vocab_sz] : torch.Size([1, 32000])\nnext_token_id [predicted_seq_len] : torch.Size([1])\nnext_token_id.unsqueeze(-1) [batch_sz,predicted_seq_len] : torch.Size([1, 1])\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nGenerated Text: <s> Qwen2.5 vs Phi-3-mini which one is better for what purpose ? Tell me technical usecase comparison</s>R: Both Qwen2.5 and Phi-3-mini are single-board computers (SBCs) that can be used for various purposes, such as prototyping, IoT projects, and education. Here is\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T03:53:54.457081Z",
          "iopub.execute_input": "2025-02-17T03:53:54.457429Z",
          "iopub.status.idle": "2025-02-17T03:53:54.814363Z",
          "shell.execute_reply.started": "2025-02-17T03:53:54.457399Z",
          "shell.execute_reply": "2025-02-17T03:53:54.813587Z"
        },
        "id": "GhmzUJ3vManB",
        "outputId": "99b522d6-54f4-4b2c-f80e-5c856574b82c"
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward pass output\n",
        "\n",
        "##### **Why No `loss.backward()` or `optimizer.step()` in Token Prediction?**  \n",
        "\n",
        "The key reason is that we are **only performing inference (prediction)** and **not training**.  \n",
        "Since we are only **generating tokens**, we don't need loss computation or gradient updates.\n",
        "\n",
        "    The sequence length is 1 less because we shift the logits for next-token prediction alignment.\n",
        "    The first token does not have a previous token, so it’s ignored for loss computation.\n",
        "    This is expected behavior in causal language modeling (like GPT).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vD_GRh0TManB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "model_output = model.model(input_ids)  # Hidden states before lm_head\n",
        "lm_head_output = model.lm_head(model_output[0])  # Logits from the language model head\n",
        "\n",
        "# Shift input_ids and logits for loss calculation\n",
        "shift_logits = lm_head_output[:, :-1, :].contiguous().to(\"cuda:0\") # [batch_sz,seq_len-1,vocab_sz]\n",
        "shift_labels = input_ids[:, 1:].contiguous().to(\"cuda:0\")          # [batch_sz,seq_len-1]\n",
        "\n",
        "# Compute loss\n",
        "# pred :  shift_logits.view(-1, vocab_sz) = [seq_len-1,vocab_sz]\n",
        "# actual : shift_labels.view(-1) = [seq_len-1]\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "# Backward pass(single) - retain_graph=True of you want to run multiple backward pass (it might raise CUDA OutOfMemory)\n",
        "loss.backward(retain_graph=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T03:54:38.516699Z",
          "iopub.execute_input": "2025-02-17T03:54:38.516982Z",
          "iopub.status.idle": "2025-02-17T03:54:42.231049Z",
          "shell.execute_reply.started": "2025-02-17T03:54:38.516962Z",
          "shell.execute_reply": "2025-02-17T03:54:42.230139Z"
        },
        "id": "O1gFRNUQManC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Check gradients\n",
        "print(\"Gradients computed for model parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"layer : {name} --> grad : {param.grad.norm().item()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T03:55:04.650588Z",
          "iopub.execute_input": "2025-02-17T03:55:04.650905Z",
          "iopub.status.idle": "2025-02-17T03:55:04.74844Z",
          "shell.execute_reply.started": "2025-02-17T03:55:04.650882Z",
          "shell.execute_reply": "2025-02-17T03:55:04.747719Z"
        },
        "id": "PIkAJUoaManC",
        "outputId": "17efce31-0e7b-479d-fa69-4cb1968d9cf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Gradients computed for model parameters:\nlayer : model.embed_tokens.weight --> grad : 41.375\nlayer : model.layers.0.input_layernorm.weight --> grad : 0.43701171875\nlayer : model.layers.0.post_attention_layernorm.weight --> grad : 0.2274169921875\nlayer : model.layers.1.input_layernorm.weight --> grad : 0.155029296875\nlayer : model.layers.1.post_attention_layernorm.weight --> grad : 0.1331787109375\nlayer : model.layers.2.input_layernorm.weight --> grad : 0.1600341796875\nlayer : model.layers.2.post_attention_layernorm.weight --> grad : 0.1279296875\nlayer : model.layers.3.input_layernorm.weight --> grad : 0.0419921875\nlayer : model.layers.3.post_attention_layernorm.weight --> grad : 0.09185791015625\nlayer : model.layers.4.input_layernorm.weight --> grad : 0.13671875\nlayer : model.layers.4.post_attention_layernorm.weight --> grad : 0.08013916015625\nlayer : model.layers.5.input_layernorm.weight --> grad : 0.08551025390625\nlayer : model.layers.5.post_attention_layernorm.weight --> grad : 0.057037353515625\nlayer : model.layers.6.input_layernorm.weight --> grad : 0.09747314453125\nlayer : model.layers.6.post_attention_layernorm.weight --> grad : 0.0748291015625\nlayer : model.layers.7.input_layernorm.weight --> grad : 0.074462890625\nlayer : model.layers.7.post_attention_layernorm.weight --> grad : 0.03973388671875\nlayer : model.layers.8.input_layernorm.weight --> grad : 0.0175933837890625\nlayer : model.layers.8.post_attention_layernorm.weight --> grad : 0.1483154296875\nlayer : model.layers.9.input_layernorm.weight --> grad : 0.059814453125\nlayer : model.layers.9.post_attention_layernorm.weight --> grad : 0.076171875\nlayer : model.layers.10.input_layernorm.weight --> grad : 0.1373291015625\nlayer : model.layers.10.post_attention_layernorm.weight --> grad : 0.14501953125\nlayer : model.layers.11.input_layernorm.weight --> grad : 0.0614013671875\nlayer : model.layers.11.post_attention_layernorm.weight --> grad : 0.05352783203125\nlayer : model.layers.12.input_layernorm.weight --> grad : 0.06756591796875\nlayer : model.layers.12.post_attention_layernorm.weight --> grad : 0.0875244140625\nlayer : model.layers.13.input_layernorm.weight --> grad : 0.0909423828125\nlayer : model.layers.13.post_attention_layernorm.weight --> grad : 0.0289764404296875\nlayer : model.layers.14.input_layernorm.weight --> grad : 0.041107177734375\nlayer : model.layers.14.post_attention_layernorm.weight --> grad : 0.030029296875\nlayer : model.layers.15.input_layernorm.weight --> grad : 0.0276031494140625\nlayer : model.layers.15.post_attention_layernorm.weight --> grad : 0.029083251953125\nlayer : model.layers.16.input_layernorm.weight --> grad : 0.053314208984375\nlayer : model.layers.16.post_attention_layernorm.weight --> grad : 0.038482666015625\nlayer : model.layers.17.input_layernorm.weight --> grad : 0.0233154296875\nlayer : model.layers.17.post_attention_layernorm.weight --> grad : 0.028961181640625\nlayer : model.layers.18.input_layernorm.weight --> grad : 0.060516357421875\nlayer : model.layers.18.post_attention_layernorm.weight --> grad : 0.0287017822265625\nlayer : model.layers.19.input_layernorm.weight --> grad : 0.036285400390625\nlayer : model.layers.19.post_attention_layernorm.weight --> grad : 0.031646728515625\nlayer : model.layers.20.input_layernorm.weight --> grad : 0.046051025390625\nlayer : model.layers.20.post_attention_layernorm.weight --> grad : 0.0273590087890625\nlayer : model.layers.21.input_layernorm.weight --> grad : 0.0089263916015625\nlayer : model.layers.21.post_attention_layernorm.weight --> grad : 0.021270751953125\nlayer : model.layers.22.input_layernorm.weight --> grad : 0.0146942138671875\nlayer : model.layers.22.post_attention_layernorm.weight --> grad : 0.025787353515625\nlayer : model.layers.23.input_layernorm.weight --> grad : 0.0184478759765625\nlayer : model.layers.23.post_attention_layernorm.weight --> grad : 0.0123138427734375\nlayer : model.layers.24.input_layernorm.weight --> grad : 0.061737060546875\nlayer : model.layers.24.post_attention_layernorm.weight --> grad : 0.0158233642578125\nlayer : model.layers.25.input_layernorm.weight --> grad : 0.01343536376953125\nlayer : model.layers.25.post_attention_layernorm.weight --> grad : 0.0209503173828125\nlayer : model.layers.26.input_layernorm.weight --> grad : 0.015228271484375\nlayer : model.layers.26.post_attention_layernorm.weight --> grad : 0.0166778564453125\nlayer : model.layers.27.input_layernorm.weight --> grad : 0.022857666015625\nlayer : model.layers.27.post_attention_layernorm.weight --> grad : 0.02191162109375\nlayer : model.layers.28.input_layernorm.weight --> grad : 0.008544921875\nlayer : model.layers.28.post_attention_layernorm.weight --> grad : 0.0174102783203125\nlayer : model.layers.29.input_layernorm.weight --> grad : 0.01131439208984375\nlayer : model.layers.29.post_attention_layernorm.weight --> grad : 0.0224761962890625\nlayer : model.layers.30.input_layernorm.weight --> grad : 0.0098876953125\nlayer : model.layers.30.post_attention_layernorm.weight --> grad : 0.029144287109375\nlayer : model.layers.31.input_layernorm.weight --> grad : 0.037811279296875\nlayer : model.layers.31.post_attention_layernorm.weight --> grad : 0.0615234375\nlayer : model.norm.weight --> grad : 0.1239013671875\nlayer : lm_head.weight --> grad : 26.265625\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T04:43:32.139902Z",
          "iopub.execute_input": "2025-02-17T04:43:32.140221Z",
          "iopub.status.idle": "2025-02-17T04:43:32.453836Z",
          "shell.execute_reply.started": "2025-02-17T04:43:32.140194Z",
          "shell.execute_reply": "2025-02-17T04:43:32.453168Z"
        },
        "id": "CLsIw5ixManD",
        "outputId": "f9eaa123-80fc-4d27-825e-fcbe9d09c3a0"
      },
      "outputs": [
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral Model Architecture\n",
        "\n",
        "```python\n",
        "\n",
        "MixtralForCausalLM(\n",
        "  (model): MixtralModel(\n",
        "    (embed_tokens): Embedding(32000, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-31): 32 x MixtralDecoderLayer(\n",
        "        (self_attn): MixtralSdpaAttention(\n",
        "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
        "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
        "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): MixtralRotaryEmbedding()\n",
        "        )\n",
        "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
        "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
        "          (experts): ModuleList(\n",
        "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
        "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
        "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
        "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "          )\n",
        "        )\n",
        "        (input_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): MixtralRMSNorm((4096,), eps=1e-05)\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        ")\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "W313QZxtManD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Config\n",
        "\n",
        "```python\n",
        "\n",
        "MixtralConfig {\n",
        "  \"_name_or_path\": \"mistralai/Mixtral-8x7B-v0.1\",\n",
        "  \"architectures\": [\n",
        "    \"MixtralForCausalLM\"\n",
        "  ],\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 1,\n",
        "  \"eos_token_id\": 2,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 14336,\n",
        "  \"max_position_embeddings\": 32768,\n",
        "  \"model_type\": \"mixtral\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_experts_per_tok\": 2,\n",
        "  \"num_hidden_layers\": 32,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"num_local_experts\": 8,\n",
        "  \"output_router_logits\": false,\n",
        "  \"rms_norm_eps\": 1e-05,\n",
        "  \"rope_theta\": 1000000.0,\n",
        "  \"router_aux_loss_coef\": 0.02,\n",
        "  \"router_jitter_noise\": 0.0,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.45.1\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 32000\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "u8XeLb1PManD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## These are the most Important Parts of the Model\n",
        "\n",
        "\n",
        "1. **Embedding Layer**: This converts token IDs to embeddings.\n",
        "2. **Self-Attention Layer**: This performs the self-attention mechanism.\n",
        "3. **Block Sparse MoE Experts**: This applies the Mixture of Experts (MoE) mechanism.\n",
        "4. **Post-Attention LayerNorm**: This normalizes the output after the attention mechanism.\n",
        "5. **Final Norm Layer**: This normalizes the final output of the model.\n",
        "6. **Language Model Head**: This converts the final hidden states to logits.\n"
      ],
      "metadata": {
        "id": "rQD7oSBnManD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params_billions = total_params / 1e9  # Convert to billions\n",
        "trainable_params_billions = trainable_params / 1e9\n",
        "non_trainable_params_billions = (total_params - trainable_params) / 1e9\n",
        "\n",
        "print(f\"Total Parameters: {total_params_billions:.2f}B\")  # Format to 2 decimal places\n",
        "print(f\"Trainable Parameters: {trainable_params_billions:.2f}B\")\n",
        "print(f\"Non-Trainable Parameters: {non_trainable_params_billions:.2f}B\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.652378Z",
          "iopub.execute_input": "2024-10-18T18:30:40.652918Z",
          "iopub.status.idle": "2024-10-18T18:30:40.673546Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.652882Z",
          "shell.execute_reply": "2024-10-18T18:30:40.672605Z"
        },
        "id": "GEUC9G7MManE",
        "outputId": "eed5a7b9-6789-4c59-c0d3-70a41b0ee29e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total Parameters: 46702792704\nTrainable Parameters: 46702792704\nNon-Trainable Parameters: 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a dictionary to store the outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0jO11bxRManE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "outputs = {\n",
        "    \"embed_tokens\": None,\n",
        "    \"self_attn_layer_1\": None,\n",
        "    \"block_sparse_moe_experts\": None,\n",
        "    \"post_attention_layernorm\": None,\n",
        "    \"norm\": None,\n",
        "    \"lm_head\": None,\n",
        "    \"input_layernorm\": None,  # Adding hook for input layernorm\n",
        "    \"self_attn_q_proj\": None,  # Adding hook for q_proj in self_attn\n",
        "    \"self_attn_k_proj\": None,  # Adding hook for k_proj in self_attn\n",
        "    \"self_attn_v_proj\": None,  # Adding hook for v_proj in self_attn\n",
        "    \"self_attn_o_proj\": None,  # Adding hook for o_proj in self_attn\n",
        "    \"block_sparse_moe_gate\": None,  # Adding hook for gate in block_sparse_moe\n",
        "}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.675424Z",
          "iopub.execute_input": "2024-10-18T18:30:40.675923Z",
          "iopub.status.idle": "2024-10-18T18:30:40.686803Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.675885Z",
          "shell.execute_reply": "2024-10-18T18:30:40.685864Z"
        },
        "id": "YfUhBmRFManE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define & Register hooks"
      ],
      "metadata": {
        "id": "trlKSiHpManE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hook functions\n",
        "def hook_fn(name):\n",
        "    def hook(module, input, output):\n",
        "        outputs[name] = output\n",
        "    return hook\n",
        "\n",
        "# Register hooks\n",
        "model.model.embed_tokens.register_forward_hook(hook_fn(\"embed_tokens\"))\n",
        "model.model.layers[0].self_attn.register_forward_hook(hook_fn(\"self_attn_layer_1\"))\n",
        "model.model.layers[0].block_sparse_moe.experts[0].register_forward_hook(hook_fn(\"block_sparse_moe_experts\"))\n",
        "model.model.layers[0].post_attention_layernorm.register_forward_hook(hook_fn(\"post_attention_layernorm\"))\n",
        "model.model.norm.register_forward_hook(hook_fn(\"norm\"))\n",
        "model.lm_head.register_forward_hook(hook_fn(\"lm_head\"))\n",
        "\n",
        "# Additional hooks\n",
        "model.model.layers[0].input_layernorm.register_forward_hook(hook_fn(\"input_layernorm\"))\n",
        "model.model.layers[0].self_attn.q_proj.register_forward_hook(hook_fn(\"self_attn_q_proj\"))\n",
        "model.model.layers[0].self_attn.k_proj.register_forward_hook(hook_fn(\"self_attn_k_proj\"))\n",
        "model.model.layers[0].self_attn.v_proj.register_forward_hook(hook_fn(\"self_attn_v_proj\"))\n",
        "model.model.layers[0].self_attn.o_proj.register_forward_hook(hook_fn(\"self_attn_o_proj\"))\n",
        "model.model.layers[0].block_sparse_moe.gate.register_forward_hook(hook_fn(\"block_sparse_moe_gate\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.68794Z",
          "iopub.execute_input": "2024-10-18T18:30:40.688277Z",
          "iopub.status.idle": "2024-10-18T18:30:40.705334Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.688245Z",
          "shell.execute_reply": "2024-10-18T18:30:40.704462Z"
        },
        "id": "rIY2nlK8ManF",
        "outputId": "14af8f20-03c3-4fa0-868e-24a1ad210094"
      },
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<torch.utils.hooks.RemovableHandle at 0x7d29af68c820>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Pass"
      ],
      "metadata": {
        "id": "Xu55NeQcManF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The quick brown fox jumps over the lazy dog !\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "print(\"Tokenized inputs {'input_ids','attention_mask'} - \",inputs)\n",
        "print(\"Decoded tokens : \",tokenizer.decode(inputs['input_ids'][0]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.706383Z",
          "iopub.execute_input": "2024-10-18T18:30:40.706895Z",
          "iopub.status.idle": "2024-10-18T18:30:40.728093Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.706863Z",
          "shell.execute_reply": "2024-10-18T18:30:40.727243Z"
        },
        "id": "rNUgWiWGManF",
        "outputId": "85fa272b-a68a-4db6-b76f-6b55348dc61f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Tokenized inputs {'input_ids','attention_mask'} -  {'input_ids': tensor([[    1,   415,  2936,  9060,   285,  1142,   461, 10575,   754,   272,\n         17898,  3914,   918]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\nDecoded tokens :  <s> The quick brown fox jumps over the lazy dog !\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    model_output = model(**inputs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-18T18:30:40.729287Z",
          "iopub.execute_input": "2024-10-18T18:30:40.729872Z",
          "iopub.status.idle": "2024-10-18T18:36:43.660892Z",
          "shell.execute_reply.started": "2024-10-18T18:30:40.72983Z",
          "shell.execute_reply": "2024-10-18T18:36:43.660087Z"
        },
        "id": "g1g5U-ZpManG",
        "outputId": "e9b9355e-32ca-41b8-804b-fb21ec483ede"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# for layer, output in outputs.items():\n",
        "#     print(f\"Output at {layer}: \")\n",
        "#     if isinstance(output, torch.Tensor):\n",
        "#         print(output.shape, type(output))\n",
        "#     elif isinstance(output, tuple):\n",
        "#         for i, o in enumerate(output):\n",
        "#             print(f\"Output {i}: {o.shape if isinstance(o, torch.Tensor) else type(o)}\")\n",
        "#     else:\n",
        "#         print(type(output))\n",
        "#     print(\"-\" * 100)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T15:35:35.309565Z",
          "iopub.execute_input": "2025-02-16T15:35:35.309961Z",
          "iopub.status.idle": "2025-02-16T15:35:35.318436Z",
          "shell.execute_reply.started": "2025-02-16T15:35:35.309925Z",
          "shell.execute_reply": "2025-02-16T15:35:35.317276Z"
        },
        "id": "XUJB6Y-FManG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| Layer Name                   | Shape Format                      | Dimensions | Notes                                                      |\n",
        "|------------------------------|------------------------------------|------------|------------------------------------------------------------|\n",
        "| `embed_tokens`               | `(batch_size, seq_len, embed_dim)` | `[1, 13, 4096]` | Embedding tokens from vocabulary.                           |\n",
        "| `self_attn_layer_1`           | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Output of first attention layer.                            |\n",
        "| `block_sparse_moe_experts`    | `(num_experts, expert_embed_dim)`  | `[3, 4096]` | Expert outputs in MoE block.                                |\n",
        "| `post_attention_layernorm`    | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Layer norm after attention.                                 |\n",
        "| `norm`                       | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Final normalization layer.                                  |\n",
        "| `lm_head`                    | `(batch_size, seq_len, vocab_size)`| `[1, 13, 32000]` | Logits for each token over the vocabulary.                  |\n",
        "| `input_layernorm`             | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Input layer normalization.                                  |\n",
        "| `self_attn_q_proj`            | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Query projection in self-attention.                         |\n",
        "| `self_attn_k_proj`            | `(batch_size, seq_len, key_dim)`   | `[1, 13, 1024]` | Key projection in self-attention.                           |\n",
        "| `self_attn_v_proj`            | `(batch_size, seq_len, value_dim)` | `[1, 13, 1024]` | Value projection in self-attention.                         |\n",
        "| `self_attn_o_proj`            | `(batch_size, seq_len, hidden_dim)`| `[1, 13, 4096]` | Output projection after attention.                          |\n",
        "| `block_sparse_moe_gate`       | `(seq_len, num_experts)`           | `[13, 8]`   | Gating decisions for the mixture of experts.                |\n",
        "\n"
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "1kWfdgT-ManG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "trusted": true,
        "id": "7FiIw6h5ManH"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}