{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30747,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "LLM-Notes-3",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SGwgY4wKwTSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T09:21:59.316775Z",
          "iopub.execute_input": "2024-08-11T09:21:59.31713Z",
          "iopub.status.idle": "2024-08-11T09:21:59.321263Z",
          "shell.execute_reply.started": "2024-08-11T09:21:59.317101Z",
          "shell.execute_reply": "2024-08-11T09:21:59.320441Z"
        },
        "trusted": true,
        "id": "64DlLAKxwTSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-17T14:03:48.898172Z",
          "iopub.execute_input": "2024-08-17T14:03:48.898569Z",
          "iopub.status.idle": "2024-08-17T14:03:48.925351Z",
          "shell.execute_reply.started": "2024-08-17T14:03:48.898537Z",
          "shell.execute_reply": "2024-08-17T14:03:48.924527Z"
        },
        "trusted": true,
        "id": "3lm8azndwTSt",
        "outputId": "5619f295-dd4c-4061-e938-f87a8f556784",
        "colab": {
          "referenced_widgets": [
            "7789db0806d04f1e86a2daee0b2215b2"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7789db0806d04f1e86a2daee0b2215b2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vLLM\n",
        "\n",
        "https://docs.vllm.ai/en/latest/getting_started/installation.html"
      ],
      "metadata": {
        "id": "0u3DdU_LwTSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q vllm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-17T13:53:29.201324Z",
          "iopub.execute_input": "2024-08-17T13:53:29.202039Z",
          "iopub.status.idle": "2024-08-17T13:56:29.113174Z",
          "shell.execute_reply.started": "2024-08-17T13:53:29.202005Z",
          "shell.execute_reply": "2024-08-17T13:56:29.112264Z"
        },
        "trusted": true,
        "id": "bJ6bdtJDwTSv",
        "outputId": "ce243f43-d14a-4fdf-e3eb-d607a39a6ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\nfastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from vllm import LLM, SamplingParams\n",
        "\n",
        "# prompts = [\n",
        "#     \"The future of AI is\",\n",
        "# ]\n",
        "# sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "# llm = LLM(\n",
        "#     model=\"facebook/opt-6.7b\",\n",
        "#     tensor_parallel_size=1,\n",
        "#     speculative_model=\"facebook/opt-125m\",\n",
        "#     num_speculative_tokens=5,\n",
        "#     use_v2_block_manager=True,\n",
        "#     gpu_memory_utilization\n",
        "# )\n",
        "# outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "# for output in outputs:\n",
        "#     prompt = output.prompt\n",
        "#     generated_text = output.outputs[0].text\n",
        "#     print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-17T13:58:55.186996Z",
          "iopub.execute_input": "2024-08-17T13:58:55.187369Z",
          "iopub.status.idle": "2024-08-17T13:58:55.192285Z",
          "shell.execute_reply.started": "2024-08-17T13:58:55.187337Z",
          "shell.execute_reply": "2024-08-17T13:58:55.191337Z"
        },
        "trusted": true,
        "id": "5hBgBsrQwTSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PTQ using Dynamic Quantization"
      ],
      "metadata": {
        "id": "EPRMngEXwTSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class EnhancedRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_rnn_layers=2, num_heads=4, bidirectional_rnn_layers=1):\n",
        "        super(EnhancedRNN, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # RNN layer with multiple layers and bidirectional configuration\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=num_rnn_layers,\n",
        "                          batch_first=True, dropout=0.5,\n",
        "                          bidirectional=bidirectional_rnn_layers > 0)\n",
        "\n",
        "        # Multi-Headed Attention\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim * (2 if bidirectional_rnn_layers > 0 else 1),\n",
        "                                               num_heads=num_heads,\n",
        "                                               batch_first=True)\n",
        "\n",
        "        # Layer Normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_dim * (2 if bidirectional_rnn_layers > 0 else 1))\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_dim * (2 if bidirectional_rnn_layers > 0 else 1))\n",
        "\n",
        "        # Additional FC layers\n",
        "        self.fc1 = nn.Linear(hidden_dim * (2 if bidirectional_rnn_layers > 0 else 1), hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        x = self.embedding(x)  # x shape: [batch_size, seq_len, embed_dim]\n",
        "\n",
        "        # RNN\n",
        "        x, _ = self.rnn(x)  # x shape: [batch_size, seq_len, hidden_dim * num_directions]\n",
        "\n",
        "        # Multi-Headed Attention\n",
        "        x_attn, _ = self.attention(x, x, x)  # Attention needs queries, keys, and values (all x here)\n",
        "        x = x + x_attn  # Skip connection\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        # Further Processing\n",
        "        x = x[:, -1, :]  # Use the output from the last sequence step, shape: [batch_size, hidden_dim * num_directions]\n",
        "\n",
        "        # Fully Connected layers\n",
        "        x = self.fc1(x)  # Shape: [batch_size, hidden_dim]\n",
        "        x = self.fc2(x)  # Shape: [batch_size, output_dim]\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "vocab_size = 10000\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = 10\n",
        "num_rnn_layers = 2\n",
        "num_heads = 4\n",
        "bidirectional_rnn_layers = 1  # Set to 1 to enable bi-directional RNN layers\n",
        "\n",
        "model = EnhancedRNN(vocab_size, embed_dim, hidden_dim, output_dim, num_rnn_layers, num_heads, bidirectional_rnn_layers)\n",
        "input_ids = torch.randint(0, vocab_size, (32, 50))  # Example input with batch_size=32 and seq_len=50\n",
        "output = model(input_ids)\n",
        "print(output.shape)  # Should print: torch.Size([32, 10])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T09:22:50.882669Z",
          "iopub.execute_input": "2024-08-11T09:22:50.883415Z",
          "iopub.status.idle": "2024-08-11T09:22:51.305686Z",
          "shell.execute_reply.started": "2024-08-11T09:22:50.88338Z",
          "shell.execute_reply": "2024-08-11T09:22:51.304821Z"
        },
        "trusted": true,
        "id": "SvZfsuD3wTSy",
        "outputId": "9d8d0aab-be82-4a28-e6d7-e90259e1f1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([32, 10])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T09:19:50.525728Z",
          "iopub.execute_input": "2024-08-11T09:19:50.525999Z",
          "iopub.status.idle": "2024-08-11T09:20:05.777495Z",
          "shell.execute_reply.started": "2024-08-11T09:19:50.525974Z",
          "shell.execute_reply": "2024-08-11T09:20:05.776483Z"
        },
        "trusted": true,
        "id": "MZa5LYAGwTSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "# Define the dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {key: torch.squeeze(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# Load IMDB dataset\n",
        "dataset = load_dataset('imdb')\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Determine vocab size\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "# Parameters\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = 2  # Binary classification\n",
        "num_rnn_layers = 2\n",
        "num_heads = 4\n",
        "bidirectional_rnn_layers = 1\n",
        "max_length = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Create datasets\n",
        "train_texts = dataset['train']['text']\n",
        "train_labels = dataset['train']['label']\n",
        "val_texts = dataset['test']['text'][:5000]  # Using a subset for validation\n",
        "val_labels = dataset['test']['label'][:5000]\n",
        "test_texts = dataset['test']['text'][5000:]  # Remaining for testing\n",
        "test_labels = dataset['test']['label'][5000:]\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "test_dataset = CustomDataset(test_texts, test_labels, tokenizer, max_length)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    print(batch)\n",
        "    break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T09:22:26.241055Z",
          "iopub.execute_input": "2024-08-11T09:22:26.241528Z",
          "iopub.status.idle": "2024-08-11T09:22:31.683175Z",
          "shell.execute_reply.started": "2024-08-11T09:22:26.241499Z",
          "shell.execute_reply": "2024-08-11T09:22:31.682294Z"
        },
        "trusted": true,
        "id": "4di1vZlBwTS1",
        "outputId": "86b82818-5502-449e-b964-5353a4a5d712",
        "colab": {
          "referenced_widgets": [
            "4f61d273ab9d48a99c635663bd0a2ec6",
            "9a8c6c3d7f184071930d12fa69ebb416",
            "34011075a2964fb4929230a1359e20a9",
            "b79e22b157344317a1b6963606386256",
            "8ffee4e38ed84067a9a1729dcdfcf999",
            "6bfa56b3cf6a498e98f2678d93b40a0f",
            "19fa6c26b88f4bf4a378dfc9c4a9c276",
            "2d8b56859e2e4305adcf75a5d39894b9",
            "cc74873efacb4e93a9b65cc7bf0653e6",
            "0904023b34444825a30be28c830d9cd1"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f61d273ab9d48a99c635663bd0a2ec6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a8c6c3d7f184071930d12fa69ebb416"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34011075a2964fb4929230a1359e20a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b79e22b157344317a1b6963606386256"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ffee4e38ed84067a9a1729dcdfcf999"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bfa56b3cf6a498e98f2678d93b40a0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19fa6c26b88f4bf4a378dfc9c4a9c276"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d8b56859e2e4305adcf75a5d39894b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc74873efacb4e93a9b65cc7bf0653e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0904023b34444825a30be28c830d9cd1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "{'input_ids': tensor([[  101,  1014,  1012,  ..., 10439,  8095,   102],\n        [  101,  1045,  4669,  ...,     0,     0,     0],\n        [  101,  1045,  3427,  ...,  3185,  1010,   102],\n        ...,\n        [  101,  2241,  2006,  ...,  2000,  2053,   102],\n        [  101,  2026,  2564,  ...,     0,     0,     0],\n        [  101,  2009,  1005,  ...,  1997,  1996,   102]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n        0, 0, 0, 1, 0, 1, 1, 0])}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EnhancedRNN(vocab_size, embed_dim, hidden_dim, output_dim, num_rnn_layers, num_heads, bidirectional_rnn_layers).to(device)\n",
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T09:22:57.647169Z",
          "iopub.execute_input": "2024-08-11T09:22:57.647851Z",
          "iopub.status.idle": "2024-08-11T09:22:58.109238Z",
          "shell.execute_reply.started": "2024-08-11T09:22:57.647819Z",
          "shell.execute_reply": "2024-08-11T09:22:58.108328Z"
        },
        "trusted": true,
        "id": "-oH_N31lwTS2",
        "outputId": "13c18676-73dc-45d3-e348-01fd3540ce3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "EnhancedRNN(\n  (embedding): Embedding(10000, 128)\n  (rnn): RNN(128, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n  (attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (fc1): Linear(in_features=512, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=10, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "def train(train_loader, net, device, epochs=5, total_iterations_limit=None):\n",
        "    cross_el = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    total_iterations = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "\n",
        "        loss_sum = 0\n",
        "        num_iterations = 0\n",
        "\n",
        "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
        "        if total_iterations_limit is not None:\n",
        "            data_iterator.total = total_iterations_limit\n",
        "\n",
        "        for batch in data_iterator:\n",
        "            num_iterations += 1\n",
        "            total_iterations += 1\n",
        "\n",
        "            # Extract inputs and labels from batch\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = cross_el(outputs, labels)\n",
        "            loss_sum += loss.item()\n",
        "            avg_loss = loss_sum / num_iterations\n",
        "            data_iterator.set_postfix(loss=avg_loss)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
        "                return\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
        "    print('Size (KB):', os.path.getsize(\"temp_delme.p\") / 1e3)\n",
        "    os.remove('temp_delme.p')\n",
        "\n",
        "def test(model: nn.Module, test_loader, device: torch.device, total_iterations: int = None):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    iterations = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc='Testing'):\n",
        "            # Extract inputs and labels from batch\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Perform forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate the predicted class\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Update the number of correct predictions and total predictions\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            iterations += 1\n",
        "            if total_iterations is not None and iterations >= total_iterations:\n",
        "                break\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(f'Accuracy: {round(accuracy, 3)}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T09:23:02.775601Z",
          "iopub.execute_input": "2024-08-11T09:23:02.776229Z",
          "iopub.status.idle": "2024-08-11T09:23:02.790339Z",
          "shell.execute_reply.started": "2024-08-11T09:23:02.776199Z",
          "shell.execute_reply": "2024-08-11T09:23:02.7893Z"
        },
        "trusted": true,
        "id": "mXsjt0oIwTS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataloader, model, device=device, epochs=5)\n",
        "print_size_of_model(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T06:44:42.212497Z",
          "iopub.execute_input": "2024-08-11T06:44:42.212866Z",
          "iopub.status.idle": "2024-08-11T06:48:30.263403Z",
          "shell.execute_reply.started": "2024-08-11T06:44:42.212836Z",
          "shell.execute_reply": "2024-08-11T06:48:30.262576Z"
        },
        "trusted": true,
        "id": "pPYNMjjkwTS4",
        "outputId": "a6cc2442-3d83-4430-8073-ef997059f995"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1: 100%|██████████| 782/782 [00:45<00:00, 17.14it/s, loss=0.698]\nEpoch 2: 100%|██████████| 782/782 [00:45<00:00, 17.13it/s, loss=0.694]\nEpoch 3: 100%|██████████| 782/782 [00:45<00:00, 17.06it/s, loss=0.672]\nEpoch 4: 100%|██████████| 782/782 [00:45<00:00, 17.21it/s, loss=0.594]\nEpoch 5: 100%|██████████| 782/782 [00:45<00:00, 17.21it/s, loss=0.532]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Size (KB): 22739.362\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_dataloader,device=device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T06:49:06.1255Z",
          "iopub.execute_input": "2024-08-11T06:49:06.126162Z",
          "iopub.status.idle": "2024-08-11T06:49:41.73928Z",
          "shell.execute_reply.started": "2024-08-11T06:49:06.126129Z",
          "shell.execute_reply": "2024-08-11T06:49:41.738398Z"
        },
        "trusted": true,
        "id": "5CpEfvzxwTS5",
        "outputId": "980e0069-b78e-441f-b3d5-a62f151ed5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Testing: 100%|██████████| 625/625 [00:35<00:00, 17.55it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Accuracy: 0.763\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save and load a PyTorch model, follow these steps:\n",
        "\n",
        "### Saving the Model\n",
        "\n",
        "1. **Save the Entire Model**:\n",
        "   ```python\n",
        "   torch.save(model, 'model.pth')\n",
        "   ```\n",
        "\n",
        "2. **Save Only the Model State Dict**:\n",
        "   ```python\n",
        "   torch.save(model.state_dict(), 'model_state_dict.pth')\n",
        "   ```\n",
        "\n",
        "### Loading the Model\n",
        "\n",
        "1. **Load the Entire Model**:\n",
        "   ```python\n",
        "   model = torch.load('model.pth')\n",
        "   model.eval()  # Set the model to evaluation mode\n",
        "   ```\n",
        "\n",
        "2. **Load the Model State Dict**:\n",
        "   ```python\n",
        "   model = EnhancedRNN(...)  # Initialize the model architecture\n",
        "   model.load_state_dict(torch.load('model_state_dict.pth'))\n",
        "   model.eval()  # Set the model to evaluation mode\n",
        "   ```\n",
        "\n",
        "### Notes:\n",
        "- **For Quantized Models**: Ensure to apply the same quantization configuration when reloading.\n",
        "- **Model Architecture**: When loading the state dict, make sure the model architecture matches the saved state dict."
      ],
      "metadata": {
        "id": "raBqwOSfwTS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_state_dict.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T06:49:48.894313Z",
          "iopub.execute_input": "2024-08-11T06:49:48.89467Z",
          "iopub.status.idle": "2024-08-11T06:49:48.934383Z",
          "shell.execute_reply.started": "2024-08-11T06:49:48.89464Z",
          "shell.execute_reply": "2024-08-11T06:49:48.93348Z"
        },
        "trusted": true,
        "id": "BlBnC4BbwTS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved model states\n",
        "q_model =  EnhancedRNN(vocab_size, embed_dim, hidden_dim, output_dim, num_rnn_layers, num_heads, bidirectional_rnn_layers).to(device)\n",
        "\n",
        "q_model.load_state_dict(torch.load('/kaggle/working/model_state_dict.pth'))\n",
        "\n",
        "device = torch.device('cpu')\n",
        "q_model.eval().to(device)\n",
        "q_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T06:50:20.248348Z",
          "iopub.execute_input": "2024-08-11T06:50:20.249283Z",
          "iopub.status.idle": "2024-08-11T06:50:20.345961Z",
          "shell.execute_reply.started": "2024-08-11T06:50:20.249239Z",
          "shell.execute_reply": "2024-08-11T06:50:20.344964Z"
        },
        "trusted": true,
        "id": "b3RTw6mewTS8",
        "outputId": "04a99b8b-15c4-444d-c11f-0b16b18cc37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "EnhancedRNN(\n  (embedding): Embedding(30522, 128)\n  (rnn): RNN(128, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n  (attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (fc1): Linear(in_features=512, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=2, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Post-training-quantization (PTQ)"
      ],
      "metadata": {
        "id": "I8O7xfOvwTS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    q_model, {nn.RNN,nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print_size_of_model(quantized_model)\n",
        "quantized_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T06:50:27.578122Z",
          "iopub.execute_input": "2024-08-11T06:50:27.578509Z",
          "iopub.status.idle": "2024-08-11T06:50:27.636939Z",
          "shell.execute_reply.started": "2024-08-11T06:50:27.578433Z",
          "shell.execute_reply": "2024-08-11T06:50:27.636053Z"
        },
        "trusted": true,
        "id": "KXTO4DW4wTS9",
        "outputId": "6e91c41a-39a9-4928-b154-18b1e36163b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Size (KB): 22348.642\n",
          "output_type": "stream"
        },
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "EnhancedRNN(\n  (embedding): Embedding(30522, 128)\n  (rnn): RNN(128, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n  (attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n  )\n  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (fc1): DynamicQuantizedLinear(in_features=512, out_features=256, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n  (fc2): DynamicQuantizedLinear(in_features=256, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test(quantized_model, test_dataloader,device=device)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T06:50:46.873718Z",
          "iopub.execute_input": "2024-08-11T06:50:46.874424Z",
          "iopub.status.idle": "2024-08-11T06:53:14.022014Z",
          "shell.execute_reply.started": "2024-08-11T06:50:46.874392Z",
          "shell.execute_reply": "2024-08-11T06:53:14.021119Z"
        },
        "trusted": true,
        "id": "u2wB00zPwTS-",
        "outputId": "91fe81fe-1eaa-419f-aa8a-63aad15a6404"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Testing: 100%|██████████| 625/625 [02:27<00:00,  4.25it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Accuracy: 0.763\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWQ - Quantization aware training\n",
        "\n",
        "\n",
        "https://github.com/leimao/PyTorch-Quantization-Aware-Training?tab=readme-ov-file"
      ],
      "metadata": {
        "id": "IsLpwRtawTS-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T09:44:08.996747Z",
          "iopub.execute_input": "2024-08-11T09:44:08.997123Z",
          "iopub.status.idle": "2024-08-11T09:44:09.007805Z",
          "shell.execute_reply.started": "2024-08-11T09:44:08.997095Z",
          "shell.execute_reply": "2024-08-11T09:44:09.006906Z"
        },
        "trusted": true,
        "id": "GCwna9r_wTS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-11T09:44:13.756993Z",
          "iopub.execute_input": "2024-08-11T09:44:13.757674Z",
          "iopub.status.idle": "2024-08-11T09:44:13.766113Z",
          "shell.execute_reply.started": "2024-08-11T09:44:13.75763Z",
          "shell.execute_reply": "2024-08-11T09:44:13.765337Z"
        },
        "trusted": true,
        "id": "VIqJTtU0wTS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-b_gq2owTS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PTQ : Encoder based model - BERT"
      ],
      "metadata": {
        "id": "wrjqS8uHwTS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "id": "aaDnyAMwwTS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Why did the scarecrow become a successful neurosurgeon?\"\n",
        "# Encode text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "# Perform inference before quantization\n",
        "with torch.no_grad():\n",
        "    original_output = model(**inputs)\n",
        "\n",
        "\n",
        "\n",
        "# Apply dynamic quantization\n",
        "quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# Perform inference with quantized model\n",
        "with torch.no_grad():\n",
        "    quantized_output = quantized_model(**inputs)\n",
        "\n",
        "\n",
        "# Compare the outputs (optional, for demonstration)\n",
        "print(\"Original output:\", original_output.logits)\n",
        "print(\"Quantized output:\", quantized_output.logits)\n",
        "# Save the quantized model using PyTorch\n",
        "torch.save(quantized_model.state_dict(), \"./quantized_bert_model.pth\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "voI6Sb6nwTS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the quantized model\n",
        "model_loaded = BertForSequenceClassification.from_pretrained(model_name)  # Load the original configuration\n",
        "model_loaded = quantize_dynamic(model_loaded, {torch.nn.Linear}, dtype=torch.qint8)  # Re-apply quantization\n",
        "model_loaded.load_state_dict(torch.load(\"./quantized_bert_model.pth\"))\n",
        "\n",
        "# Perform inference with the loaded quantized model\n",
        "model_loaded.eval()\n",
        "with torch.no_grad():\n",
        "    output_loaded = model_loaded(**inputs)\n",
        "\n",
        "print(\"Output from loaded quantized model:\", output_loaded.logits)\n",
        "\n",
        "predicted_class_indices = torch.argmax(output_loaded.logits, dim=-1)\n",
        "class_labels = [\"negative\", \"positive\"]  # Replace with your actual labels\n",
        "predicted_labels = [class_labels[idx.item()] for idx in predicted_class_indices]\n",
        "\n",
        "print(\"Predicted class indices:\", predicted_class_indices)\n",
        "print(\"Predicted labels:\", predicted_labels)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cBSuczCJwTS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PTQ : Decoder based model - GPT2"
      ],
      "metadata": {
        "id": "wI3ytOfnwTTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import time\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()"
      ],
      "metadata": {
        "trusted": true,
        "id": "sNFZoA2pwTTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Sample input text\n",
        "input_text = \"Why did the scarecrow become a successful neurosurgeon?\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "# Measure performance before quantization\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    original_outputs = model.generate(input_ids, max_length=50)\n",
        "original_duration = time.time() - start_time\n",
        "original_text = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
        "original_text"
      ],
      "metadata": {
        "trusted": true,
        "id": "JLqeuv_EwTTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "quantized_model.eval()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7jrxsaXDwTTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Measure performance after quantization\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    quantized_outputs = quantized_model.generate(input_ids, max_length=50)\n",
        "quantized_duration = time.time() - start_time\n",
        "quantized_text = tokenizer.decode(quantized_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Output results\n",
        "print(\"Original Text:\", original_text)\n",
        "print(\"Quantized Text:\", quantized_text)\n",
        "print(f\"Time taken (original): {original_duration:.3f} seconds\")\n",
        "print(f\"Time taken (quantized): {quantized_duration:.3f} seconds\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "niBKRBh9wTTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Mistal-7B AWQ-4bit model\n",
        "\n",
        "`"
      ],
      "metadata": {
        "id": "dviq-dHcwTTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General model architecture of Mistral-7B-v3\n",
        "\n",
        "```python\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MistralRotaryEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralRotaryEmbedding, self).__init__()\n",
        "        # Initialize as needed\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply rotary embedding\n",
        "        return x\n",
        "\n",
        "class MistralRMSNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(MistralRMSNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "class MistralSdpaAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralSdpaAttention, self).__init__()\n",
        "        self.q_proj = nn.Linear(4096, 4096, bias=False)\n",
        "        self.k_proj = nn.Linear(4096, 1024, bias=False)\n",
        "        self.v_proj = nn.Linear(4096, 1024, bias=False)\n",
        "        self.o_proj = nn.Linear(4096, 4096, bias=False)\n",
        "        self.rotary_emb = MistralRotaryEmbedding()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply rotary embedding and attention mechanism\n",
        "        return x\n",
        "\n",
        "class MistralMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralMLP, self).__init__()\n",
        "        self.gate_proj = nn.Linear(4096, 14336, bias=False)\n",
        "        self.up_proj = nn.Linear(4096, 14336, bias=False)\n",
        "        self.down_proj = nn.Linear(14336, 4096, bias=False)\n",
        "        self.act_fn = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply gate projection, activation, and down projection\n",
        "        return x\n",
        "\n",
        "class MistralDecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralDecoderLayer, self).__init__()\n",
        "        self.self_attn = MistralSdpaAttention()\n",
        "        self.mlp = MistralMLP()\n",
        "        self.input_layernorm = MistralRMSNorm(4096)\n",
        "        self.post_attention_layernorm = MistralRMSNorm(4096)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply attention and MLP with layer normalization\n",
        "        return x\n",
        "\n",
        "class MistralModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralModel, self).__init__()\n",
        "        self.embed_tokens = nn.Embedding(32768, 4096)\n",
        "        self.layers = nn.ModuleList([MistralDecoderLayer() for _ in range(32)])\n",
        "        self.norm = MistralRMSNorm(4096)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed_tokens(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class MistralForCausalLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralForCausalLM, self).__init__()\n",
        "        self.model = MistralModel()\n",
        "        self.lm_head = nn.Linear(4096, 32768, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.lm_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4_XVUHF1wTTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## General model architecture after 4-bit AWQ\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Custom layers for quantization and attention\n",
        "class RoPE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RoPE, self).__init__()\n",
        "        # Initialize as needed\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply rotary positional embedding\n",
        "        return x\n",
        "\n",
        "class WQLinear_GEMM(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=False, w_bit=4, group_size=128):\n",
        "        super(WQLinear_GEMM, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        self.w_bit = w_bit\n",
        "        self.group_size = group_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply quantized linear projection\n",
        "        return self.linear(x)  # Placeholder for actual quantized operation\n",
        "\n",
        "class FasterTransformerRMSNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(FasterTransformerRMSNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "class QuantAttentionFused(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuantAttentionFused, self).__init__()\n",
        "        self.qkv_proj = WQLinear_GEMM(4096, 6144, bias=False, w_bit=4, group_size=128)\n",
        "        self.o_proj = WQLinear_GEMM(4096, 4096, bias=False, w_bit=4, group_size=128)\n",
        "        self.rope = RoPE()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply fused attention projection and rotary embeddings\n",
        "        x = self.qkv_proj(x)\n",
        "        x = self.o_proj(x)\n",
        "        x = self.rope(x)\n",
        "        return x\n",
        "\n",
        "class QuantFusedMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QuantFusedMLP, self).__init__()\n",
        "        self.down_proj = WQLinear_GEMM(14336, 4096, bias=False, w_bit=4, group_size=128)\n",
        "        self.activation = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down_proj(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class MistralDecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralDecoderLayer, self).__init__()\n",
        "        self.self_attn = QuantAttentionFused()\n",
        "        self.mlp = QuantFusedMLP()\n",
        "        self.input_layernorm = FasterTransformerRMSNorm(4096)\n",
        "        self.post_attention_layernorm = FasterTransformerRMSNorm(4096)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layernorm(x)\n",
        "        x = self.self_attn(x)\n",
        "        x = self.post_attention_layernorm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "class MistralModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralModel, self).__init__()\n",
        "        self.embed_tokens = nn.Embedding(32768, 4096)\n",
        "        self.layers = nn.ModuleList([MistralDecoderLayer() for _ in range(32)])\n",
        "        self.norm = FasterTransformerRMSNorm(4096)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed_tokens(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class MistralForCausalLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MistralForCausalLM, self).__init__()\n",
        "        self.model = MistralModel()\n",
        "        self.lm_head = nn.Linear(4096, 32768, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.lm_head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "z1to8FaLwTTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers autoawq accelerate\n",
        "# !pip install -q nvidia-ml-py3"
      ],
      "metadata": {
        "trusted": true,
        "id": "51ZtbWiDwTTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_tokens = \"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\""
      ],
      "metadata": {
        "id": "9Y8YbZdFwTTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "1xqwF1SpwTTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_path = \"mistralai/Mistral-7B-v0.3\"\n",
        "quant_path = \"Mistral-7B-AWQ-4bit\"\n",
        "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\":4}\n",
        "\n",
        "# Load model\n",
        "model = AutoAWQForCausalLM.from_pretrained(model_path,**{\"low_cpu_mem_usage\": True})\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vbpMFEt0wTTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model contains following files\n",
        "\n",
        "1. **`config.json`**\n",
        "   - **Contents:** This file contains the configuration settings for the model architecture. It includes hyperparameters, model type, and other settings that define how the model is structured and how it should be initialized.\n",
        "\n",
        "2. **`generation_config.json`**\n",
        "   - **Contents:** This file includes configuration settings specific to the text generation process. This might include parameters like maximum sequence length, sampling strategies, and other settings relevant to generating text.\n",
        "\n",
        "3. **`model.safetensors.index.json`**\n",
        "   - **Contents:** This file is associated with the `safetensors` format and contains metadata and indexing information for efficiently accessing the model weights stored in the `safetensors` files. It helps in managing large model weights across multiple files.\n",
        "\n",
        "4. **`model-00001-of-00003.safetensors`**, **`model-00002-of-00003.safetensors`**, **`model-00003-of-00003.safetensors`**\n",
        "   - **Contents:** These files contain the actual quantized model weights. Since the model is large, the weights are split across multiple files. The `.safetensors` format is used to safely and efficiently store these weights, ensuring that they are properly serialized and deserialized during model loading.\n",
        "\n",
        "5. **`special_tokens_map.json`**\n",
        "   - **Contents:** This file maps special tokens used by the tokenizer to their respective identifiers. Special tokens include things like `[CLS]`, `[SEP]`, or any other token that has a specific role in the tokenization process.\n",
        "\n",
        "6. **`tokenizer.json`**\n",
        "   - **Contents:** This file contains the tokenizer's vocabulary and configuration in a JSON format. It includes mappings from token strings to token IDs and other tokenizer-specific settings.\n",
        "\n",
        "7. **`tokenizer.model`**\n",
        "   - **Contents:** This file is the actual binary model used by the tokenizer. It typically contains the underlying data structures required for tokenization and detokenization.\n",
        "\n",
        "8. **`tokenizer_config.json`**\n",
        "   - **Contents:** This file includes configuration settings for the tokenizer itself. It contains details on how the tokenizer should process text, such as pre-tokenization, normalization, and other settings.\n",
        "\n",
        "### Summary:\n",
        "- **Configuration Files (`config.json`, `generation_config.json`, `tokenizer_config.json`)**: These files define the setup and parameters for both the model and tokenizer.\n",
        "- **Model Weights Files (`model-*.safetensors`)**: These files contain the trained and quantized weights of the model.\n",
        "- **Tokenizer Files (`tokenizer.json`, `tokenizer.model`, `special_tokens_map.json`)**: These files are used for tokenizing and detokenizing text, including the tokenizer's vocabulary and special tokens.\n",
        "\n",
        "When loading the model and tokenizer, you typically need to load all these files to ensure that the model functions correctly and the tokenizer behaves as expected. If you're using a library like `transformers`, it handles the loading of these files automatically when you use functions like `from_pretrained`."
      ],
      "metadata": {
        "id": "sesAT-YMwTTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize\n",
        "model.quantize(tokenizer, quant_config=quant_config)"
      ],
      "metadata": {
        "trusted": true,
        "id": "XyUC4k-iwTTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the quantized model\n",
        "model.save_quantized(\"./\"+quant_path, safetensors=True)\n",
        "tokenizer.save_pretrained(\"./\"+quant_path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kV443Fo9wTTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "username = \"pritam3355\"\n",
        "MODEL_NAME = quant_path\n",
        "\n",
        "api = HfApi(token=hf_tokens)\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id = f\"{username}/{MODEL_NAME}\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "api.upload_folder(\n",
        "    repo_id = f\"{username}/{MODEL_NAME}\",\n",
        "    folder_path = \"/kaggle/working/Mistral-7B-AWQ-4bit\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "iZHX1B1FwTTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Mistral-7B`vs quantized `Mistral-7B 4-bit AWQ`\n",
        "\n",
        "### **1. Attention Mechanism**\n",
        "\n",
        "#### **Base Model:**\n",
        "- **`self_attn`:**\n",
        "  - **Components:**\n",
        "    - `q_proj`: Linear projection for queries.\n",
        "    - `k_proj`: Linear projection for keys.\n",
        "    - `v_proj`: Linear projection for values.\n",
        "    - `o_proj`: Linear projection for the output.\n",
        "  - **Attention Type:** Standard attention mechanism with linear projections for `q`, `k`, and `v`.\n",
        "\n",
        "#### **Quantized Model:**\n",
        "- **`self_attn`:**\n",
        "  - **Components:**\n",
        "    - `qkv_proj`: A fused linear projection for queries, keys, and values combined (`WQLinear_GEMM`). This uses 4-bit quantization and groups weights to optimize computation.\n",
        "    - `o_proj`: Linear projection for the output, also quantized.\n",
        "    - `rope`: Rotatory positional embeddings, likely optimized for efficient computation.\n",
        "  - **Attention Type:** Quantized attention with a fused projection that combines `q`, `k`, and `v` into a single operation for efficiency.\n",
        "\n",
        "### **2. MLP (Feedforward Network)**\n",
        "\n",
        "#### **Base Model:**\n",
        "- **`mlp`:**\n",
        "  - **Components:**\n",
        "    - `gate_proj`, `up_proj`, `down_proj`: Linear projections with different sizes for the gate, up, and down processes.\n",
        "    - `act_fn`: SiLU (Sigmoid Linear Unit) activation function.\n",
        "  \n",
        "#### **Quantized Model:**\n",
        "- **`mlp`:**\n",
        "  - **Components:**\n",
        "    - `down_proj`: Fused linear projection with 4-bit quantization (`WQLinear_GEMM`).\n",
        "    - `activation`: SiLU activation function.\n",
        "  - **MLP Type:** Quantized and fused MLP layers for more efficient computation.\n",
        "\n",
        "### **3. Normalization Layers**\n",
        "\n",
        "#### **Base Model:**\n",
        "- **`input_layernorm`, `post_attention_layernorm`, `norm`:**\n",
        "  - **Type:** `MistralRMSNorm` for normalization layers.\n",
        "\n",
        "#### **Quantized Model:**\n",
        "- **`input_layernorm`, `post_attention_layernorm`, `norm`:**\n",
        "  - **Type:** `FasterTransformerRMSNorm` for improved performance and efficiency in the quantized model.\n",
        "\n",
        "### **4. Linear Output Layer**\n",
        "\n",
        "#### **Base Model:**\n",
        "- **`lm_head`:** Linear projection from hidden state size to vocabulary size.\n",
        "\n",
        "#### **Quantized Model:**\n",
        "- **`lm_head`:** Same as in the base model, but the quantization primarily affects the weights of the model rather than this output layer.\n",
        "\n",
        "### **5. Quantization Specific Changes**\n",
        "\n",
        "#### **Base Model:**\n",
        "- **Weights Precision:** Full precision weights (float32 or float16).\n",
        "\n",
        "#### **Quantized Model:**\n",
        "- **Quantization:**\n",
        "  - **Weights Precision:** Reduced to 4-bit precision.\n",
        "  - **Quantization Methods:** Use of `WQLinear_GEMM` for efficient quantized linear operations.\n",
        "  - **Fused Operations:** Some operations are fused to optimize computation and reduce memory usage.\n",
        "\n",
        "### **Summary of Key Differences**\n",
        "\n",
        "- **Attention Mechanism:** The quantized model uses fused and quantized attention operations (`QuantAttentionFused`), which combine the projections for efficiency.\n",
        "- **MLP:** The quantized model features a fused MLP with quantized weights (`QuantFusedMLP`).\n",
        "- **Normalization:** The quantized model uses `FasterTransformerRMSNorm`, likely optimized for performance with quantized models.\n",
        "- **Quantization:** The model’s weights are quantized to 4-bit precision to reduce memory and computational requirements.\n",
        "\n",
        "These changes are designed to make the quantized model more efficient, particularly in terms of memory usage and computational speed, while maintaining similar performance to the base model."
      ],
      "metadata": {
        "id": "FzhD4sBWwTTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Inference from Mistral-7B-AWQ-4bit\n",
        "\n",
        "For Mistral-7B-Instruct use appropriate model_id , performance drops significantly in AWQ Llama3 rather use Mistral-7B (format is same) but if you're using AutoAWQForCausalLM for just loading the model\n",
        "\n",
        "```python\n",
        "\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model = AutoAWQForCausalLM.from_quantized(model_id, fuse_layers=True,\n",
        "                                          trust_remote_code=False, safetensors=True)\n",
        "\n",
        "```\n",
        "\n",
        "transformers inference pipeline is also available for few models\n",
        "\n",
        "```python\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"*** Pipeline:\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "print(pipe(prompt_template)[0]['generated_text'])\n",
        "```\n"
      ],
      "metadata": {
        "id": "jvcoaYMswTTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "\n",
        "# model_id = \"pritam3355/Mistral-7B-AWQ-4bit\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# def get_current_weather(location: str, format: str):\n",
        "#     \"\"\"\n",
        "#     Get the current weather\n",
        "\n",
        "#     Args:\n",
        "#         location: The city and state, e.g. San Francisco, CA\n",
        "#         format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n",
        "#     \"\"\"\n",
        "#     pass\n",
        "\n",
        "# conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
        "# tools = [get_current_weather]\n",
        "\n",
        "# # render the tool use prompt as a string:\n",
        "# tool_use_prompt = tokenizer.apply_chat_template(\n",
        "#             conversation,\n",
        "#             tools=tools,\n",
        "#             tokenize=False,\n",
        "#             add_generation_prompt=True,\n",
        "# )\n",
        "\n",
        "# inputs = tokenizer(tool_use_prompt, return_tensors=\"pt\")\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens=1000)\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "AHq1veIhwTTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
        "\n",
        "# Model and quantization configuration\n",
        "model_id = \"pritam3355/Mistral-7B-AWQ-4bit\" # TechxGenus/Mistral-7B-v0.3-AWQ,kaitchup/Mistral-7B-awq-4bit\n",
        "quantization_config = AwqConfig(\n",
        "    bits=4,\n",
        "    fuse_max_seq_len=512,  # Note: Update this as per your use-case\n",
        "    do_fuse=True,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=False)\n",
        "\n",
        "# Define the system and user prompts\n",
        "system_prompt = \"You are an AI assistant knowledgeable in various fields.\"\n",
        "user_prompt = \"Tell me about continuous batching for faster inference in LLM\"\n",
        "\n",
        "# Create the prompt template\n",
        "prompt_template = f'{system_prompt}\\n\\nUser: {user_prompt}\\nAssistant:'\n",
        "\n",
        "# Print the prompt template for debugging\n",
        "print(\"Prompt Template:\\n\", prompt_template)\n",
        "\n",
        "# Tokenize the input\n",
        "tokens = tokenizer(\n",
        "    prompt_template,\n",
        "    return_tensors='pt'\n",
        ").input_ids.cuda()\n",
        "\n",
        "# Generate output\n",
        "generation_output = model.generate(\n",
        "    tokens,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "print(\"Output: \", tokenizer.decode(generation_output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "KsGJDiIlwTTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "here is the chat_template for the same\n",
        "\n",
        "```python\n",
        "\n",
        "prompt = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant, that responds as a pirate.\"},\n",
        "  {\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "  prompt,\n",
        "  tokenize=True,\n",
        "  add_generation_prompt=True,\n",
        "  return_tensors=\"pt\",\n",
        "  return_dict=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n",
        "print(tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "WvhiWax2wTTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BitsAndBytes\n",
        "\n",
        "```python\n",
        "!pip install -qqq bitsandbytes accelerate datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "if torch.cuda.is_bf16_supported():\n",
        "    compute_dtype = torch.bfloat16\n",
        "else:\n",
        "    compute_dtype = torch.float16\n",
        "\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "quant_path = 'Phi-3-mini-4k-instruct-bnb-4bit'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_name, quantization_config=bnb_config, trust_remote_code=True\n",
        ")\n",
        "\n",
        "model.save_pretrained(\"./\"+quant_path, safetensors=True)\n",
        "tokenizer.save_pretrained(\"./\"+quant_path)import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "quant_path = 'Phi-3-mini-4k-instruct-bnb-4bit'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_name, quantization_config=bnb_config, trust_remote_code=True\n",
        ")\n",
        "\n",
        "model.save_pretrained(\"./\"+quant_path, safetensors=True)\n",
        "tokenizer.save_pretrained(\"./\"+quant_path)\n",
        "```"
      ],
      "metadata": {
        "id": "7B7LPv6BwTTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto GPTQ\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "!pip install -qqq auto-gptq optimum\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from optimum.gptq import GPTQQuantizer\n",
        "import torch\n",
        "model_path = 'microsoft/Phi-3-mini-4k-instruct'\n",
        "w = 4 #quantization to 4-bit. Change to 2, 3, or 8 to quantize with another precision\n",
        "\n",
        "quant_path = 'Phi-3-mini-4k-instruct-gptq-'+str(w)+'bit'\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
        "quantizer = GPTQQuantizer(bits=w, dataset=\"c4\", model_seqlen = 2048)\n",
        "quantized_model = quantizer.quantize_model(model, tokenizer)\n",
        "\n",
        "quantized_model.save_pretrained(\"./\"+quant_path, safetensors=True)\n",
        "tokenizer.save_pretrained(\"./\"+quant_path)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "KhWO6q-IwTTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Small Language Model\n",
        "\n",
        "https://github.com/AIAnytime/Training-Small-Language-Model/blob/main/Training_a_Small_Language_Model.ipynb"
      ],
      "metadata": {
        "id": "ueDBWchswTTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q --upgrade torchtext sentencepiece datasets"
      ],
      "metadata": {
        "trusted": true,
        "id": "LlUzrEXLwTTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import pandas as pd\n",
        "import ast\n",
        "import datasets\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "trusted": true,
        "id": "X5DclpH7wTTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data set from huggingface\n",
        "data_sample = load_dataset(\"FreedomIntelligence/Disease_Database\",'en')\n",
        "\n",
        "data_sample"
      ],
      "metadata": {
        "trusted": true,
        "id": "tJpLgH6lwTTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a pandas dataframe\n",
        "updated_data = [{'Name': item['disease'], 'Symptoms': item['common_symptom']} for item in data_sample['train']]\n",
        "df = pd.DataFrame(updated_data)\n",
        "df['Symptoms'] = df['Symptoms'].apply(lambda x: ', '.join(x.split(', ')))\n",
        "df.head(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0LiCte0XwTTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    # If Apple Silicon, set to 'mps' - otherwise 'cpu' (not advised)\n",
        "    try:\n",
        "        device = torch.device('mps')\n",
        "    except Exception:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "device"
      ],
      "metadata": {
        "trusted": true,
        "id": "18IlEgcswTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "id": "i9eP4qwMwTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "df.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_ho3IcHhwTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Prep\n",
        "class LanguageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    An extension of the Dataset object to:\n",
        "      - Make training loop cleaner\n",
        "      - Make ingestion easier from pandas df's\n",
        "    \"\"\"\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.labels = df.columns\n",
        "        self.data = df.to_dict(orient='records')\n",
        "        self.tokenizer = tokenizer\n",
        "        x = self.fittest_max_length(df)  # Fix here\n",
        "        self.max_length = x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx][self.labels[1]] # Symptoms\n",
        "        y = self.data[idx][self.labels[0]] # Disease\n",
        "        text = f\"{x} | {y}\"\n",
        "        tokens = self.tokenizer.encode_plus(text, return_tensors='pt', max_length=128, padding='max_length', truncation=True)\n",
        "        return tokens\n",
        "\n",
        "    def fittest_max_length(self, df):  # Fix here\n",
        "        \"\"\"\n",
        "        Smallest power of two larger than the longest term in the data set.\n",
        "        Important to set up max length to speed training time.\n",
        "        \"\"\"\n",
        "        max_length = max(len(max(df[self.labels[0]], key=len)), len(max(df[self.labels[1]], key=len)))\n",
        "        x = 2\n",
        "        while x < max_length: x = x * 2\n",
        "        return x\n",
        "\n",
        "# Cast the Huggingface data set as a LanguageDataset we defined above\n",
        "data_sample = LanguageDataset(df, tokenizer)\n",
        "\n",
        "train_size = int(0.8 * len(data_sample))\n",
        "valid_size = len(data_sample) - train_size\n",
        "train_data, valid_data = random_split(data_sample, [train_size, valid_size])\n",
        "\n",
        "# Make the iterators\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rGIzLtlVwTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "trusted": true,
        "id": "KSS0p3C4wTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Training parameters\n",
        "batch_size = BATCH_SIZE\n",
        "model_name = 'distilgpt2'\n",
        "gpu = 0\n",
        "\n",
        "# Set the learning rate and loss function\n",
        "## CrossEntropyLoss measures how close answers to the truth.\n",
        "## More punishing for high confidence wrong answers\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "trusted": true,
        "id": "AkGINxNYwTTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init a results dataframe\n",
        "results = pd.DataFrame(columns=['epoch', 'transformer', 'batch_size', 'gpu',\n",
        "                                'training_loss', 'validation_loss', 'epoch_duration_sec'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "k6Q-s509wTTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training loop\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Start the timer for the epoch\n",
        "\n",
        "    # Training\n",
        "    ## This line tells the model we're in 'learning mode'\n",
        "    model.train()\n",
        "    epoch_training_loss = 0\n",
        "    train_iterator = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs} Batch Size: {batch_size}, Transformer: {model_name}\")\n",
        "    for batch in train_iterator:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch['input_ids'].squeeze(1).to(device)\n",
        "        targets = inputs.clone()\n",
        "        outputs = model(input_ids=inputs, labels=targets)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_iterator.set_postfix({'Training Loss': loss.item()})\n",
        "        epoch_training_loss += loss.item()\n",
        "    avg_epoch_training_loss = epoch_training_loss / len(train_iterator)\n",
        "\n",
        "    # Validation\n",
        "    ## This line below tells the model to 'stop learning'\n",
        "    model.eval()\n",
        "    epoch_validation_loss = 0\n",
        "    total_loss = 0\n",
        "    valid_iterator = tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\")\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_iterator:\n",
        "            inputs = batch['input_ids'].squeeze(1).to(device)\n",
        "            targets = inputs.clone()\n",
        "            outputs = model(input_ids=inputs, labels=targets)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss\n",
        "            valid_iterator.set_postfix({'Validation Loss': loss.item()})\n",
        "            epoch_validation_loss += loss.item()\n",
        "\n",
        "    avg_epoch_validation_loss = epoch_validation_loss / len(valid_loader)\n",
        "\n",
        "    end_time = time.time()  # End the timer for the epoch\n",
        "    epoch_duration_sec = end_time - start_time  # Calculate the duration in seconds\n",
        "\n",
        "    new_row = {'transformer': model_name,\n",
        "               'batch_size': batch_size,\n",
        "               'gpu': gpu,\n",
        "               'epoch': epoch+1,\n",
        "               'training_loss': avg_epoch_training_loss,\n",
        "               'validation_loss': avg_epoch_validation_loss,\n",
        "               'epoch_duration_sec': epoch_duration_sec}  # Add epoch_duration to the dataframe\n",
        "\n",
        "    results.loc[len(results)] = new_row\n",
        "    print(f\"Epoch: {epoch+1}, Validation Loss: {total_loss/len(valid_loader)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "LBdIT6e5wTTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\tName\tSymptoms\n",
        "    \n",
        "    \n",
        "        0\tCarotid Sinus Syndrome\tSyncope, Cardiac Arrest, Dizziness, Fatigue, T...\n",
        "        1\tCranial Osteomyelitis\tHeadache and fever, localized scalp redness, s...\n",
        "        2\tTuberculous Osteomyelitis\tLow-grade fever, fatigue, night sweats, loss o...\n",
        "        3\tLipodystrophy\tHematuria, pyuria, acute abdominal pain, edema...\n",
        "        4\tVulvar Pemphigus\tBlisters, pain, anorexia, mucosal damage, hype..."
      ],
      "metadata": {
        "id": "mln_ny7WwTTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"Headache and feve,Blisters,\" # Meralgia Paresthetica,Kidney Failure\n",
        "input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=20,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    top_k=8,\n",
        "    top_p=0.95,\n",
        "    temperature=0.5,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(decoded_output)\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "fPhEdPiHwTTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'SmallMedLM.pt')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZcdVPjgPwTTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "trusted": true,
        "id": "x0fr7VXkwTTU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}