
	"https://arxiv.org/pdf/2109.12507
"	
	https://arxiv.org/abs/2009.06732	
	"https://arxiv.org/pdf/2205.06743
"	
	"https://arxiv.org/abs/2109.08668
"	
	https://arxiv.org/pdf/2205.12662	
	https://arxiv.org/abs/2311.02300	
	"https://arxiv.org/html/2401.00368v1
"	
	https://blog.research.google/2022/12/google-at-emnlp-2022.html	
		
Illustrated Transformer 	https://news.ycombinator.com/item?id=35712334	
Adapter based fine tuning 	https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters	
Adapter based fine tuning 	https://smashinggradient.com/2023/04/11/summary-of-adapter-based-performance-efficient-fine-tuning-peft-techniques-for-large-language-models/	
LLM fine tuning 	https://github.com/rasbt/LLM-finetuning-scripts/tree/main	
Understanding LLM	https://magazine.sebastianraschka.com/p/understanding-large-language-models	
RHLF & alternate 	https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives	
Prompt Engineering 	https://github.com/PromtEngineer/YoutTube-Tutorial	
Adapter learning 	https://github.com/calpt/awesome-adapter-resources	
LLM from scratch 	https://github.com/rasbt/LLMs-from-scratch	
llama3 from scratch 	https://github.com/naklecha/llama3-from-scratch	
		