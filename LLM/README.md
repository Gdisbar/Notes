## Blogs

[Illustrated Transformer](https://news.ycombinator.com/item?id=35712334)

[Transformers](https://deeprevision.github.io/posts/001-transformer/#transformer-current-challenges-and-future-directions)

[Adapter based fine tuning](https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters)	

[Adapter based fine tuning 2](https://smashinggradient.com/2023/04/11/summary-of-adapter-based-performance-efficient-fine-tuning-peft-techniques-for-large-language-models/)	

[Understanding LLM](https://magazine.sebastianraschka.com/p/understanding-large-language-models)

[RHLF & it's alternate](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives)

[LangChain](https://blog.futuresmart.ai/building-a-production-ready-rag-chatbot-with-fastapi-and-langchain)

[Quantization](https://newsletter.maartengrootendorst.com/p/which-quantization-method-is-right)

[Evaluation metrics - RAG & Fine-Tuning](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#faithfulness)

## YouTube

[Pre-train 3B LLM](https://www.youtube.com/watch?v=aPzbR1s1O_8)

[Reproduce GPT-2](https://www.youtube.com/watch?v=l8pRSuU81PU)

[Building Production RAG](https://www.youtube.com/watch?v=dI_TmTW9S4c)

[LangChain vs LangGraph](https://www.youtube.com/watch?v=bvMX-Zlfv68)

[DataStream with LangChain + FastAPI](https://www.youtube.com/watch?v=Gn54EbU9mRg)

[Build & Deploy AI ChatBot](https://www.youtube.com/watch?v=KyQKTJhSIak)

[Knowledge Graph](https://www.youtube.com/watch?app=desktop&v=yNCI3DC3tLg)


## Github

[LLM fine tuning](https://github.com/rasbt/LLM-finetuning-scripts/tree/main)	

[LLM from scratch](https://github.com/rasbt/LLMs-from-scratch)	

[LLM+GenAI](https://github.com/Vasanthengineer4949/NLP-Projects-NHV)

[llama3 from scratch](https://github.com/naklecha/llama3-from-scratch)

[Adapter learning](https://github.com/calpt/awesome-adapter-resources)

[Prompt Engineering](https://github.com/PromtEngineer/YoutTube-Tutorial)		

[LLM+GenAI](https://github.com/sunnysavita10/Generative-AI-Indepth-Basic-to-Advance)

[LangChain-FastAPI](https://github.com/Coding-Crashkurse/Advanced-LangChain-with-FastAPI)


# TPU training


[LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)

[DAIGT Mistral-7B TPU BFloat16 (Train)](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)

[LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)

[LLAMA 3 TPU (Train)](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/notebook)

# GPU Training

[Gemma-2 9b 4-bit QLoRA fine-tuning](https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning)

# Fine-Tuning - deeplearning.ai

[Fine-tune FLAN-T5 with PEFT/LoRA](https://www.kaggle.com/code/paultimothymooney/fine-tune-flan-t5-with-peft-lora-deeplearning-ai)

[Fine-tune FLAN-T5 with PPO](https://www.kaggle.com/code/paultimothymooney/fine-tune-flan-t5-with-ppo-deeplearning-ai)

# Training Small Language Model,MLOps

[Training Small Language Model](https://github.com/AIAnytime/Training-Small-Language-Model/blob/main/Training_a_Small_Language_Model.ipynb)

[Trelis Research](https://github.com/TrelisResearch/install-guides?tab=readme-ov-file)

[MLOps](https://github.com/olonok69/LLM_Notebooks/blob/main/mlflow/mlflow_transformers_fine_tuning.ipynb)


# Applications

[Prompt Prediction - Miztral,Gemma,Llama](https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama)


[Code Interpreter Baseline](https://www.kaggle.com/code/huikang/code-interpreter-baseline)



