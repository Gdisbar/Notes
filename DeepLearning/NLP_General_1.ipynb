{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "NLP-General-1",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "JwSiRx-Uha1b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ``spaCy pipeline`` components\n",
        "\n",
        "### **tok2vec** (Token-to-Vector)\n",
        "- `add_label(label)`: Adds a label to the tok2vec model.\n",
        "- `create_optimizer()`: Creates an optimizer for training the model.\n",
        "- `from_bytes(bytes_data)`: Loads the component from a binary format.\n",
        "- `from_disk(path)`: Loads the component from a directory.\n",
        "- `initialize()`: Initializes the model for training.\n",
        "- `model`: The underlying neural network model used for token-to-vector transformation.\n",
        "- `predict(docs)`: Predicts token representations for input documents.\n",
        "- `update(docs, golds)`: Updates the model using labeled data.\n",
        "- `to_bytes()`: Serializes the component to a binary format.\n",
        "- `to_disk(path)`: Saves the component to a directory.\n",
        "\n",
        "### **tagger** (Part-of-Speech Tagger)\n",
        "- `add_label(label)`: Adds a new POS tag label.\n",
        "- `create_optimizer()`: Creates an optimizer for training.\n",
        "- `from_bytes(bytes_data)`: Loads the tagger from a binary format.\n",
        "- `from_disk(path)`: Loads the tagger from a directory.\n",
        "- `initialize()`: Initializes the tagger with training data.\n",
        "- `labels`: The list of POS labels the tagger recognizes.\n",
        "- `predict(docs)`: Predicts POS tags for input documents.\n",
        "- `update(docs, golds)`: Updates the tagger model with labeled data.\n",
        "- `to_bytes()`: Serializes the tagger to a binary format.\n",
        "- `to_disk(path)`: Saves the tagger model to disk.\n",
        "\n",
        "### **parser** (Dependency Parser)\n",
        "- `add_label(label)`: Adds a dependency label.\n",
        "- `beam_parse(docs)`: Performs dependency parsing using beam search.\n",
        "- `greedy_parse(docs)`: Performs dependency parsing using greedy decoding.\n",
        "- `initialize()`: Initializes the parser with training data.\n",
        "- `labels`: The list of dependency labels used by the parser.\n",
        "- `predict(docs)`: Predicts dependency structures for input documents.\n",
        "- `update(docs, golds)`: Updates the parser model using labeled examples.\n",
        "- `to_bytes()`: Serializes the parser to a binary format.\n",
        "- `to_disk(path)`: Saves the parser to a directory.\n",
        "\n",
        "### **attribute_ruler** (Rule-based Attribute Modification)\n",
        "- `add(name, value)`: Adds an attribute modification rule.\n",
        "- `add_patterns(patterns)`: Adds multiple attribute modification patterns.\n",
        "- `from_bytes(bytes_data)`: Loads attribute rules from a binary format.\n",
        "- `from_disk(path)`: Loads attribute rules from a directory.\n",
        "- `match(docs)`: Matches input documents against stored attribute rules.\n",
        "- `patterns`: The list of patterns used for attribute modification.\n",
        "- `pipe(docs)`: Applies attribute rules in a pipeline.\n",
        "- `to_bytes()`: Serializes the attribute ruler to a binary format.\n",
        "- `to_disk(path)`: Saves attribute rules to disk.\n",
        "\n",
        "### **lemmatizer** (Word Lemmatization)\n",
        "- `initialize()`: Initializes the lemmatizer with language-specific rules.\n",
        "- `is_base_form(word)`: Checks if a word is already in its base form.\n",
        "- `lemmatize(token)`: Returns the lemma of a given token.\n",
        "- `lookup_lemmatize(token)`: Uses a lookup table for lemmatization.\n",
        "- `rule_lemmatize(token)`: Uses predefined rules for lemmatization.\n",
        "- `mode`: Specifies the lemmatization mode (rule-based or lookup).\n",
        "- `to_bytes()`: Serializes the lemmatizer to a binary format.\n",
        "- `to_disk(path)`: Saves the lemmatizer to a directory.\n",
        "\n",
        "### **ner** (Named Entity Recognizer)\n",
        "- `add_label(label)`: Adds a new entity label.\n",
        "- `beam_parse(docs)`: Performs named entity recognition using beam search.\n",
        "- `greedy_parse(docs)`: Performs named entity recognition using greedy decoding.\n",
        "- `initialize()`: Initializes the NER model with training data.\n",
        "- `labels`: The list of named entity labels recognized by the model.\n",
        "- `predict(docs)`: Predicts named entities in input documents.\n",
        "- `update(docs, golds)`: Updates the NER model using labeled data.\n",
        "- `to_bytes()`: Serializes the NER model to a binary format.\n",
        "- `to_disk(path)`: Saves the NER model to a directory.\n",
        "\n",
        "This provides a concise overview of key methods and attributes for each component in spaCy's pipeline. ðŸš€"
      ],
      "metadata": {
        "id": "LEkxx4yHha1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `spaCy Token` attributes and methods:  \n",
        "\n",
        "- **`text`**: The original token text.  \n",
        "- **`lemma_`**: The base form of the token.  \n",
        "- **`pos_`**: The part of speech (POS) tag.  \n",
        "- **`tag_`**: The detailed POS tag.  \n",
        "- **`dep_`**: The syntactic dependency relation of the token.  \n",
        "- **`head`**: The token's syntactic head (governing word).  \n",
        "- **`ent_type_`**: The named entity type (if applicable).  \n",
        "- **`is_alpha`, `is_punct`, `is_space`, `is_stop`**: Boolean flags for character type.\n",
        "- **`vector`**: The word embedding vector representation.  \n",
        "- **`similarity()`**: Computes similarity between two tokens.  \n",
        "- **`sent`**: Returns the sentence the token belongs to.  \n",
        "- **`i`**: The index of the token in the document.  \n",
        "- **`idx`**: The character index of the token in the original text.  \n",
        "- **`morph`**: Morphological features of the token.  \n",
        "- **`shape_`**: The shape of the token (e.g., \"Xx\" for \"Apple\").  \n",
        "- **`lefts` / `rights`**: Iterators over the tokenâ€™s left/right children in the dependency tree.  \n",
        "- **`subtree`**: An iterator over all descendant tokens.  \n",
        "\n"
      ],
      "metadata": {
        "id": "q9QpoIW2ha1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  \n",
        "\n",
        "### `spaCy Doc` Core Attributes & Methods\n",
        "\n",
        "- **`text`**: The full text of the document.  \n",
        "- **`text_with_ws`**: The document text, preserving original whitespace.  \n",
        "- **`ents`**: A list of named entities in the document.  \n",
        "- **`sents`**: An iterator over the sentences in the document.  \n",
        "- **`noun_chunks`**: A list of noun phrases (NPs) in the document.  \n",
        "- **`vector`**: The document-level word embedding.  \n",
        "- **`vector_norm`**: The L2 norm of the documentâ€™s word embedding.  \n",
        "- **`similarity()`**: Computes semantic similarity between two `Doc` objects.  \n",
        "\n",
        "### **Tokenization & Parsing**  \n",
        "- **`doc[i]`**: Retrieves the `i`-th token in the document.  \n",
        "- **`from_array()` / `to_array() , from_dict()` / `to_dict()`**: Converts a document to/from a NumPy array/Python dictionary.  \n",
        "- **`from_bytes()` / `to_bytes()` , `from_json()` / `to_json()`**: Serializes/deserializes a document in binary/JSON format.      \n",
        "- **`from_disk()` / `to_disk()`**: Loads/saves a document from/to disk.  \n",
        "\n",
        "### **Linguistic Analysis**  \n",
        "- **`has_vector`**: Checks if any token in the document has a word vector.  \n",
        "- **`is_parsed`**: Checks if the document has been syntactically parsed.  \n",
        "- **`is_tagged`**: Checks if the document has been POS-tagged.  \n",
        "- **`is_sentenced`**: Checks if sentence boundaries are defined.  \n",
        "- **`count_by()`**: Counts token attributes (e.g., POS tags, dependency labels).  \n",
        "- **`get_lca_matrix()`**: Returns the Lowest Common Ancestor (LCA) matrix for dependency parsing.  \n",
        "\n",
        "### **Customization & Extensions**  \n",
        "- **`set_extension()` / `get_extension()`**: Defines and retrieves custom attributes.  \n",
        "- **`remove_extension()`**: Removes a custom extension.  \n",
        "- **`user_data`**: A dictionary for storing user-defined metadata.  \n",
        "\n",
        "### **Token Merging & Retokenization**  \n",
        "- **`retokenize()`**: Allows modifying token boundaries (e.g., merging tokens).  \n"
      ],
      "metadata": {
        "id": "dDPojxdOha1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phrase matcher"
      ],
      "metadata": {
        "id": "1jm4rURhha1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "nlp = English()\n",
        "matcher = PhraseMatcher(nlp.vocab,attr=\"SHAPE\")\n",
        "\n",
        "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\",\"Washington\"]\n",
        "matcher.add(\"Terminology\", patterns)\n",
        "matcher.add(\"IPaddress\", [nlp(\"127.0.0.1\"), nlp(\"127.127.0.0\")])\n",
        "\n",
        "\n",
        "text = \"\"\"German Chancellor Angela Merkel and US President Barack Obama converse in the Oval Office inside the\n",
        "White House in Washington, D.C. Often their router will have an IP address such as 192.168.1.1 or 192.168.2.1.\n",
        "Lee, an experienced CEO, has founded two AI startups.\"\"\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "    span = doc[start:end]\n",
        "    match_name = nlp.vocab.strings[match_id]  # name of the matcher\n",
        "    print(f\"{span.text} (Matched by: {match_name})\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-07T14:42:40.58862Z",
          "iopub.execute_input": "2025-02-07T14:42:40.588963Z",
          "iopub.status.idle": "2025-02-07T14:42:40.754102Z",
          "shell.execute_reply.started": "2025-02-07T14:42:40.588935Z",
          "shell.execute_reply": "2025-02-07T14:42:40.753018Z"
        },
        "id": "KPLkVNWHha1u",
        "outputId": "ff93d213-d348-45d0-8072-bcbfc4d60f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "German Chancellor (Matched by: Terminology)\nChancellor Angela (Matched by: Terminology)\nAngela Merkel (Matched by: Terminology)\nPresident Barack (Matched by: Terminology)\nBarack Obama (Matched by: Terminology)\nWhite House (Matched by: Terminology)\nWashington, D.C. (Matched by: Terminology)\n192.168.1.1 (Matched by: IPaddress)\n192.168.2.1 (Matched by: IPaddress)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency matcher\n",
        "\n",
        " `spacy.matcher` module to create a matcher that works based on dependency relations between words in a sentence.\n",
        "\n",
        "Here's a breakdown of the keys in the `DependencyMatcher` pattern:\n",
        "\n",
        "* **`RIGHT_ID`**:  A unique identifier for this part of the pattern.  Used to link different parts of the pattern together.\n",
        "\n",
        "* **`RIGHT_ATTRS`**:  Conditions that the token being matched must satisfy.  Here, it specifies the word form (`ORTH`) or dependency relation (`DEP`).\n",
        "\n",
        "* **`LEFT_ID`**: Refers back to the `RIGHT_ID` of a previously defined part of the pattern. This establishes the relationships between tokens.\n",
        "\n",
        "* **`REL_OP`**: Specifies the relationship between the \"left\" and \"right\" tokens. `>` means \"directly governed by\" (a direct dependency relation).\n",
        "\n",
        "* **`RIGHT_ATTRS: {\"DEP\": \"nsubj\"}`**: The token must have the dependency relation \"nsubj\" (nominal subject).\n",
        "\n",
        "* **`RIGHT_ATTRS: {\"DEP\": \"dobj\"}`**: The token must have the dependency relation \"dobj\" (direct object).\n",
        "\n",
        "* **`RIGHT_ATTRS: {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}}`**: The token's dependency relation must be *either* \"amod\" (adjectival modifier) *or* \"compound\" (part of a compound noun).  `{\"IN\": [...]}` means the value must be one of the items in the list.\n",
        "\n",
        "\n",
        "**Key Points:**\n",
        "\n",
        "- The dependency matcher is a powerful tool for finding specific syntactic structures in text.\n",
        "- The `REL_OP` and `RIGHT_ATTRS` are used to define the relationships between tokens and their attributes.\n",
        "- Token indices are used to identify the matched tokens within the document.\n",
        "\n"
      ],
      "metadata": {
        "id": "XeAfTfVzha1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import DependencyMatcher\n",
        "\n",
        "matcher = DependencyMatcher(nlp.vocab)\n",
        "pattern = [\n",
        "    {\n",
        "        \"RIGHT_ID\": \"anchor_founded\",\n",
        "        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
        "    },\n",
        "    {\n",
        "        \"LEFT_ID\": \"anchor_founded\",\n",
        "        \"REL_OP\": \">\",\n",
        "        \"RIGHT_ID\": \"founded_subject\",\n",
        "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n",
        "    },\n",
        "    {\n",
        "        \"LEFT_ID\": \"anchor_founded\",\n",
        "        \"REL_OP\": \">\",\n",
        "        \"RIGHT_ID\": \"founded_object\",\n",
        "        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n",
        "    },\n",
        "    {\n",
        "        \"LEFT_ID\": \"founded_object\",\n",
        "        \"REL_OP\": \">\",\n",
        "        \"RIGHT_ID\": \"founded_object_modifier\",\n",
        "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
        "    }\n",
        "]\n",
        "\n",
        "matcher.add(\"FOUNDED\", [pattern])\n",
        "doc = nlp(\"Lee, an experienced CEO, has founded two AI startups.\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "print(matches) # [(4851363122962674176, [6, 0, 10, 9])]\n",
        "# Each token_id corresponds to one pattern dict\n",
        "match_id, token_ids = matches[0]\n",
        "for i in range(len(token_ids)):\n",
        "    print(pattern[i][\"RIGHT_ID\"] + \":\", doc[token_ids[i]].text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-07T14:34:17.131918Z",
          "iopub.execute_input": "2025-02-07T14:34:17.132267Z",
          "iopub.status.idle": "2025-02-07T14:34:17.156111Z",
          "shell.execute_reply.started": "2025-02-07T14:34:17.132241Z",
          "shell.execute_reply": "2025-02-07T14:34:17.15519Z"
        },
        "id": "x2wyfGI9ha1z",
        "outputId": "da156a7b-35ac-4241-8410-b7b3f5774595"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[(4851363122962674176, [7, 0, 10, 9])]\nanchor_founded: founded\nfounded_subject: Lee\nfounded_object: startups\nfounded_object_modifier: AI\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `EntityRuler` is applied *after* the default entity recognizer. This means you can use it to:\n",
        "\n",
        "- **Add new entities:**  Identify entities that the default model might miss.\n",
        "- **Override existing entities:** Correct misclassified entities or change the entity type.\n",
        "- **Add specific entity variations:** Handle different spellings, abbreviations, or forms of the same entity (as with \"San Francisco\" and \"San Fran\").\n",
        "\n",
        "\n",
        "**How the EntityRuler Works:**\n",
        "\n",
        "The `EntityRuler` works by matching the specified patterns against the text.  If a match is found, it adds or overwrites the entity annotation for the matched span. In your example, the ruler will:\n",
        "\n",
        "- Find \"Apple\" and label it as \"ORG\".\n",
        "- Find \"San Francisco\" (and \"San Fran\") and label it as \"GPE\".\n",
        "- Find \"MyCorp Inc.\" and label it as \"ORG\".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hnCb8PkRha12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
        "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"},\n",
        "            {\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\",\"id\": \"mycorp\"}]\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "text = \"Apple is opening its first big office in San Francisco. MyCorp Inc. is a company in the U.S.\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print([ent.text, ent.label])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-07T14:32:33.229419Z",
          "iopub.execute_input": "2025-02-07T14:32:33.229807Z",
          "iopub.status.idle": "2025-02-07T14:32:34.227082Z",
          "shell.execute_reply.started": "2025-02-07T14:32:33.229766Z",
          "shell.execute_reply": "2025-02-07T14:32:34.226031Z"
        },
        "id": "bQtsTv8cha13",
        "outputId": "83f3ad19-aec2-48f7-ac5a-a791b20677c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "['Apple', 383]\n['first', 396]\n['San Francisco', 384]\n['MyCorp Inc.', 383]\n['U.S.', 384]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Paraphrase-Identification\n",
        "https://github.com/wasiahmad/paraphrase_identification"
      ],
      "metadata": {
        "id": "ivScx74mha16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "patterns = [[{'POS':'ADJ'}, {'POS':'NOUN'}],]\n",
        "matcher.add(\"noun_adj\", patterns)\n",
        "\n",
        "\n",
        "# Function to tokenize sentence into phrases\n",
        "def extract_phrases(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    matches = matcher(doc)\n",
        "    phrases = []\n",
        "    for match_id,start,end in matches:\n",
        "      span = doc[start:end]\n",
        "      phrases.append(span.text)\n",
        "    phrases.extend(chunk.text for chunk in doc.noun_chunks)\n",
        "    return phrases\n",
        "\n",
        "\n",
        "text = \"\"\"The cat sat on the mat and licked its paws.A dog chased the ball and barked loudly.\n",
        "The sun set behind the mountains, casting a golden glow.\"\"\"\n",
        "\n",
        "for sentence in text.split(\".\"):\n",
        "    sentence = sentence.replace(\"\\n\",\"\")\n",
        "    if sentence:\n",
        "        phrases = extract_phrases(sentence)\n",
        "        print(f\"{sentence} - {phrases}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-07T15:26:05.558434Z",
          "iopub.execute_input": "2025-02-07T15:26:05.558835Z",
          "iopub.status.idle": "2025-02-07T15:26:06.358372Z",
          "shell.execute_reply.started": "2025-02-07T15:26:05.558801Z",
          "shell.execute_reply": "2025-02-07T15:26:06.357188Z"
        },
        "id": "shDxr8v3ha18",
        "outputId": "51c0cb90-ef55-49b1-e6f4-dd08cd5524b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The cat sat on the mat and licked its paws - ['The cat', 'the mat', 'its paws']\nA dog chased the ball and barked loudly - ['A dog', 'the ball']\nThe sun set behind the mountains, casting a golden glow - ['golden glow', 'The sun', 'the mountains', 'a golden glow']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Noun,Verb & Prepositional phrases"
      ],
      "metadata": {
        "id": "VYP7RIPaha1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "# Noun chunking\n",
        "noun_phrases = []\n",
        "for chunk in doc.noun_chunks:\n",
        "    noun_phrases.append(chunk.text)\n",
        "\n",
        "print(\"Noun Phrases:\", noun_phrases)\n",
        "print(\"-\"*100)\n",
        "\n",
        "# Verb phrases\n",
        "verb_phrases = []\n",
        "for token in doc:\n",
        "    if token.pos_ == \"VERB\":\n",
        "        verb_phrase = token.text\n",
        "        for child in token.children:\n",
        "            if child.dep_ in [\"aux\", \"auxpass\", \"advmod\", \"prt\"]:\n",
        "                verb_phrase += \" \" + child.text\n",
        "        verb_phrases.append(verb_phrase)\n",
        "\n",
        "\n",
        "print(\"Verb Phrases:\", verb_phrases)\n",
        "print(\"-\"*100)\n",
        "\n",
        "# Prepositional phrases\n",
        "prepositional_phrases = []\n",
        "for token in doc:\n",
        "    if token.pos_ == \"ADP\":  # Check if token is a preposition\n",
        "        prep_phrase = \" \".join([tok.text for tok in token.subtree])\n",
        "        prepositional_phrases.append(prep_phrase)\n",
        "\n",
        "print(\"Prepositional Phrases:\", prepositional_phrases)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-07T15:43:05.027323Z",
          "iopub.execute_input": "2025-02-07T15:43:05.027703Z",
          "iopub.status.idle": "2025-02-07T15:43:05.896926Z",
          "shell.execute_reply.started": "2025-02-07T15:43:05.027673Z",
          "shell.execute_reply": "2025-02-07T15:43:05.894695Z"
        },
        "id": "GpsN-71Sha2A",
        "outputId": "5a863d52-0fbb-4f3b-f020-88980d441b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "noun Phrases: ['The quick brown fox', 'the lazy dog']\n----------------------------------------------------------------------------------------------------\nVerb Phrases: ['jumps']\n----------------------------------------------------------------------------------------------------\nPrepositional Phrases: ['over the lazy dog']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analyzer"
      ],
      "metadata": {
        "id": "-w3spK5lha2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "HF_USERNAME = \"\"\n",
        "HF_TOKEN = \"\"\n",
        "\n",
        "try:\n",
        "  login(token=HF_TOKEN)\n",
        "except ValueError:\n",
        "  login(username=HF_USERNAME, token=HF_TOKEN)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:02:23.702532Z",
          "iopub.execute_input": "2025-02-08T14:02:23.702823Z",
          "iopub.status.idle": "2025-02-08T14:02:23.748288Z",
          "shell.execute_reply.started": "2025-02-08T14:02:23.702801Z",
          "shell.execute_reply": "2025-02-08T14:02:23.747584Z"
        },
        "id": "kPHFQEdaha2C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ds= load_dataset(\"Falah/sentiments-dataset-381-classes\")\n",
        "df = pd.DataFrame(ds['train'])\n",
        "ds"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:05:54.942788Z",
          "iopub.execute_input": "2025-02-08T14:05:54.943086Z",
          "iopub.status.idle": "2025-02-08T14:05:55.792709Z",
          "shell.execute_reply.started": "2025-02-08T14:05:54.943064Z",
          "shell.execute_reply": "2025-02-08T14:05:55.79174Z"
        },
        "id": "U0ExOTaSha2D",
        "outputId": "111bc37d-bafc-457f-948c-2244031a8a32"
      },
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'sentiment'],\n        num_rows: 1061\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "#Get the unique class names from the \"sentiment\" column\n",
        "label_names = df['sentiment'].unique()\n",
        "label_names = label_names.tolist()\n",
        "label_names[:5]\n",
        "label_counter = Counter(df['sentiment'].tolist())\n",
        "\n",
        "tmp_label_counter = {}\n",
        "for label, count in label_counter.items():\n",
        "    if count >= 10:\n",
        "        tmp_label_counter[label] = count\n",
        "\n",
        "label_counter = tmp_label_counter\n",
        "labels_to_keep = list(label_counter.keys())\n",
        "df = df[df['sentiment'].isin(labels_to_keep)]\n",
        "df.shape,len(labels_to_keep)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:05:58.882136Z",
          "iopub.execute_input": "2025-02-08T14:05:58.882434Z",
          "iopub.status.idle": "2025-02-08T14:05:58.891232Z",
          "shell.execute_reply.started": "2025-02-08T14:05:58.882412Z",
          "shell.execute_reply": "2025-02-08T14:05:58.890432Z"
        },
        "id": "q9H6V_ljha2F",
        "outputId": "78f42295-1ca5-457e-df34-b0d6fd3ead0c"
      },
      "outputs": [
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/plain": "((436, 2), 16)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict, Dataset, ClassLabel\n",
        "ds = Dataset.from_pandas(df)\n",
        "ds = ds.remove_columns('__index_level_0__')\n",
        "ds"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:06:02.221262Z",
          "iopub.execute_input": "2025-02-08T14:06:02.221575Z",
          "iopub.status.idle": "2025-02-08T14:06:02.236042Z",
          "shell.execute_reply.started": "2025-02-08T14:06:02.221553Z",
          "shell.execute_reply": "2025-02-08T14:06:02.235271Z"
        },
        "id": "nahqyyiJha2H",
        "outputId": "d5b6a411-8f4e-4dbd-924f-a6526c2d407e"
      },
      "outputs": [
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['text', 'sentiment'],\n    num_rows: 436\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict, Dataset, ClassLabel\n",
        "\n",
        "# Convert sentiment to ClassLabel (int)\n",
        "\n",
        "ds = ds.cast_column(\"sentiment\", ClassLabel(names=labels_to_keep))\n",
        "id2label = ds.features[\"sentiment\"].int2str\n",
        "\n",
        "# First split: 85% train + 15% test\n",
        "train_test = ds.train_test_split(test_size=0.15, stratify_by_column=\"sentiment\")\n",
        "\n",
        "# Second split: From the remaining 85%, split into 10% validation and the rest as train (75%)\n",
        "train_valid = train_test[\"train\"].train_test_split(test_size=10/85, stratify_by_column=\"sentiment\") # 10/85 to get 10% of the original data\n",
        "\n",
        "final_dataset = DatasetDict({\n",
        "    \"train\": train_valid[\"train\"],\n",
        "    \"test\": train_test[\"test\"],\n",
        "    \"valid\": train_valid[\"test\"],\n",
        "})\n",
        "\n",
        "print(final_dataset)\n",
        "print(id2label)\n",
        "\n",
        "# Verification (optional - check the sizes and distribution)\n",
        "print(len(final_dataset[\"train\"]) / len(ds)) # Should be close to 0.75\n",
        "print(len(final_dataset[\"valid\"]) / len(ds)) # Should be close to 0.10\n",
        "print(len(final_dataset[\"test\"]) / len(ds))  # Should be close to 0.15\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:06:05.122656Z",
          "iopub.execute_input": "2025-02-08T14:06:05.122964Z",
          "iopub.status.idle": "2025-02-08T14:06:05.168728Z",
          "shell.execute_reply.started": "2025-02-08T14:06:05.122943Z",
          "shell.execute_reply": "2025-02-08T14:06:05.168083Z"
        },
        "id": "chh9puOpha2I",
        "outputId": "0b6e0299-7bc0-4706-c918-839aac7ac12b",
        "colab": {
          "referenced_widgets": [
            "9a5ae0d318c34c939ada9b642c07eaef"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Casting the dataset:   0%|          | 0/436 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a5ae0d318c34c939ada9b642c07eaef"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'sentiment'],\n        num_rows: 326\n    })\n    test: Dataset({\n        features: ['text', 'sentiment'],\n        num_rows: 66\n    })\n    valid: Dataset({\n        features: ['text', 'sentiment'],\n        num_rows: 44\n    })\n})\n<bound method ClassLabel.int2str of ClassLabel(names=['Positive', 'Joyful', 'Disappointed', 'Worried', 'Grateful', 'Indifferent', 'Sad', 'Angry', 'Relieved', 'Excited', 'Anxious', 'Satisfied', 'Happy', 'Nostalgic', 'Inspired', 'Impressed'], id=None)>\n0.7477064220183486\n0.10091743119266056\n0.15137614678899083\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check distribution\n",
        "# def check_distribution(dataset, split_name):\n",
        "#     sentiments = dataset[split_name][\"sentiment\"]\n",
        "#     unique_sentiments = set(sentiments)\n",
        "#     for sentiment in unique_sentiments:\n",
        "#       count = sentiments.count(sentiment)\n",
        "#       print(f\"{split_name} - Sentiment {sentiment}: {count} ({count/len(sentiments)*100:.2f}%)\")\n",
        "\n",
        "# check_distribution(final_dataset, \"train\")\n",
        "# check_distribution(final_dataset, \"valid\")\n",
        "# check_distribution(final_dataset, \"test\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:02:25.925502Z",
          "iopub.execute_input": "2025-02-08T14:02:25.925793Z",
          "iopub.status.idle": "2025-02-08T14:02:25.929004Z",
          "shell.execute_reply.started": "2025-02-08T14:02:25.925771Z",
          "shell.execute_reply": "2025-02-08T14:02:25.928151Z"
        },
        "id": "PommYcctha2K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoConfig\n",
        "\n",
        "\n",
        "model_name = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "base_config = AutoConfig.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:06:10.555351Z",
          "iopub.execute_input": "2025-02-08T14:06:10.555654Z",
          "iopub.status.idle": "2025-02-08T14:06:11.030994Z",
          "shell.execute_reply.started": "2025-02-08T14:06:10.555633Z",
          "shell.execute_reply": "2025-02-08T14:06:11.030056Z"
        },
        "id": "Xo7OJzc-ha2L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class Emojify(nn.Module):\n",
        "    def __init__(self, embed_dim, output_dim,input_sz=512,hidden_sz=256, num_heads=4):\n",
        "        super(Emojify, self).__init__()\n",
        "        # Use pre-trained DistilBERT embeddings - (vocab_size,embed_dim)\n",
        "        self.embedding = base_model.distilbert.embeddings\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.lstm1 = nn.LSTM(embed_dim, hidden_sz, batch_first=True, bidirectional=True)\n",
        "        self.layer_norm1 = nn.LayerNorm(input_sz)\n",
        "        self.lstm2 = nn.LSTM(input_sz, hidden_sz, batch_first=True, bidirectional=True)\n",
        "        self.layer_norm2 = nn.LayerNorm(input_sz)\n",
        "        self.lstm3 = nn.LSTM(input_sz, hidden_sz, batch_first=True)\n",
        "        self.layer_norm3 = nn.LayerNorm(hidden_sz)\n",
        "        self.fc = nn.Linear(hidden_sz, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad():  # Use frozen embeddings from DistilBERT\n",
        "            embeddings = self.embedding(input_ids)\n",
        "            # embeddings - (batch_size,seq_length,embed_dim)\n",
        "\n",
        "        attn_output, attn_weight = self.attention(embeddings, embeddings, embeddings)\n",
        "        # attn_output - (batch_size,seq_length,embed_dim)\n",
        "        # attn_weight - (batch_size,seq_length,seq_length)\n",
        "        attn_output = embeddings + attn_output  # skip connection\n",
        "        # attn_output - (batch_size, seq_length, embed_dim)\n",
        "        output, _ = self.lstm1(attn_output)\n",
        "        output = self.layer_norm1(output)\n",
        "        # layer_norm1 - (batch_size, seq_length, input_sz)\n",
        "        output, _ = self.lstm2(output)\n",
        "        output = self.layer_norm2(output)\n",
        "        # layer_norm2 - (batch_size, seq_length, input_sz)\n",
        "        output, (hidden, _) = self.lstm3(output)\n",
        "        # lstm3_output - (batch_size, seq_length, hidden_sz)\n",
        "        # lstm3_hidden - (1, batch_size, hidden_sz)  (1 because it's the last layer)\n",
        "        hidden_output = self.layer_norm3(hidden.squeeze(0))  # Squeeze the hidden state\n",
        "        # layer_norm3 - (batch_size, hidden_sz)\n",
        "        hidden_output = self.fc(hidden_output)\n",
        "        # hidden_output - (batch_size, output_dim)\n",
        "        return hidden_output\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:06:13.677758Z",
          "iopub.execute_input": "2025-02-08T14:06:13.678062Z",
          "iopub.status.idle": "2025-02-08T14:06:13.685594Z",
          "shell.execute_reply.started": "2025-02-08T14:06:13.67804Z",
          "shell.execute_reply": "2025-02-08T14:06:13.684776Z"
        },
        "id": "0uQcvJteha2M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "tBcpRkkYha2N"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dummy data for testing\n",
        "# batch_size = 32\n",
        "# sequence_length = 64\n",
        "# vocab_size = 1000\n",
        "# embed_dim = 768\n",
        "# output_dim = 16\n",
        "# hidden_sz = 256\n",
        "# input_sz = hidden_sz * 2\n",
        "\n",
        "\n",
        "# input_ids = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "# attention_mask = torch.ones((batch_size, sequence_length))\n",
        "\n",
        "# model = Emojify(embed_dim, output_dim,input_sz,hidden_sz)\n",
        "# output = model(input_ids, attention_mask)\n",
        "# print(f\"Final output shape: {output.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:03:01.284424Z",
          "iopub.execute_input": "2025-02-08T14:03:01.284747Z",
          "iopub.status.idle": "2025-02-08T14:03:01.404789Z",
          "shell.execute_reply.started": "2025-02-08T14:03:01.284718Z",
          "shell.execute_reply": "2025-02-08T14:03:01.404188Z"
        },
        "id": "mhYgD6e-ha2O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import DatasetDict\n",
        "\n",
        "\n",
        "class EmojifyDataset(Dataset):\n",
        "    def __init__(self, dataset, base_tokenizer, max_length=512):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = base_tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataset[idx]['text']\n",
        "        label = self.dataset[idx]['sentiment']\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),   # batch_size\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "max_input_length = base_config.max_position_embeddings  # 512\n",
        "# base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "# Convert datasets into PyTorch datasets\n",
        "train_dataset = EmojifyDataset(final_dataset['train'], base_tokenizer,max_input_length)\n",
        "valid_dataset = EmojifyDataset(final_dataset['valid'], base_tokenizer,max_input_length)\n",
        "test_dataset = EmojifyDataset(final_dataset['test'], base_tokenizer,max_input_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:06:27.288243Z",
          "iopub.execute_input": "2025-02-08T14:06:27.28856Z",
          "iopub.status.idle": "2025-02-08T14:06:27.297579Z",
          "shell.execute_reply.started": "2025-02-08T14:06:27.288534Z",
          "shell.execute_reply": "2025-02-08T14:06:27.296684Z"
        },
        "id": "fTFs4UnIha2P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Model\n",
        "output_dim = len(labels_to_keep)  # 16\n",
        "vocab_size = base_tokenizer.vocab_size  # 30522\n",
        "embed_dim = base_config.hidden_size  # 768\n",
        "hidden_sz = 256\n",
        "input_sz = hidden_sz * 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "emojify_model = Emojify(embed_dim, output_dim,input_sz,hidden_sz, num_heads=4).to(device)\n",
        "emojify_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:06:36.591463Z",
          "iopub.execute_input": "2025-02-08T14:06:36.5918Z",
          "iopub.status.idle": "2025-02-08T14:06:36.689245Z",
          "shell.execute_reply.started": "2025-02-08T14:06:36.591771Z",
          "shell.execute_reply": "2025-02-08T14:06:36.688558Z"
        },
        "id": "vhCjWnukha2Q",
        "outputId": "e20e8bf2-e563-4b24-ca2e-b353b6761a70"
      },
      "outputs": [
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Emojify(\n  (embedding): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (lstm1): LSTM(768, 256, batch_first=True, bidirectional=True)\n  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (lstm2): LSTM(512, 256, batch_first=True, bidirectional=True)\n  (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  (lstm3): LSTM(512, 256, batch_first=True)\n  (layer_norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (fc): Linear(in_features=256, out_features=16, bias=True)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(emojify_model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:06:42.377445Z",
          "iopub.execute_input": "2025-02-08T14:06:42.377748Z",
          "iopub.status.idle": "2025-02-08T14:06:42.38203Z",
          "shell.execute_reply.started": "2025-02-08T14:06:42.377723Z",
          "shell.execute_reply": "2025-02-08T14:06:42.381094Z"
        },
        "id": "p-xPrIvLha2S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 10\n",
        "best_validation_loss = float('inf')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:06:44.860653Z",
          "iopub.execute_input": "2025-02-08T14:06:44.860957Z",
          "iopub.status.idle": "2025-02-08T14:06:44.864541Z",
          "shell.execute_reply.started": "2025-02-08T14:06:44.860935Z",
          "shell.execute_reply": "2025-02-08T14:06:44.863811Z"
        },
        "id": "7W4GEpZqha2T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    emojify_model.train()\n",
        "    epoch_training_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = emojify_model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "        # loss.backward()\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(emojify_model.parameters(), max_norm=1.0)  # Prevent large updates that can destabilize training\n",
        "        # optimizer.step()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        epoch_training_loss += loss.item()\n",
        "    avg_epoch_training_loss = epoch_training_loss / len(train_loader)\n",
        "\n",
        "    # Validation Step\n",
        "    emojify_model.eval()\n",
        "    epoch_validation_loss = 0\n",
        "    correct_pred_labels = 0\n",
        "    total_labels = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = emojify_model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            epoch_validation_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct_pred_labels += (preds == labels).sum().item()\n",
        "            total_labels += labels.size(0)\n",
        "    avg_epoch_validation_loss = epoch_validation_loss / len(valid_loader)\n",
        "    avg_validation_accuracy = correct_pred_labels / total_labels\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_epoch_training_loss:.4f}, Val Loss = {avg_epoch_validation_loss:.4f}, Val Accuracy = {avg_validation_accuracy:.4f}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if avg_epoch_validation_loss < best_validation_loss:\n",
        "        best_validation_loss = avg_epoch_validation_loss\n",
        "        torch.save(emojify_model.state_dict(), \"best_emojify_model.pth\")\n",
        "        print(\"Model saved!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:10:56.161749Z",
          "iopub.execute_input": "2025-02-08T14:10:56.162076Z",
          "iopub.status.idle": "2025-02-08T14:11:16.507669Z",
          "shell.execute_reply.started": "2025-02-08T14:10:56.162052Z",
          "shell.execute_reply": "2025-02-08T14:11:16.506959Z"
        },
        "id": "7jYuDtHMha2U",
        "outputId": "532793ea-04c2-4f16-f74d-795855c5d62f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: Train Loss = 2.4914, Val Loss = 2.4793, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: Train Loss = 2.5087, Val Loss = 2.4777, Val Accuracy = 0.2500\nModel saved!\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: Train Loss = 2.5006, Val Loss = 2.4803, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: Train Loss = 2.5059, Val Loss = 2.4851, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: Train Loss = 2.4909, Val Loss = 2.4844, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6: Train Loss = 2.4705, Val Loss = 2.4806, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7: Train Loss = 2.4850, Val Loss = 2.4826, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8: Train Loss = 2.4954, Val Loss = 2.4822, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9: Train Loss = 2.5023, Val Loss = 2.4833, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 11.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10: Train Loss = 2.4766, Val Loss = 2.4857, Val Accuracy = 0.2500\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Step\n",
        "emojify_model.load_state_dict(torch.load(\"best_emojify_model.pth\",weights_only=True))\n",
        "emojify_model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = emojify_model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:11:20.496152Z",
          "iopub.execute_input": "2025-02-08T14:11:20.496433Z",
          "iopub.status.idle": "2025-02-08T14:11:20.995431Z",
          "shell.execute_reply.started": "2025-02-08T14:11:20.496412Z",
          "shell.execute_reply": "2025-02-08T14:11:20.994624Z"
        },
        "id": "UIbc5KCiha2V",
        "outputId": "a7019f1c-d739-4759-abf4-24f98c056cf7"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-44-505cb922639f>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  emojify_model.load_state_dict(torch.load(\"best_emojify_model.pth\"))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Accuracy: 0.2576\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, model, tokenizer, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Tokenize and preprocess text\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "    # Get predicted class\n",
        "    predicted_label = torch.argmax(outputs, dim=1).item()\n",
        "    # Convert numerical prediction to label\n",
        "    predicted_sentiment = id2label(predicted_label)\n",
        "\n",
        "    return predicted_sentiment  # Returns numerical sentiment class\n",
        "\n",
        "# Load the trained model\n",
        "emojify_model.load_state_dict(torch.load(\"best_emojify_model.pth\", map_location=device))\n",
        "emojify_model.to(device)\n",
        "\n",
        "# Example text to test\n",
        "new_text = \"I am not sure about ! Everything is going great. ðŸ˜Š\"\n",
        "\n",
        "# Predict sentiment\n",
        "predicted_sentiment = predict_sentiment(new_text, emojify_model, base_tokenizer, device)\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:12:02.867923Z",
          "iopub.execute_input": "2025-02-08T14:12:02.86827Z",
          "iopub.status.idle": "2025-02-08T14:12:03.021208Z",
          "shell.execute_reply.started": "2025-02-08T14:12:02.868244Z",
          "shell.execute_reply": "2025-02-08T14:12:03.020207Z"
        },
        "id": "mrDK_F6gha2X",
        "outputId": "5ca84bb6-583c-43ae-def7-0bc6f88227e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Predicted Sentiment: Happy\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "<ipython-input-47-9e4f6a5ab767>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  emojify_model.load_state_dict(torch.load(\"best_emojify_model.pth\", map_location=device))\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:12:06.861769Z",
          "iopub.execute_input": "2025-02-08T14:12:06.862064Z",
          "iopub.status.idle": "2025-02-08T14:12:06.867504Z",
          "shell.execute_reply.started": "2025-02-08T14:12:06.862042Z",
          "shell.execute_reply": "2025-02-08T14:12:06.866577Z"
        },
        "id": "CIACp02cha2Z",
        "outputId": "3532f450-21fe-4caf-84f3-4cfa2c3f81ee"
      },
      "outputs": [
        {
          "execution_count": 48,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<bound method ClassLabel.int2str of ClassLabel(names=['Positive', 'Joyful', 'Disappointed', 'Worried', 'Grateful', 'Indifferent', 'Sad', 'Angry', 'Relieved', 'Excited', 'Anxious', 'Satisfied', 'Happy', 'Nostalgic', 'Inspired', 'Impressed'], id=None)>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Classification"
      ],
      "metadata": {
        "id": "OQO4elfeha2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # %%\n",
        "\n",
        "# import itertools\n",
        "# from tqdm import tqdm\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from transformers import BertJapaneseTokenizer, BertForTokenClassification\n",
        "# import pytorch_lightning as pl\n",
        "\n",
        "# # from torch.utils.data import DataLoader\n",
        "# # import from_XML_to_json as XtC\n",
        "# # import random\n",
        "# # import json\n",
        "# # import unicodedata\n",
        "# # import pandas as pd\n",
        "\n",
        "# # %%\n",
        "# # 8-16\n",
        "# # PyTorch Lightning model\n",
        "# class BertForTokenClassification_pl(pl.LightningModule):\n",
        "\n",
        "#     def __init__(self, model_name, num_labels, lr):\n",
        "#         super().__init__()\n",
        "#         self.save_hyperparameters()\n",
        "#         self.bert_tc = BertForTokenClassification.from_pretrained(\n",
        "#             model_name,\n",
        "#             num_labels=num_labels\n",
        "#         )\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         output = self.bert_tc(**batch)\n",
        "#         loss = output.loss\n",
        "#         self.log('train_loss', loss)\n",
        "#         return loss\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         output = self.bert_tc(**batch)\n",
        "#         val_loss = output.loss\n",
        "#         self.log('val_loss', val_loss)\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "\n",
        "\n",
        "# # %%\n",
        "# class NER_tokenizer_BIO(BertJapaneseTokenizer):\n",
        "\n",
        "#     # The number of categories of named entities `num_entity_type` at initialization\n",
        "#     # make it accept.\n",
        "#     def __init__(self, *args, **kwargs):\n",
        "#         self.num_entity_type = kwargs.pop('num_entity_type')\n",
        "#         super().__init__(*args, **kwargs)\n",
        "\n",
        "#     def encode_plus_tagged(self, text, entities, max_length):\n",
        "#         \"\"\"\n",
        "#         Given a sentence and named entities,\n",
        "#         Encode and create a label string.\n",
        "#         \"\"\"\n",
        "#         # Divide the text before and after the named entity and label each.\n",
        "#         splitted = [] # Add the string after division\n",
        "#         position = 0\n",
        "\n",
        "#         for entity in entities:\n",
        "#             start = entity['span'][0]\n",
        "#             end = entity['span'][1]\n",
        "#             label = entity['type_id']\n",
        "#             splitted.append({'text':text[position:start], 'label':0})\n",
        "#             splitted.append({'text':text[start:end], 'label':label})\n",
        "#             position = end\n",
        "#         splitted.append({'text': text[position:], 'label':0})\n",
        "#         splitted = [ s for s in splitted if s['text'] ]\n",
        "\n",
        "#         # Tokenize and label each segmented sentence\n",
        "#         tokens = []\n",
        "#         labels = []\n",
        "#         for s in splitted:\n",
        "#             tokens_splitted = self.tokenize(s['text'])\n",
        "#             label = s['label']\n",
        "#             if label > 0: # å›ºæœ‰è¡¨ç¾\n",
        "#                 # First, assign I-tags to all tokens\n",
        "#                 # Number order O-tag: 0, B-tag: 1 ~ number of tags, I-tag: number of tags ~\n",
        "#                 labels_splitted =  \\\n",
        "#                     [ label + self.num_entity_type ] * len(tokens_splitted)\n",
        "#                 # Make the first token a B-tag\n",
        "#                 labels_splitted[0] = label\n",
        "#             else:\n",
        "#                 labels_splitted =  [0] * len(tokens_splitted)\n",
        "\n",
        "#             tokens.extend(tokens_splitted)\n",
        "#             labels.extend(labels_splitted)\n",
        "\n",
        "#         # Encode it and put it into a format that can be input to BERT.\n",
        "#         input_ids = self.convert_tokens_to_ids(tokens)\n",
        "#         encoding = self.prepare_for_model(\n",
        "#             input_ids,\n",
        "#             max_length=max_length,\n",
        "#             padding='max_length',\n",
        "#             truncation=True\n",
        "#         )\n",
        "\n",
        "#         # Add Special Tokens to Labels\n",
        "#         # Cut by max_length and put labels before and after to add [CLS] and [SEP]\n",
        "#         labels = [0] + labels[:max_length-2] + [0]\n",
        "#         # If it is less than max_length, add the missing part to the end\n",
        "#         labels = labels + [0]*( max_length - len(labels) )\n",
        "#         encoding['labels'] = labels\n",
        "\n",
        "#         return encoding\n",
        "\n",
        "#     def encode_plus_untagged(\n",
        "#         self, text, max_length=None, return_tensors=None\n",
        "#     ):\n",
        "#         \"\"\"\n",
        "#         Tokenize the sentences and identify the position of each token in the sentence.\n",
        "#         Same as encode_plus_untagged in IO method tokenizer\n",
        "#         \"\"\"\n",
        "#         # Tokenize the text and associate each token with the character string in the text.\n",
        "#         tokens = [] # Add tokens.\n",
        "#         tokens_original = [] # Add the character strings in the sentence corresponding to the token.\n",
        "#         words = self.word_tokenizer.tokenize(text) # Split into words with MeCab\n",
        "#         for word in words:\n",
        "#             # Split word into subwords\n",
        "#             tokens_word = self.subword_tokenizer.tokenize(word)\n",
        "#             tokens.extend(tokens_word)\n",
        "#             if tokens_word[0] == '[UNK]': # Dealing with unknown words\n",
        "#                 tokens_original.append(word)\n",
        "#             else:\n",
        "#                 tokens_original.extend([\n",
        "#                     token.replace('##','') for token in tokens_word\n",
        "#                 ])\n",
        "\n",
        "#         # Find the position of each token in the sentence. (considering blank positions)\n",
        "#         position = 0\n",
        "#         spans = [] # Add token positions.\n",
        "#         for token in tokens_original:\n",
        "#             l = len(token)\n",
        "#             while 1:\n",
        "#                 if token != text[position:position+l]:\n",
        "#                     position += 1\n",
        "#                 else:\n",
        "#                     spans.append([position, position+l])\n",
        "#                     position += l\n",
        "#                     break\n",
        "\n",
        "#         # Encode it and put it into a format that can be input to BERT.\n",
        "#         input_ids = self.convert_tokens_to_ids(tokens)\n",
        "#         encoding = self.prepare_for_model(\n",
        "#             input_ids,\n",
        "#             max_length=max_length,\n",
        "#             padding='max_length' if max_length else False,\n",
        "#             truncation=True if max_length else False\n",
        "#         )\n",
        "#         sequence_length = len(encoding['input_ids'])\n",
        "#         # Added dummy span for special token [CLS].\n",
        "#         spans = [[-1, -1]] + spans[:sequence_length-2]\n",
        "#         # Added dummy spans for special tokens [SEP], [PAD].\n",
        "#         spans = spans + [[-1, -1]] * ( sequence_length - len(spans) )\n",
        "\n",
        "#         # Make it a torch.Tensor if necessary.\n",
        "#         if return_tensors == 'pt':\n",
        "#             encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
        "\n",
        "#         return encoding, spans\n",
        "\n",
        "#     @staticmethod\n",
        "#     def Viterbi(scores_bert, num_entity_type, penalty=10000):\n",
        "#         \"\"\"\n",
        "#         Find the optimal solution with the Viterbi algorithm.\n",
        "#         \"\"\"\n",
        "#         m = 2*num_entity_type + 1\n",
        "#         penalty_matrix = np.zeros([m, m])\n",
        "#         for i in range(m):\n",
        "#             for j in range(1+num_entity_type, m):\n",
        "#                 if not ( (i == j) or (i+num_entity_type == j) ):\n",
        "#                     penalty_matrix[i,j] = penalty\n",
        "#         path = [ [i] for i in range(m) ]\n",
        "#         scores_path = scores_bert[0] - penalty_matrix[0,:]\n",
        "#         scores_bert = scores_bert[1:]\n",
        "\n",
        "\n",
        "\n",
        "#         for scores in scores_bert:\n",
        "#             assert len(scores) == 2*num_entity_type + 1\n",
        "#             score_matrix = np.array(scores_path).reshape(-1,1) \\\n",
        "#                 + np.array(scores).reshape(1,-1) \\\n",
        "#                 - penalty_matrix\n",
        "#             scores_path = score_matrix.max(axis=0)\n",
        "#             argmax = score_matrix.argmax(axis=0)\n",
        "#             path_new = []\n",
        "#             for i, idx in enumerate(argmax):\n",
        "#                 path_new.append( path[idx] + [i] )\n",
        "#             path = path_new\n",
        "\n",
        "#         labels_optimal = path[np.argmax(scores_path)]\n",
        "#         return labels_optimal\n",
        "\n",
        "#     def convert_bert_output_to_entities(self, text, scores, spans):\n",
        "#         \"\"\"\n",
        "#         Obtain named entities from sentences, classification scores, and the position of each token.\n",
        "#         Classification scores are two-dimensional arrays of size (series length, number of labels)\n",
        "#         \"\"\"\n",
        "#         assert len(spans) == len(scores)\n",
        "#         num_entity_type = self.num_entity_type\n",
        "\n",
        "#         # Remove parts corresponding to special tokens\n",
        "#         scores = [score for score, span in zip(scores, spans) if span[0]!=-1]\n",
        "#         spans = [span for span in spans if span[0]!=-1]\n",
        "\n",
        "#         # Determine the predicted value of the label with the Viterbi algorithm.\n",
        "#         labels = self.Viterbi(scores, num_entity_type)\n",
        "\n",
        "#         # Tokens with the same label are grouped together to extract named entities.\n",
        "#         entities = []\n",
        "#         for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
        "#             group = list(group)\n",
        "#             start = spans[group[0][0]][0]\n",
        "#             end = spans[group[-1][0]][1]\n",
        "\n",
        "#             if label != 0: # if it is a named entity\n",
        "#                 if 1 <= label <= num_entity_type:\n",
        "#                      # Add new entity if label is `B-`\n",
        "#                     entity = {\n",
        "#                         \"name\": text[start:end],\n",
        "#                         \"span\": [start, end],\n",
        "#                         \"type_id\": label\n",
        "#                     }\n",
        "#                     entities.append(entity)\n",
        "#                 else:\n",
        "#                     # If the label is `I-`, update the last entity\n",
        "#                     entity['span'][1] = end\n",
        "#                     entity['name'] = text[entity['span'][0]:entity['span'][1]]\n",
        "\n",
        "#         return entities\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-08T14:03:02.378234Z",
          "iopub.status.idle": "2025-02-08T14:03:02.37847Z",
          "shell.execute_reply": "2025-02-08T14:03:02.378373Z"
        },
        "id": "KfH0r6cLha2b"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}