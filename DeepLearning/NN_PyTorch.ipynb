{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# with open(\"/kaggle/input/Alice-adventure/allice.txt\",\"r\") as f:\n#     text = f.read().replace(\"\\n\\n\",\" \").replace(\"\\n\",\" \").replace(\"*\",\"   \")\n#     chapters = text.split(\"CHAPTER\")[1:]\n\n# chapter1 = chapters[0][2:][1:] # skipping first character of the chapter\n\n# nlp = spacy.load(\"en_core_web_sm\")\n# doc = nlp(chapter1)\n# sentences = list(doc.sents)\n# print(sentences[0])\n# # print(chapters[0])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LLaMA\n\nhttps://github.com/facebookresearch/llama","metadata":{}},{"cell_type":"code","source":"# \n# Tokenizer\nfrom sentencepiece import SentencePieceProcessor # This has encode-decode\nclass Tokenizer:\n    def __init__(self, model_path: str):\n        1. reload tokenizer # self.sp_model = SentencePieceProcessor(model_file=model_path)\n        2. BOS / EOS token IDs\n    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n        1. encode model & add to bos_id,eos_id \n    def decode(self, t: List[int]) -> str:\n        1. decode model\n\n# Generation\nfrom llama.tokenizer import Tokenizer\nfrom llama.model import Transformer\n\nclass LLaMA:\n    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n    def sample_top_p(probs, p):\n        1. probs_sort, probs_idx by sorting probs in descending ,dim=-1\n        2. probs_sum & mask = checker probs_sum - probs_sort > p , probs_sort[mask]=0.0\n        3. probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n        4. next_token = torch.multinomial(probs_sort, num_samples=1)\n           next_token = torch.gather(probs_idx, -1, next_token)\n    \n    def generate(self,prompts: List[str],max_gen_len: int,temperature: float = 0.8,top_p: float = 0.95,) -> List[str]:\n        \n        1. bcz=prompts size <= params.max_batch_size size\n        2. prompt_tokens = [tokenize each prompts]\n        3. get min_prompt_size & max_prompt_size for each prompts : List[str]\n        4. get total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n        5. tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n            tokens[k, : len(t)] = torch.tensor(t).long() for each prompt_tokens\n            input_text_mask = tokens not having tokenized padding\n            assign start_pos , prev_pos\n        6. for cur_pos in range(start_pos,total_len):\n                logits = forward pass with (tokens[:, prev_pos:cur_pos], prev_pos)\n                if temperature > 0:\n                    probs = use softmax with logits & temperature, get dim=-1\n                    next_token = sample_top_p(probs, top_p).reshape(-1)\n                else:\n                    next_token = argmax(logits,dim=-1).reshape(-1)\n            \n                only replace tokens[:, cur_pos] if prompt has already been generated -> input_text_mask[:, cur_pos] : bool \n                tokens[:, cur_pos] = next_token\n                prev_pos = cur_pos\n        7. get each token for tokens.tolist() & cut to max_gen_len or eos_id if any then decode","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model\nimport fairscale.nn.model_parallel.initialize as fs_init\nfrom fairscale.nn.model_parallel.layers import (\n    ParallelEmbedding,\n    RowParallelLinear,\n    ColumnParallelLinear,\n)\n\n@dataclass\nclass ModelArgs:\n    ...\n    vocab_size: int = -1  # defined later by tokenizer\n    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n        \nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__(),assign eps & self.weight = nn.Parameter(torch.ones(dim))\n    def _norm(self, x): return x* rsqrt(...)\n    def forward(self, x): return output * weight # output = _norm(x)\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    1. pre-computation for rotary embedding , use torch.polar() to get dtype=complex64 \n\ndef reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    # here 0 <= 1 < ndim=x.ndim , freqs_cis must have same dimension as x except for 1 & ndim-1\n    return freqs_cis.view(*shape)\n\ndef apply_rotary_emb(xq: torch.Tensor,xk: torch.Tensor,) -> Tuple[torch.Tensor, torch.Tensor]:\n    1. find xq_ & xk_ by using proper field from xq & xk to get real ,img components\n    2. freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n    3. xq_out,xk_out = real components but with rotatry embeddings\n\nclass Attention(nn.Module):\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.n_local_heads = args.n_heads // fs_init.get_model_parallel_world_size()\n        self.head_dim = args.dim // args.n_heads\n        1. use ColumnParallelLinear toget wq,wk,wv,wo; set cache_k,cache_v\n\n    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n        bsz, seqlen, _ = x.shape,xq, xk, xv \n        1. xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis) & put into cache_k(xq),cache_c(xq)\n        2. get keys,values,scores -> add score + mask -> Softmax => score\n        3. return wo(score @ values)\n\nclass FeedForward(nn.Module):\n    def __init__(self,dim: int,hidden_dim: int,multiple_of: int,):\n        super().__init__(),hidden_dim,assign w1,w2,w3 using ColumnParallelLinear\n\n    def forward(self, x): return sigmoid linear unit applied on w1,w2,w3\n\n    \nclass TransformerBlock(nn.Module):\n    def __init__(self, layer_id: int, args: ModelArgs):\n        super().__init__(),get attention,feed-forward,ffn_norm & other head,dim parameters\n\n    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n        # both are for forward pass\n        1. calculate h = x + attention(attention_norm(x),start_pos,freqs_cis, mask)\n        2. return out = h + feed_forward(ffn_norm(h))\n\n\nclass Transformer(nn.Module):\n    def __init__(self, params: ModelArgs):\n        super().__init__(),assign params,vocab_size,tok_embed,layers,norm ->freqs_cis,output\n        \n    @torch.inference_mode()\n    def forward(self, tokens: torch.Tensor, start_pos: int):\n        _bsz, seqlen = tokens.shape\n        1. h = tok_embeddings(token) -> freqs_cis(h)\n        2. seqlen > 1: add mask() not the complete but triu part\n        3. h = layer(h, start_pos, freqs_cis, mask) for each layers -> norm(h)\n        4. output = compute with last logits of output(h[:, -1, :])\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HuggingFace - Transformers\n\nall models ->  https://github.com/huggingface/transformers/tree/main/src/transformers/models\n\nBERT (https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert)\n\n        1. model - https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/bert/modeling_bert.py\n        2. tokenizer - https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/bert/tokenization_bert_fast.py\n\nGPT-2 (https://github.com/huggingface/transformers/tree/main/src/transformers/models/gpt2)\n\nAlpaca (https://github.com/tatsu-lab/stanford_alpaca)\n\nLoRA (https://github.com/microsoft/LoRA)\n\nGPT4ALL (https://github.com/nomic-ai/gpt4all) -> advanced not needed\n\n### Transformer for Long Sentences -> LongFormer\n\nLongFormer (https://github.com/huggingface/transformers/tree/main/src/transformers/models/longformer)","metadata":{}},{"cell_type":"markdown","source":"# HuggingFace - Tokenizers\n\nPre-Tokenizer : Breaks sentence into word to avoid overlap then we can use Tokenizer\n\nTokenizer Pipeline\n-----------------------\n\n    normalization\n    pre-tokenization\n    model\n            models.BPE\n            models.Unigram\n            models.WordLevel\n            models.WordPiece\n    post-processing\n    \n BPE (https://github.com/soaxelbrooke/python-bpe)\n ","metadata":{}},{"cell_type":"markdown","source":"# HuggingFace - Diffusers \n\nall diffusers -> https://github.com/huggingface/diffusers/tree/main/src/diffusers/models","metadata":{}},{"cell_type":"code","source":"#","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:07:17.828607Z","iopub.execute_input":"2023-05-25T19:07:17.829047Z","iopub.status.idle":"2023-05-25T19:07:23.389487Z","shell.execute_reply.started":"2023-05-25T19:07:17.829011Z","shell.execute_reply":"2023-05-25T19:07:23.388413Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 64\nnum_classes = 10\nlearning_rate = 0.001\nepochs = 10","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:07:42.546786Z","iopub.execute_input":"2023-05-25T19:07:42.548131Z","iopub.status.idle":"2023-05-25T19:07:42.554213Z","shell.execute_reply.started":"2023-05-25T19:07:42.548094Z","shell.execute_reply":"2023-05-25T19:07:42.552877Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_dataset = datasets.MNIST(root='data/',train=True, transform=transforms.ToTensor(),download=True)\ntrain_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n\ntest_dataset = datasets.MNIST(root='data/',train=False, transform=transforms.ToTensor(),download=True)\ntest_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:07:44.397165Z","iopub.execute_input":"2023-05-25T19:07:44.397534Z","iopub.status.idle":"2023-05-25T19:07:45.469361Z","shell.execute_reply.started":"2023-05-25T19:07:44.397505Z","shell.execute_reply":"2023-05-25T19:07:45.468379Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 94631985.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 71047327.76it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1648877/1648877 [00:00<00:00, 26605517.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 5719162.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# NN","metadata":{}},{"cell_type":"code","source":"input_size = 784 #[28X28]\nhidden_layers = 50\n","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:07:48.824374Z","iopub.execute_input":"2023-05-25T19:07:48.824868Z","iopub.status.idle":"2023-05-25T19:07:48.828987Z","shell.execute_reply.started":"2023-05-25T19:07:48.824835Z","shell.execute_reply":"2023-05-25T19:07:48.827980Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class NN(nn.Module):\n    def __init__(self,input_size,hidden_layers,num_classes):\n        super(NN,self).__init__()\n        self.fc1 = nn.Linear(input_size,hidden_layers)\n        self.fc2 = nn.Linear(hidden_layers,num_classes)\n    def forward(self,x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    \n# model = NN(784,50,10)\n# x = torch.rand(64,784).to(device)\n# model(x).shape","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:07:51.366069Z","iopub.execute_input":"2023-05-25T19:07:51.366499Z","iopub.status.idle":"2023-05-25T19:07:51.378434Z","shell.execute_reply.started":"2023-05-25T19:07:51.366468Z","shell.execute_reply":"2023-05-25T19:07:51.377434Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = NN(input_size,hidden_layers,num_classes).to(device)\n\noptimizer = optim.SGD(params=model.parameters(),lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2,factor=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:07:55.673435Z","iopub.execute_input":"2023-05-25T19:07:55.673796Z","iopub.status.idle":"2023-05-25T19:07:59.068675Z","shell.execute_reply.started":"2023-05-25T19:07:55.673766Z","shell.execute_reply":"2023-05-25T19:07:59.067725Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    losses = []\n    for batch_idx,(data,target) in tqdm(enumerate(train_loader)):\n        data = data.to(device)\n        target = target.to(device)\n        data = data.reshape(data.shape[0],-1) # flatten : (64,1X768)\n        score = model(data) # [64,10]\n        loss = criterion(score,target)\n        losses.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    mean_loss = sum(losses)/len(losses)\n    scheduler.step(mean_loss)\n    print(f\"Cost for epoch={epoch} is {mean_loss:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:08:05.169419Z","iopub.execute_input":"2023-05-25T19:08:05.169812Z","iopub.status.idle":"2023-05-25T19:09:28.294540Z","shell.execute_reply.started":"2023-05-25T19:08:05.169779Z","shell.execute_reply":"2023-05-25T19:09:28.293436Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"938it [00:10, 91.50it/s] \n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=0 is 2.25\n","output_type":"stream"},{"name":"stderr","text":"938it [00:08, 112.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=1 is 2.09\n","output_type":"stream"},{"name":"stderr","text":"938it [00:07, 118.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=2 is 1.85\n","output_type":"stream"},{"name":"stderr","text":"938it [00:07, 117.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=3 is 1.56\n","output_type":"stream"},{"name":"stderr","text":"938it [00:07, 117.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=4 is 1.29\n","output_type":"stream"},{"name":"stderr","text":"938it [00:08, 112.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=5 is 1.07\n","output_type":"stream"},{"name":"stderr","text":"938it [00:07, 118.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=6 is 0.92\n","output_type":"stream"},{"name":"stderr","text":"938it [00:08, 117.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=7 is 0.80\n","output_type":"stream"},{"name":"stderr","text":"938it [00:07, 118.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=8 is 0.72\n","output_type":"stream"},{"name":"stderr","text":"938it [00:08, 114.12it/s]","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=9 is 0.66\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef check_accuracy(model,loader):\n    if loader.dataset.train:\n        print(\"Training Accuracy\")\n    else:\n        print(\"Testing Accuracy\")\n    samples = 0\n    corrects = 0\n    model.eval()\n    with torch.no_grad():\n        for x,y in tqdm(loader):\n            x = x.to(device)\n            y=y.to(device)\n            x = x.reshape(x.shape[0],-1)\n            score = model(x) # [64,10]\n            _,pred_index = score.max(1) #[64] ,don't care about the value\n            corrects += (pred_index==y).sum()\n            samples += pred_index.size(0)\n        print(f\"Correct = {corrects}/{samples} Accuracy = {float(corrects)/float(samples)*100:.2f}\")\n    model.train()\n    \n    \ncheck_accuracy(model,train_loader)\ncheck_accuracy(model,test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:10:05.746033Z","iopub.execute_input":"2023-05-25T19:10:05.746573Z","iopub.status.idle":"2023-05-25T19:10:14.641277Z","shell.execute_reply.started":"2023-05-25T19:10:05.746536Z","shell.execute_reply":"2023-05-25T19:10:14.640177Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training Accuracy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:07<00:00, 124.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Correct = 51065/60000 Accuracy = 85.11\nTesting Accuracy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 115.79it/s]","output_type":"stream"},{"name":"stdout","text":"Correct = 8607/10000 Accuracy = 86.07\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# CNN","metadata":{}},{"cell_type":"code","source":"n_channels = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self,n_channels,num_classes):\n        super(CNN,self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=n_channels,out_channels=8,kernel_size=(3,3),\n                              padding=(1,1),stride=(1,1))\n        self.pool = nn.MaxPool2d(kernel_size=(2,2),stride=(2,2))\n        self.conv2 = nn.Conv2d(in_channels=8,out_channels=16,kernel_size=(3,3),\n                              padding=(1,1),stride=(1,1))\n        self.fc = nn.Linear(16*7*7,num_classes)\n        \n    def forward(self,x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.reshape(x.shape[0],-1)\n        x = self.fc(x)\n        return x\n    \n# model = CNN(1,10)\n# x = torch.rand(64,1,28,28).to(device)\n# model(x).shape  # [64,10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CNN(n_channels,num_classes).to(device)\n\noptimizer = optim.SGD(params=model.parameters(),lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2,factor=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    losses = []\n    for batch_idx,(data,target) in tqdm(enumerate(train_loader)):\n        data = data.to(device)\n        target = target.to(device)\n        #data = data.reshape(data.shape[0],-1) # flatten : (64,1X768)\n        score = model(data) # [64,10]\n        loss = criterion(score,target)\n        losses.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    mean_loss = sum(losses)/len(losses)\n    scheduler.step(mean_loss)\n    print(f\"Cost for epoch={epoch} is {mean_loss:.2f}\")\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        \ndef check_accuracy(model,loader):\n    if loader.dataset.train:\n        print(\"Training Accuracy\")\n    else:\n        print(\"Testing Accuracy\")\n    samples = 0\n    corrects = 0\n    model.eval()\n    with torch.no_grad():\n        for x,y in tqdm(loader):\n            x = x.to(device)\n            y=y.to(device)\n            #x = x.reshape(x.shape[0],-1)\n            score = model(x) # [64,10]\n            _,pred_index = score.max(1) #[64] , don't care about the value\n            corrects += (pred_index==y).sum()\n            samples += pred_index.size(0)\n        print(f\"Correct = {corrects}/{samples} Accuracy = {float(corrects)/float(samples)*100:.2f}\")\n    model.train()\n    \n    \n    \ncheck_accuracy(model,train_loader)\ncheck_accuracy(model,test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN","metadata":{}},{"cell_type":"code","source":"input_size = 28\nsequence_length = 28\nnum_layers = 2\nhidden_layers = 256","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:10:25.244811Z","iopub.execute_input":"2023-05-25T19:10:25.245212Z","iopub.status.idle":"2023-05-25T19:10:25.250014Z","shell.execute_reply.started":"2023-05-25T19:10:25.245181Z","shell.execute_reply":"2023-05-25T19:10:25.248986Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self,input_size,hidden_layers,num_layers,num_classes):\n        super(RNN,self).__init__()\n        self.hidden_layers = hidden_layers\n        self.num_layers = num_layers\n        # [NXtime_sequenceXfeatures] -> batch_first=True\n        self.rnn = nn.RNN(input_size,hidden_layers,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_layers*sequence_length,num_classes)\n        \n    def forward(self,x):\n        h0 = torch.zeros(self.num_layers,x.size(0),self.hidden_layers).to(device)\n        out,_= self.rnn(x,h0) # don't need to store hidden_state\n        out = out.reshape(out.shape[0],-1) #[64,256*sequence_length]\n        out = self.fc(out)\n        return out\n    \n# model = RNN(28,256,2,10)\n# x = torch.rand(64,28,28).to(device)\n# model(x).shape","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:10:28.033121Z","iopub.execute_input":"2023-05-25T19:10:28.033793Z","iopub.status.idle":"2023-05-25T19:10:28.047766Z","shell.execute_reply.started":"2023-05-25T19:10:28.033754Z","shell.execute_reply":"2023-05-25T19:10:28.046747Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = RNN(input_size,hidden_layers,num_layers,num_classes).to(device)\n\noptimizer = optim.SGD(params=model.parameters(),lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2,factor=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:10:31.639591Z","iopub.execute_input":"2023-05-25T19:10:31.640132Z","iopub.status.idle":"2023-05-25T19:10:33.225943Z","shell.execute_reply.started":"2023-05-25T19:10:31.640101Z","shell.execute_reply":"2023-05-25T19:10:33.224964Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    losses = []\n    for batch_idx,(data,target) in tqdm(enumerate(train_loader)):\n        data = data.to(device).squeeze(1) # [64,1,28,28] -> [64,1*28,28]\n        target = target.to(device)\n        #data = data.reshape(data.shape[0],-1) # flatten : (64,1X768)\n        score = model(data) # [64,10]\n        loss = criterion(score,target)\n        losses.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    mean_loss = sum(losses)/len(losses)\n    scheduler.step(mean_loss)\n    print(f\"Cost for epoch={epoch} is {mean_loss:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:10:38.687519Z","iopub.execute_input":"2023-05-25T19:10:38.687860Z","iopub.status.idle":"2023-05-25T19:12:26.472207Z","shell.execute_reply.started":"2023-05-25T19:10:38.687831Z","shell.execute_reply":"2023-05-25T19:12:26.471244Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"938it [00:11, 84.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=0 is 2.10\n","output_type":"stream"},{"name":"stderr","text":"938it [00:10, 85.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=1 is 1.09\n","output_type":"stream"},{"name":"stderr","text":"938it [00:10, 89.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=2 is 0.66\n","output_type":"stream"},{"name":"stderr","text":"938it [00:10, 89.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=3 is 0.52\n","output_type":"stream"},{"name":"stderr","text":"938it [00:11, 84.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=4 is 0.46\n","output_type":"stream"},{"name":"stderr","text":"938it [00:10, 90.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=5 is 0.42\n","output_type":"stream"},{"name":"stderr","text":"938it [00:10, 89.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=6 is 0.39\n","output_type":"stream"},{"name":"stderr","text":"938it [00:11, 83.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=7 is 0.37\n","output_type":"stream"},{"name":"stderr","text":"938it [00:10, 88.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=8 is 0.35\n","output_type":"stream"},{"name":"stderr","text":"938it [00:10, 88.52it/s]","output_type":"stream"},{"name":"stdout","text":"Cost for epoch=9 is 0.34\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef check_accuracy(model,loader):\n    if loader.dataset.train:\n        print(\"Training Accuracy\")\n    else:\n        print(\"Testing Accuracy\")\n    samples = 0\n    corrects = 0\n    model.eval()\n    with torch.no_grad():\n        for x,y in tqdm(loader):\n            x = x.to(device).squeeze(1)\n            y=y.to(device)\n            #x = x.reshape(x.shape[0],-1)\n            score = model(x) # [64,10]\n            _,pred_index = score.max(1) #[64] , don't care about the value\n            corrects += (pred_index==y).sum()\n            samples += pred_index.size(0)\n        print(f\"Correct = {corrects}/{samples} Accuracy = {float(corrects)/float(samples)*100:.2f}\")\n    model.train()\n    \n    \n    \ncheck_accuracy(model,train_loader)\ncheck_accuracy(model,test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:12:38.031525Z","iopub.execute_input":"2023-05-25T19:12:38.031891Z","iopub.status.idle":"2023-05-25T19:12:47.584511Z","shell.execute_reply.started":"2023-05-25T19:12:38.031861Z","shell.execute_reply":"2023-05-25T19:12:47.583599Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Training Accuracy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:08<00:00, 114.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Correct = 54203/60000 Accuracy = 90.34\nTesting Accuracy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 115.73it/s]","output_type":"stream"},{"name":"stdout","text":"Correct = 9053/10000 Accuracy = 90.53\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_size, hidden_layers, num_layers, num_classes):\n        super(LSTM, self).__init__()\n        self.hidden_layers = hidden_layers\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_layers, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_layers, num_classes)\n\n    def forward(self, x):\n        # Set initial hidden and cell states\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_layers).to(device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_layers).to(device)\n        # out: tensor of shape (batch_size, seq_length, hidden_size)\n        out, _ = self.lstm( x, (h0, c0))  \n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out) # no need to reshape if we're taking only last layer\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:42:46.582518Z","iopub.execute_input":"2023-05-25T19:42:46.582898Z","iopub.status.idle":"2023-05-25T19:42:46.590505Z","shell.execute_reply.started":"2023-05-25T19:42:46.582867Z","shell.execute_reply":"2023-05-25T19:42:46.589617Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"model = LSTM(input_size,hidden_layers,num_layers,num_classes).to(device)\n\noptimizer = optim.SGD(params=model.parameters(),lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2,factor=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:42:51.015861Z","iopub.execute_input":"2023-05-25T19:42:51.016565Z","iopub.status.idle":"2023-05-25T19:42:51.034238Z","shell.execute_reply.started":"2023-05-25T19:42:51.016529Z","shell.execute_reply":"2023-05-25T19:42:51.033076Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    losses = []\n    for batch_idx,(data,target) in tqdm(enumerate(train_loader)):\n        data = data.to(device).squeeze(1) # [64,1,28,28] -> [64,1*28,28]\n        target = target.to(device)\n        #data = data.reshape(data.shape[0],-1) # flatten : (64,1X768)\n        score = model(data) # [64,10]\n        loss = criterion(score,target)\n        losses.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    mean_loss = sum(losses)/len(losses)\n    scheduler.step(mean_loss)\n    print(f\"Cost for epoch={epoch} is {mean_loss:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:42:55.064354Z","iopub.execute_input":"2023-05-25T19:42:55.064705Z","iopub.status.idle":"2023-05-25T19:42:55.198742Z","shell.execute_reply.started":"2023-05-25T19:42:55.064676Z","shell.execute_reply":"2023-05-25T19:42:55.197332Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[68], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#data = data.reshape(data.shape[0],-1) # flatten : (64,1X768)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [64,10]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(score,target)\n\u001b[1;32m      9\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[66], line 16\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm( x, (h0, c0))  \n\u001b[1;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# no need to reshape if we're taking only last layer\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x7168 and 256x10)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (64x7168 and 256x10)","output_type":"error"}]},{"cell_type":"code","source":"def check_accuracy(model,loader):\n    if loader.dataset.train:\n        print(\"Training Accuracy\")\n    else:\n        print(\"Testing Accuracy\")\n    samples = 0\n    corrects = 0\n    model.eval()\n    with torch.no_grad():\n        for x,y in tqdm(loader):\n            x = x.to(device).squeeze(1)\n            y=y.to(device)\n            #x = x.reshape(x.shape[0],-1)\n            score = model(x) # [64,10]\n            _,pred_index = score.max(1) #[64] , don't care about the value\n            corrects += (pred_index==y).sum()\n            samples += pred_index.size(0)\n        print(f\"Correct = {corrects}/{samples} Accuracy/epoch = {float(corrects)/float(samples)*100:.2f}\")\n    model.train()\n    \n    \n    \ncheck_accuracy(model,train_loader)\ncheck_accuracy(model,test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:14:56.858796Z","iopub.execute_input":"2023-05-25T19:14:56.859681Z","iopub.status.idle":"2023-05-25T19:15:06.593315Z","shell.execute_reply.started":"2023-05-25T19:14:56.859638Z","shell.execute_reply":"2023-05-25T19:15:06.592247Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Training Accuracy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:08<00:00, 113.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Correct = 6742/60000 Accuracy/epoch = 11.24\nTesting Accuracy\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 110.95it/s]","output_type":"stream"},{"name":"stdout","text":"Correct = 1135/10000 Accuracy/epoch = 11.35\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Bidirectional LSTM","metadata":{}},{"cell_type":"code","source":"class BLSTM(nn.Module):\n    def __init__(self, input_size, hidden_layers, num_layers, num_classes):\n        super(BLSTM, self).__init__()\n        self.hidden_layers = hidden_layers\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_layers, num_layers, batch_first=True,bidirectional=True)\n        self.fc = nn.Linear(hidden_layers * 2, num_classes)\n\n    def forward(self, x):\n        # Set initial hidden and cell states\n        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_layers).to(device)\n        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_layers).to(device)\n        # out: tensor of shape (batch_size, seq_length, hidden_size)\n        out, _ = self.lstm(x, (h0, c0))  \n        #out = out.reshape(out.shape[0], -1)\n        out = self.fc(out[:,-1,:]) \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:21:12.332017Z","iopub.execute_input":"2023-05-25T19:21:12.332400Z","iopub.status.idle":"2023-05-25T19:21:12.341818Z","shell.execute_reply.started":"2023-05-25T19:21:12.332370Z","shell.execute_reply":"2023-05-25T19:21:12.340633Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model = BLSTM(input_size,hidden_layers,num_layers,num_classes).to(device)\n\noptimizer = optim.SGD(params=model.parameters(),lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=2,factor=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:21:15.910848Z","iopub.execute_input":"2023-05-25T19:21:15.911220Z","iopub.status.idle":"2023-05-25T19:21:15.948010Z","shell.execute_reply.started":"2023-05-25T19:21:15.911188Z","shell.execute_reply":"2023-05-25T19:21:15.947100Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    losses = []\n    for batch_idx,(data,target) in tqdm(enumerate(train_loader)):\n        data = data.to(device).squeeze(1) # [64,1,28,28] -> [64,1*28,28]\n        target = target.to(device)\n        #data = data.reshape(data.shape[0],-1) # flatten : (64,1X768)\n        score = model(data) # [64,10]\n        loss = criterion(score,target)\n        losses.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    mean_loss = sum(losses)/len(losses)\n    scheduler.step(mean_loss)\n    print(f\"Cost for epoch={epoch} is {mean_loss:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-25T19:21:37.660846Z","iopub.execute_input":"2023-05-25T19:21:37.661860Z","iopub.status.idle":"2023-05-25T19:21:37.888948Z","shell.execute_reply.started":"2023-05-25T19:21:37.661815Z","shell.execute_reply":"2023-05-25T19:21:37.887309Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten : (64,1X768)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [64,10]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(score,target)\n\u001b[1;32m      9\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[30], line 14\u001b[0m, in \u001b[0;36mBLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# out: tensor of shape (batch_size, seq_length, hidden_size)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     15\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out) \n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:803\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    801\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 803\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    804\u001b[0m         hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"],"ename":"RuntimeError","evalue":"For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors","output_type":"error"}]},{"cell_type":"code","source":"def check_accuracy(model,loader):\n    if loader.dataset.train:\n        print(\"Training Accuracy\")\n    else:\n        print(\"Testing Accuracy\")\n    samples = 0\n    corrects = 0\n    model.eval()\n    with torch.no_grad():\n        for x,y in tqdm(loader):\n            x = x.to(device).squeeze(1)\n            y=y.to(device)\n            #x = x.reshape(x.shape[0],-1)\n            score = model(x) # [64,10]\n            _,pred_index = score.max(1) #[64] , don't care about the value\n            corrects += (pred_index==y).sum()\n            samples += pred_index.size(0)\n        print(f\"Correct = {corrects}/{samples} Accuracy/epoch = {float(corrects)/float(samples)*100:.2f}\")\n    model.train()\n    \n    \n    \ncheck_accuracy(model,train_loader)\ncheck_accuracy(model,test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}