{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9982959,
          "sourceType": "datasetVersion",
          "datasetId": 6143132
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Mistral-DataGen",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Uo72K0XDX65n"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "tsr564_response_path = kagglehub.dataset_download('tsr564/response')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "LDs3gWlcX65p"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade together groq\n",
        "# !pip install -q --upgrade git+https://github.com/huggingface/transformers.git\n",
        "\n",
        "# !pip install -q llama-cpp-python\n",
        "# !pip install -q --upgrade accelerate optimum"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T09:31:20.338251Z",
          "iopub.execute_input": "2024-11-23T09:31:20.338622Z",
          "iopub.status.idle": "2024-11-23T09:31:30.747154Z",
          "shell.execute_reply.started": "2024-11-23T09:31:20.338584Z",
          "shell.execute_reply": "2024-11-23T09:31:30.745647Z"
        },
        "id": "kOltpDhrX65q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Fetch Hugging Face username and token from Colab secrets\n",
        "HF_USERNAME = \"pritam3355\"\n",
        "HF_TOKEN = \"\"\n",
        "groq_api_key=\"\"\n",
        "together_api_key=\"\"\n",
        "# Login to Hugging Face\n",
        "try:\n",
        "  login(token=HF_TOKEN)\n",
        "except ValueError:\n",
        "  # If token is not valid or found, login with username and token\n",
        "  # (likely requires manual authorization)\n",
        "  login(username=HF_USERNAME, token=HF_TOKEN)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T14:03:44.67136Z",
          "iopub.execute_input": "2024-11-23T14:03:44.671831Z",
          "iopub.status.idle": "2024-11-23T14:03:45.627821Z",
          "shell.execute_reply.started": "2024-11-23T14:03:44.671787Z",
          "shell.execute_reply": "2024-11-23T14:03:45.62694Z"
        },
        "id": "P0HSccMRX65s",
        "outputId": "45248ed1-b674-474d-c614-d57964441380"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import groq\n",
        "# import together\n",
        "\n",
        "\n",
        "# def generate_market_reaction(prompt, api_service, max_tokens=4096):\n",
        "#     generation_kwargs = {\n",
        "#         \"max_tokens\": max_tokens,  # Max number of new tokens to generate\n",
        "#         \"stop\": [\"<|endoftext|>\", \"</s>\", \"<|eot_id|>\", \"<|eom_id|>\",\"[/INST]\"],  # Stop sequences\n",
        "#         \"top_k\": 10,  #\n",
        "#         \"top_p\": 0.7,\n",
        "#         \"temperature\": 0.3,\n",
        "#         \"repeat_penalty\": 1.0,\n",
        "#     }\n",
        "\n",
        "#     # call API\n",
        "#     if api_service == \"Together\":\n",
        "#         client = together.Together(api_key=together_api_key)\n",
        "#         result = client.chat.completions.create(\n",
        "#             model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "#             max_tokens=generation_kwargs[\"max_tokens\"],\n",
        "#             temperature=generation_kwargs[\"temperature\"],\n",
        "#             top_p=generation_kwargs[\"top_p\"],\n",
        "#             top_k=generation_kwargs[\"top_k\"],\n",
        "#             repetition_penalty=generation_kwargs[\"repeat_penalty\"],\n",
        "#             stop=generation_kwargs[\"stop\"],\n",
        "#             stream=False  # Get the full response at once\n",
        "#         )\n",
        "#     else:\n",
        "#         client = groq.Client(api_key=groq_api_key)\n",
        "#         result = client.chat.completions.create(\n",
        "#             messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "#             model=\"llama3-8b-8192\",\n",
        "#             max_tokens=generation_kwargs[\"max_tokens\"],\n",
        "#             top_p=generation_kwargs[\"top_p\"],\n",
        "#             temperature=generation_kwargs[\"temperature\"]\n",
        "#         )\n",
        "\n",
        "#     return result.choices[0].message.content"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T09:31:30.853694Z",
          "iopub.status.idle": "2024-11-23T09:31:30.854282Z",
          "shell.execute_reply.started": "2024-11-23T09:31:30.853978Z",
          "shell.execute_reply": "2024-11-23T09:31:30.854007Z"
        },
        "id": "7AZ1WW4gX65t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "SPEAKER-1 0:00:00  : April 2nd, 2019. Say your numbers. 47, 48. Alright, you're having fun. Sorry. Fun. It's going to be a great time guys. What did y'all say for four? For four I said ten and two. Ten and two, why two? For B. I don't want to. Okay, but like is it because like numb takes B? Okay, okay.\n",
        "SPEAKER-3 0:00:57  : So if I change like, oh yeah, yeah. I thought I meant like changing it to another thing, whatever. It's four, one and two. I gave up. Ten and two? Yeah, because numb takes it in B, I think. No.\n",
        "SPEAKER-2 0:01:34  : What do you mean? It's still five, because it's the public static void method. Oh no, it runs. No, you know what, never mind. I'm going to confuse myself. I'm talking about four,\n",
        "SPEAKER-1 0:01:44  : because it's a public static void method. So what? I guess. But I'm talking, I'm not,\n",
        "SPEAKER-3 0:01:54  : I'm not worried about ten, but I'm talking about B. But you said like, after, after it's called,\n",
        "SPEAKER-1 0:02:02  : it's. See, I understand, but like it doesn't return anything. I'll just take your word for it. Okay.\n",
        "SPEAKER-3 0:03:31  : Did you run a seven? Let me guess the number. Fifty. It's fifty. It's five. I thought it was\n",
        "SPEAKER-1 0:03:49  : one output. How many outputs are there? Five, ten. Five, fifteen. Is it an odd number or even number? So five, nine. So ten, five. Nine, five. Fifteen, five. Is it a big number? How is it eleven? What'd you? Oh. I just guessed on that. I'll take one for the team. Oh no, Davis can take one for the team. You take one for the team. I did it last time,\n",
        "SPEAKER-3 0:04:49  : and you still fucked up. Tadah. And the above code, what is the main difference between the What do you think number nine is? I haven't gotten nine yet. Oh wait, oh, I skipped eight.\n",
        "SPEAKER-1 0:06:10  : I think it's C. What's C? Eight. Eight? Yeah, I put C too. Okay. I'll put C. Davis put C.\n",
        "SPEAKER-3 0:06:34  : I don't know though. I think program one is the values of list two copied into list one, but they still remain separate. Does that make sense? Why are you over here ahead? Eight, nine.\n",
        "SPEAKER-1 0:07:07  : What'd you put for one? Program one copies the references of list two, because you can't just do\n",
        "SPEAKER-2 0:07:13  : list one equals list two because, like, I just remember reading that in the book saying, like,\n",
        "SPEAKER-3 0:07:18  : you can't just do that or else it just copies the references. So are you sure? Pretty sure. Okay. I think so. You know, I'm gonna go second one. Oh shit.\n",
        "SPEAKER-1 0:07:55  : Yeah, I asked you about that one and you told me it was ten. What's five? What's five and six? Y'all are supposed to scroll all y'all over. Five's false, six is true. Okay. That's what I had. Do it. False and true. And then ten was A. Why'd you put A for the last one? Why'd you put A for the last one? No, I put B. Oh, then who said it was A? You was right. Oh, okay. Show off. Gosh. Watch me still I don't know how, but you will. Damn. Okay, let's do this. Let's get this class activity. Because I already didn't turn in the other one. So. Yeah, me neither. Find an array of... fuck me. This array will get ten names from the user.\n",
        "SPEAKER-3 0:10:08  : Okay, it doesn't seem too bad. Thanks. Do you remember how to constantly ask them to enter something?\n",
        "SPEAKER-2 0:13:54  : I know you had to make a for statement and then you had to do like...\n",
        "SPEAKER-1 0:13:57  : Do you remember how to do it?\n",
        "SPEAKER-3 0:14:04  : How many times to put in a name or something?\n",
        "SPEAKER-1 0:14:14  : I think it was Lab 7, Part 2 or Part 3. Yeah, Lab 7.\n",
        "SPEAKER-3 0:14:20  : What's that?\n",
        "SPEAKER-1 0:14:26  : Oh, oh, oh, okay. So you do like...\n",
        "SPEAKER-3 0:14:29  : For other? No. This is how we did it last week.\n",
        "SPEAKER-1 0:14:44  : It's like we'd ask the questions.\n",
        "SPEAKER-3 0:14:48  : Oh, okay, you did it after.\n",
        "SPEAKER-1 0:15:07  : I think I remember how to... Wait, what's that index?\n",
        "SPEAKER-3 0:15:11  : Oh, is that something else?\n",
        "SPEAKER-1 0:15:14  : How long did it take you to finish that lab? It wasn't that long. I stayed like 30 minutes after.\n",
        "SPEAKER-3 0:15:20  : And she stayed with you?\n",
        "SPEAKER-1 0:15:22  : Yeah, that was really fast. Because I was asking her like how does this work?\n",
        "SPEAKER-3 0:15:26  : How does that work? Blah, blah, blah, blah.\n",
        "SPEAKER-1 0:15:32  : I mean, lab ends at 530.\n",
        "SPEAKER-3 0:15:34  : Yeah. Oh, okay, so you just...\n",
        "SPEAKER-1 0:15:46  : This is specifically asking for the index, right?\n",
        "SPEAKER-3 0:15:48  : Yeah.\n",
        "SPEAKER-1 0:15:52  : No, because like enter student names. So like...\n",
        "SPEAKER-2 0:15:56  : Number.\n",
        "SPEAKER-1 0:15:57  : So like 10, enter 10 or 3, whatever.\n",
        "SPEAKER-3 0:16:00  : And then you can put in 3 names.\n",
        "SPEAKER-1 0:16:11  : I don't remember. Okay, just...\n",
        "SPEAKER-3 0:16:13  : I know.\n",
        "SPEAKER-1 0:16:21  : Do you need to import the array?\n",
        "SPEAKER-3 0:16:24  : I don't know. So what you can do is figure out where to... So you can import it in a different way. So you can hear the changes to specifically the index 1 to also J4. We're just trying to figure out what to do.\n",
        "SPEAKER-1 0:16:38  : Okay, so they pointed these out.\n",
        "SPEAKER-3 0:17:10  : I think I sort of got it. Let me see again.\n",
        "SPEAKER-1 0:17:53  : So where does it print?\n",
        "SPEAKER-3 0:17:55  : All right, down there.\n",
        "SPEAKER-1 0:18:08  : So I can do array... To stream.\n",
        "SPEAKER-3 0:18:14  : Yeah, okay.\n",
        "SPEAKER-1 0:18:24  : Do you get it? Do you understand it?\n",
        "SPEAKER-3 0:18:27  : I don't really, but I feel better about this than I did earlier today.\n",
        "SPEAKER-1 0:18:57  : Okay.\n",
        "SPEAKER-3 0:19:07  : Okay. So I got it.\n",
        "SPEAKER-2 0:20:15  : It's asking to enter 10 names.\n",
        "SPEAKER-1 0:20:17  : So you have to do a for statement with the... You should probably put the system out after... Before the for statement, because it's going to do that 10 times too.\n",
        "SPEAKER-3 0:20:29  : Oh, okay. Other than that, that's good. Now you... yeah. Memorize it. I got it.\n",
        "SPEAKER-1 0:20:51  : You need it? I just don't understand. Oh, well, I haven't done the second part yet with the method. Oh, what's the first part?\n",
        "SPEAKER-3 0:20:59  : And then you're going to search it with the key. Okay. I'm going to head out soon.\n",
        "SPEAKER-1 0:22:01  : Oh, so you're betraying me. You're betraying me again. Okay, Mr. Doesn't Show Up to the Lab on Time. You heard me.\n",
        "SPEAKER-2 0:22:09  : I don't... I didn't show up to what on time?\n",
        "SPEAKER-1 0:22:11  : The lab?\n",
        "SPEAKER-2 0:22:13  : I have shit to do, man. I got children to save.\n",
        "SPEAKER-1 0:22:15  : I know. I got shit to do. And by shit, I mean to school.\n",
        "SPEAKER-3 0:22:19  : Yeah, I'm saving people.\n",
        "SPEAKER-1 0:22:25  : I just hate it when students from other schools who have it like up here and they're like, don't come here.\n",
        "SPEAKER-2 0:22:31  : It's a bad school. I was like, you don't know what bad is.\n",
        "SPEAKER-1 0:22:35  : It was like, you guys don't have fights.\n",
        "SPEAKER-2 0:22:39  : You have good food. You have some money.\n",
        "SPEAKER-1 0:22:41  : Oh.\n",
        "SPEAKER-2 0:22:43  : Like the school I visited, they were always...\n",
        "SPEAKER-1 0:22:45  : The school I visited.\n",
        "SPEAKER-2 0:22:47  : Yeah, they were complaining. They were like, this is a bad school. Don't come here.\n",
        "SPEAKER-1 0:22:49  : I've heard about Beedemont.\n",
        "SPEAKER-2 0:22:51  : No, it's not. No, that's Pearl Ridge.\n",
        "SPEAKER-3 0:22:55  : No, I've heard Beedemont.\n",
        "SPEAKER-1 0:22:57  : I've only seen one black person though. I'm not supposed to be.\n",
        "SPEAKER-3 0:23:03  : No, my string won't work like your string. Because you don't have to. That was a silly mistake. I'm sorry. Okay. I know. Oh. Yeah. That's been good. Nice. Thanks. Yeah. Hey. Whistle. Thank you. Thank you. Thank you. Thank you. Thank you. Can you get this?\n",
        "SPEAKER-1 0:24:11  : Thank you.\n",
        "SPEAKER-3 0:24:13  : Thanks for coming.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T14:03:51.291282Z",
          "iopub.execute_input": "2024-11-23T14:03:51.291607Z",
          "iopub.status.idle": "2024-11-23T14:03:51.299686Z",
          "shell.execute_reply.started": "2024-11-23T14:03:51.291576Z",
          "shell.execute_reply": "2024-11-23T14:03:51.298623Z"
        },
        "id": "yOIrlZFAX65u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-cpp-python\n",
        "# !pip install llama-cpp-python  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu126"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T14:03:56.574727Z",
          "iopub.execute_input": "2024-11-23T14:03:56.575663Z"
        },
        "id": "zhOExbQ5X65v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "trusted": true,
        "id": "QlIe910zX65w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "## Download the GGUF model\n",
        "model_name = \"bartowski/Mistral-7B-Instruct-v0.3-GGUF\"\n",
        "model_file = \"Mistral-7B-Instruct-v0.3-Q5_K_S.gguf\"\n",
        "model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF\"\n",
        "model_file = \"mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
        "model_path = hf_hub_download(model_name, filename=model_file)\n",
        "\n",
        "## Instantiate model from downloaded file\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=16000,  # Context length to use\n",
        "    n_threads=4,            # Number of CPU threads to use\n",
        "    n_gpu_layers=0        # Number of model layers to offload to GPU\n",
        ")\n",
        "\n",
        "## Generation kwargs\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":20000,\n",
        "    \"stop\":[\"</s>\"],\n",
        "    \"echo\":False, # Echo the prompt in the output\n",
        "    \"top_k\":1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
        "}\n",
        "\n",
        "## Run inference\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "uAap70ycX65x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Give a meaningful abstractive summarization this transcript in 30 words : \"\n",
        "prompt = task+\"\\n\"+prompt\n",
        "\n",
        "response = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
        "summary = response[\"choices\"][0][\"text\"]\n",
        "print(summary)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OSabkByUX65y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Give a meaningful abstractive summarization this transcript in 30 words : \"\n",
        "prompt = task+\"\\n\"+prompt\n",
        "summary = generate_market_reaction(prompt,api_service= \"Together\")\n",
        "print(summary)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T09:16:24.33893Z",
          "iopub.execute_input": "2024-11-23T09:16:24.339489Z",
          "iopub.status.idle": "2024-11-23T09:16:25.731562Z",
          "shell.execute_reply.started": "2024-11-23T09:16:24.339451Z",
          "shell.execute_reply": "2024-11-23T09:16:25.730136Z"
        },
        "id": "mC2cktizX65y",
        "outputId": "8d23894d-d2eb-4874-b083-b3ba06b48bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " Three speakers discuss programming concepts, including methods and arrays, while working on a class activity. They help each other understand the material, share insights, and work through challenges in their code. The conversation also touches on their personal experiences and opinions.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Detect the topics of discussion in the following conversation transcript (with timestamp): \"\n",
        "prompt = task+\"\\n\"+prompt\n",
        "topic_detection = generate_market_reaction(prompt,api_service= \"Together\")\n",
        "print(topic_detection)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T09:21:35.025688Z",
          "iopub.execute_input": "2024-11-23T09:21:35.02606Z",
          "iopub.status.idle": "2024-11-23T09:21:38.170478Z",
          "shell.execute_reply.started": "2024-11-23T09:21:35.026028Z",
          "shell.execute_reply": "2024-11-23T09:21:38.169138Z"
        },
        "id": "kfEOZX7jX65z",
        "outputId": "a2347058-58db-4145-ece4-a4c4bd6f6dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " Topics of Discussion:\n\n1. Discussion on numbers and methods in programming (0:00)\n\t* Interpretation of number four as \"ten and two\" (0:00)\n\t* Discussion on public static void method (0:01)\n\t* Discussion on numb function taking argument B (0:01)\n\t* Discussion on function return values (0:02)\n2. Discussion on a code snippet (0:03)\n\t* Discussion on the output of a program (0:03)\n\t* Discussion on program differences (0:05)\n\t* Discussion on copying references of lists (0:07)\n3. Discussion on a lab exercise (0:10)\n\t* Discussion on entering user inputs in a loop (0:10)\n\t* Discussion on importing arrays (0:16)\n\t* Discussion on array index (0:15)\n\t* Discussion on printing array elements (0:18)\n4. Discussion on a method (0:21)\n\t* Discussion on searching an array with a key (0:21)\n5. Miscellaneous conversation (0:22)\n\t* Casual conversation and banter (0:22)\n\t* Discussion on different schools (0:22)\n\nAbstractive Summarization: Speakers discuss programming concepts, including numbers and methods, and review a code snippet. They also discuss a lab exercise involving user inputs and arrays, and share miscellaneous conversations.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Detect the tone & intent of discussion in the following conversation transcript (with timestamp): \"\n",
        "prompt = task+\"\\n\"+prompt\n",
        "tone_detection = generate_market_reaction(prompt,api_service= \"Together\")\n",
        "print(tone_detection)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T09:23:07.691982Z",
          "iopub.execute_input": "2024-11-23T09:23:07.692411Z",
          "iopub.status.idle": "2024-11-23T09:23:09.412367Z",
          "shell.execute_reply.started": "2024-11-23T09:23:07.692374Z",
          "shell.execute_reply": "2024-11-23T09:23:09.411143Z"
        },
        "id": "21et5jJTX65z",
        "outputId": "ba1f6fe6-ca55-4540-f99e-52ff1e3ebef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " Tone & Intent: The tone of the conversation is casual and friendly, with the speakers engaging in a technical discussion about programming. The intent is to clarify doubts and solve programming problems.\n\nTopics of Discussion:\n\n* Programming concepts such as public static void method (0:01:34)\n* Discussion about the values and references of lists (0:06:10)\n* Creating an array of ten names (0:09:59)\n* Importing arrays and specific indexes (0:16:21)\n\nAbstractive Summarization: In a casual conversation, speakers discuss programming concepts such as public static void method, values and references of lists, and creating an array of ten names, while clarifying doubts and solving programming problems.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Detect satisfaction level of the speaker(s) in following conversation transcript (with timestamp): \"\n",
        "prompt = task+\"\\n\"+prompt\n",
        "performance_detection = generate_market_reaction(prompt,api_service= \"Together\")\n",
        "print(performance_detection)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T09:27:44.54461Z",
          "iopub.execute_input": "2024-11-23T09:27:44.544987Z",
          "iopub.status.idle": "2024-11-23T09:27:46.739672Z",
          "shell.execute_reply.started": "2024-11-23T09:27:44.544954Z",
          "shell.execute_reply": "2024-11-23T09:27:46.738286Z"
        },
        "id": "TksqrG5mX650",
        "outputId": "2324abb3-e2ae-40dc-fd5e-4ced6f3b08e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " Satisfaction level: Neutral to slightly satisfied\n\nSpeaker satisfaction regarding their contribution: Neutral\n\nTone & intent: The tone is generally casual and collaborative, with the speakers working through a programming problem together. The intent is to understand the problem and come up with a solution.\n\nTopics of discussion: Programming, discussing code, problem-solving, and sharing knowledge about programming concepts.\n\nAbstractive summarization: In a programming class, three students discuss a problem, share their thoughts on how to approach it, and work through the solution together. They exchange ideas on how to implement specific features, ask each other questions, and provide feedback. The conversation covers topics such as copying list values, references, and using for statements. They also discuss importing arrays and searching for keys in the array.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Detect if speaker(s) are able to resolve their query in following conversation transcript (with timestamp): \"\n",
        "prompt = task+\"\\n\"+prompt\n",
        "resolution_detection = generate_market_reaction(prompt,api_service= \"Together\")\n",
        "print(resolution_detection)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-23T09:31:43.16626Z",
          "iopub.execute_input": "2024-11-23T09:31:43.166671Z",
          "iopub.status.idle": "2024-11-23T09:31:44.955612Z",
          "shell.execute_reply.started": "2024-11-23T09:31:43.166636Z",
          "shell.execute_reply": "2024-11-23T09:31:44.954448Z"
        },
        "id": "Qb_oisKCX650",
        "outputId": "c6eedf22-65e0-4376-b63d-eade78d16a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": " Based on the conversation transcript, it is not clear if the speakers were able to resolve their query. They discussed various topics related to programming, such as methods, variables, arrays, and loops, but it seems that they were working on different problems or projects. There were some moments of confusion and misunderstanding, but they also had some moments of clarity and agreement. However, the transcript does not provide a clear answer to the question of whether they were able to resolve their query.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Card for SP500-EDGAR-10K\n",
        "y\n",
        "This dataset contains the annual reports for all SP500 historical constituents from 2010-2022 from SEC EDGAR Form 10-K filings. It also contains n-day future returns of each firm's stock price from each filing datn Needed]\n",
        "\n",
        "Datase\n",
        "Source Data\n",
        "Initial Data Collection and Normalization\n",
        "10-K filings data was collected and processed using edgar-crawler available here. Return data was computed manually from other price data sources."
      ],
      "metadata": {
        "id": "-sSRR0lYX651"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "ds2 = load_dataset(\"jlohding/sp500-edgar-10k\",split=\"train\")\n",
        "print(ds2)\n",
        "# df2 = pd.DataFrame(ds2[:5])\n",
        "# df2.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T17:26:36.83123Z",
          "iopub.execute_input": "2024-11-22T17:26:36.831636Z",
          "iopub.status.idle": "2024-11-22T17:26:57.260952Z",
          "shell.execute_reply.started": "2024-11-22T17:26:36.831601Z",
          "shell.execute_reply": "2024-11-22T17:26:57.259543Z"
        },
        "id": "-Hwcl3GdX652",
        "outputId": "40c9a405-48f4-4798-91ba-86215b98f423",
        "colab": {
          "referenced_widgets": [
            "1b1f88610fc64fccb990133fc1bddb35",
            "a9fe5c0be5804d18a520c7ccf32426ae",
            "ebc6faf26a36464298fe722336b30784",
            "fb0c34d09d2f4ea2b4d60634dd7ad8b6",
            "481ec395ce82403f86363fecd73829c0",
            "9776ef71176440ce8ea866f41138f30c",
            "fbda4727ae904ed0b5f8b136a1e16c8b",
            "18881f8f9e5b423f9b2f17d78826357e",
            "0c91691b8c8349fc8d9e411c957190ef",
            "32e019e3222945efbc912364eff95b5e",
            "2904262572f245c3851d7e45fb1e454d",
            "148ce77b3cd84851ae700dfd8547d306",
            "f9b4a38bf60141cd83c27f949ef0405a",
            "5fd8918e36744ef99b1f132f86aa4f28",
            "92359ff81d6542f693af57e238ead020"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/1.26k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b1f88610fc64fccb990133fc1bddb35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2010.parquet:   0%|          | 0.00/68.8M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9fe5c0be5804d18a520c7ccf32426ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2011.parquet:   0%|          | 0.00/69.8M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebc6faf26a36464298fe722336b30784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2012.parquet:   0%|          | 0.00/71.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb0c34d09d2f4ea2b4d60634dd7ad8b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2013.parquet:   0%|          | 0.00/71.6M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "481ec395ce82403f86363fecd73829c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2014.parquet:   0%|          | 0.00/72.9M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9776ef71176440ce8ea866f41138f30c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2015.parquet:   0%|          | 0.00/72.2M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbda4727ae904ed0b5f8b136a1e16c8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2016.parquet:   0%|          | 0.00/74.6M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18881f8f9e5b423f9b2f17d78826357e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2017.parquet:   0%|          | 0.00/74.8M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c91691b8c8349fc8d9e411c957190ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2018.parquet:   0%|          | 0.00/76.7M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32e019e3222945efbc912364eff95b5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2019.parquet:   0%|          | 0.00/76.9M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2904262572f245c3851d7e45fb1e454d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2020.parquet:   0%|          | 0.00/75.6M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "148ce77b3cd84851ae700dfd8547d306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2021.parquet:   0%|          | 0.00/80.2M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9b4a38bf60141cd83c27f949ef0405a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "2022.parquet:   0%|          | 0.00/79.2M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fd8918e36744ef99b1f132f86aa4f28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/6282 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92359ff81d6542f693af57e238ead020"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset({\n    features: ['cik', 'sic', 'company', 'date', 'item_1', 'item_1A', 'item_1B', 'item_2', 'item_3', 'item_4', 'item_5', 'item_6', 'item_7', 'item_7A', 'item_8', 'item_9', 'item_9A', 'item_9B', 'item_10', 'item_11', 'item_12', 'item_13', 'item_14', 'item_15', 'ret', 'mkt_cap', '1_day_return', '3_day_return', '5_day_return', '10_day_return', '20_day_return', '40_day_return', '60_day_return', '80_day_return', '100_day_return', '150_day_return', '252_day_return', '__index_level_0__'],\n    num_rows: 6282\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "company_list = list(set(ds2[\"company\"]))\n",
        "company_date_count = Counter(ds2['company'])\n",
        "company_list_sorted = sorted(company_list, key=lambda company: company_date_count.get(company, 0), reverse=True)\n",
        "print(len(company_list_sorted))\n",
        "compliant_count = 5\n",
        "compliant_companies = company_list_sorted[:compliant_count]\n",
        "companies = ds2.filter(lambda x: x['company'] in compliant_companies)\n",
        "companies"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T18:03:18.758709Z",
          "iopub.execute_input": "2024-11-22T18:03:18.760265Z",
          "iopub.status.idle": "2024-11-22T18:03:18.800746Z",
          "shell.execute_reply.started": "2024-11-22T18:03:18.760219Z",
          "shell.execute_reply": "2024-11-22T18:03:18.799544Z"
        },
        "id": "e9dQyeL0X653",
        "outputId": "9c475c77-bfbc-44e5-df2b-30ea88fa2e95"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "883\n",
          "output_type": "stream"
        },
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['cik', 'sic', 'company', 'date', 'item_1', 'item_1A', 'item_1B', 'item_2', 'item_3', 'item_4', 'item_5', 'item_6', 'item_7', 'item_7A', 'item_8', 'item_9', 'item_9A', 'item_9B', 'item_10', 'item_11', 'item_12', 'item_13', 'item_14', 'item_15', 'ret', 'mkt_cap', '1_day_return', '3_day_return', '5_day_return', '10_day_return', '20_day_return', '40_day_return', '60_day_return', '80_day_return', '100_day_return', '150_day_return', '252_day_return', '__index_level_0__'],\n    num_rows: 65\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt"
      ],
      "metadata": {
        "id": "q1IvvzIvX653"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import pandas as pd\n",
        "# with open('/kaggle/input/response/response.json') as f:\n",
        "#     d = json.load(f)\n",
        "#     df = pd.DataFrame.from_dict(d, orient='index')\n",
        "#     df.reset_index(level=0, inplace=True)\n",
        "\n",
        "# df.head(9)\n",
        "print(len(companies[:2]),type(companies[:2]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T14:15:52.409766Z",
          "iopub.execute_input": "2024-11-22T14:15:52.410218Z",
          "iopub.status.idle": "2024-11-22T14:15:52.423065Z",
          "shell.execute_reply.started": "2024-11-22T14:15:52.410178Z",
          "shell.execute_reply": "2024-11-22T14:15:52.421114Z"
        },
        "id": "bA2Pk9JbX653",
        "outputId": "33d4306e-decd-46a2-d1a1-815bd20ae7a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "38 <class 'dict'>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# !transformers-cli cache clear"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T14:09:21.476026Z",
          "iopub.execute_input": "2024-11-22T14:09:21.476363Z",
          "iopub.status.idle": "2024-11-22T14:09:21.496857Z",
          "shell.execute_reply.started": "2024-11-22T14:09:21.476331Z",
          "shell.execute_reply": "2024-11-22T14:09:21.495734Z"
        },
        "id": "tbpUmc-sX654"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(data, label):\n",
        "    # Ensure all variables are correctly formatted as strings where necessary\n",
        "    company = data.get('company', 'N/A')\n",
        "    date = data.get('date', 'N/A')\n",
        "    mkt_cap = data.get('mkt_cap', 'N/A')\n",
        "    ret = data.get('ret', 'N/A')\n",
        "\n",
        "    item_1 = data.get('item_1', 'N/A')\n",
        "    item_1A = data.get('item_1A', 'N/A')\n",
        "    item_3 = data.get('item_3', 'N/A')\n",
        "    item_6 = data.get('item_6', 'N/A')\n",
        "    item_7 = data.get('item_7', 'N/A')\n",
        "    item_8 = data.get('item_8', 'N/A')\n",
        "    item_9 = data.get('item_9', 'N/A')\n",
        "    item_10 = data.get('item_10', 'N/A')\n",
        "    item_12 = data.get('item_12', 'N/A')\n",
        "\n",
        "    # Stock return data\n",
        "    return_1_day = label.get('1_day_return', 'N/A')\n",
        "    return_5_day = label.get('5_day_return', 'N/A')\n",
        "    return_10_day = label.get('10_day_return', 'N/A')\n",
        "    return_20_day = label.get('20_day_return', 'N/A')\n",
        "    return_40_day = label.get('40_day_return', 'N/A')\n",
        "    return_60_day = label.get('60_day_return', 'N/A')\n",
        "    return_100_day = label.get('100_day_return', 'N/A')\n",
        "    return_150_day = label.get('150_day_return', 'N/A')\n",
        "    return_252_day = label.get('252_day_return', 'N/A')\n",
        "\n",
        "    # Building the prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are a stock market analyst and have been provided with SEC filing data and stock price information.\n",
        "    Your task is to carefully analyze the data and identify why the stock price moved in a specific direction.\n",
        "    Consider common factors such as earnings guidance, market sentiment, macroeconomic conditions, or\n",
        "    company-specific events mentioned in the SEC filing.\n",
        "    If the reason for the price movement is unclear, you should respond with: \"Reason unknown.\" Do not provide\n",
        "    vague or fabricated analysis.\n",
        "\n",
        "    You need to output the analysis in the following \"JSON\" format (single output value is given as example do the same for rest):\n",
        "    {{\n",
        "      \"1_day_return_reason\": \"your reason for the 1-day price movement in 10-15 words\",\n",
        "      \"5_day_return_reason\":\n",
        "      \"10_day_return_reason\":\n",
        "      \"20_day_return_reason\":\n",
        "      \"40_day_return_reason\":\n",
        "      \"60_day_return_reason\":\n",
        "      \"100_day_return_reason\":\n",
        "      \"150_day_return_reason\":\n",
        "      \"252_day_return_reason\":\n",
        "    }}\n",
        "\n",
        "    Below is the data for your analysis from SEC filing and Stock price on n-th day provided as follows::\n",
        "\n",
        "    - **Company**: {company}\n",
        "    - **Date**: {date}\n",
        "    - **Market Capitalization**: {mkt_cap}\n",
        "    - **Immediate Return (ret)**: {ret}\n",
        "    - **Item 1: Business**: Operations and market position: {item_1}\n",
        "    - **Item 1A: Risk Factors**: Risks affecting operations: {item_1A}\n",
        "    - **Item 3: Legal Proceedings**: Legal issues impacting price: {item_3}\n",
        "    - **Item 6: Selected Financial Data**: Key financial trends: {item_6}\n",
        "    - **Item 7: Management’s Discussion and Analysis**: Management’s view on strategy: {item_7}\n",
        "    - **Item 8: Financial Statements and Supplementary Data**: Financial data insights: {item_8}\n",
        "    - **Item 9: Changes in and Disagreements with Accountants**: Auditor concerns: {item_9}\n",
        "    - **Item 10: Directors, Executive Officers, and Corporate Governance**: Leadership changes: {item_10}\n",
        "    - **Item 12: Security Ownership of Certain Beneficial Owners and Management**: Ownership stakes: {item_12}\n",
        "\n",
        "    - **1-day return**: {return_1_day}\n",
        "    - **5-day return**: {return_5_day}\n",
        "    - **10-day return**: {return_10_day}\n",
        "    - **20-day return**: {return_20_day}\n",
        "    - **40-day return**: {return_40_day}\n",
        "    - **60-day return**: {return_60_day}\n",
        "    - **100-day return**: {return_100_day}\n",
        "    - **150-day return**: {return_150_day}\n",
        "    - **252-day return**: {return_252_day}\n",
        "    \"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T18:03:25.769565Z",
          "iopub.execute_input": "2024-11-22T18:03:25.769941Z",
          "iopub.status.idle": "2024-11-22T18:03:25.780684Z",
          "shell.execute_reply.started": "2024-11-22T18:03:25.769909Z",
          "shell.execute_reply": "2024-11-22T18:03:25.779511Z"
        },
        "id": "xa1h0lOlX654"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T17:51:19.751295Z",
          "iopub.execute_input": "2024-11-22T17:51:19.751775Z",
          "iopub.status.idle": "2024-11-22T17:51:30.338165Z",
          "shell.execute_reply.started": "2024-11-22T17:51:19.751738Z",
          "shell.execute_reply": "2024-11-22T17:51:30.336642Z"
        },
        "id": "DRiie-oKX655"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "data_columns = list(companies.features.keys())[2:26]\n",
        "label_columns = list(companies.features.keys())[-12:-1]\n",
        "\n",
        "datas = companies.select_columns(data_columns).to_pandas()\n",
        "labels = companies.select_columns(label_columns).to_pandas()\n",
        "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-Instruct-v0.1')\n",
        "\n",
        "for i, row in datas.iterrows():\n",
        "    prompt = generate_prompt(row, labels.iloc[i])  # Pass the correct row and corresponding label\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    print(f\"{i}-th row token length - {len(tokens)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T18:06:31.251025Z",
          "iopub.execute_input": "2024-11-22T18:06:31.251601Z",
          "iopub.status.idle": "2024-11-22T18:06:47.214163Z",
          "shell.execute_reply.started": "2024-11-22T18:06:31.251555Z",
          "shell.execute_reply": "2024-11-22T18:06:47.21288Z"
        },
        "id": "J_AqXyyMX655",
        "outputId": "0fd95f0f-479e-4c0b-c1f4-c0f90dba62e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "0-th row token length - 16920\n1-th row token length - 5416\n2-th row token length - 111789\n3-th row token length - 30589\n4-th row token length - 64300\n5-th row token length - 103721\n6-th row token length - 64935\n7-th row token length - 30714\n8-th row token length - 18301\n9-th row token length - 5484\n10-th row token length - 70268\n11-th row token length - 5306\n12-th row token length - 90351\n13-th row token length - 21285\n14-th row token length - 29757\n15-th row token length - 29511\n16-th row token length - 76998\n17-th row token length - 22111\n18-th row token length - 88560\n19-th row token length - 5551\n20-th row token length - 69764\n21-th row token length - 5609\n22-th row token length - 31787\n23-th row token length - 23038\n24-th row token length - 88051\n25-th row token length - 82113\n26-th row token length - 66482\n27-th row token length - 30606\n28-th row token length - 20325\n29-th row token length - 7083\n30-th row token length - 18931\n31-th row token length - 7052\n32-th row token length - 31270\n33-th row token length - 86363\n34-th row token length - 64590\n35-th row token length - 88569\n36-th row token length - 7240\n37-th row token length - 36000\n38-th row token length - 54252\n39-th row token length - 66273\n40-th row token length - 37827\n41-th row token length - 56348\n42-th row token length - 93828\n43-th row token length - 73598\n44-th row token length - 55714\n45-th row token length - 94278\n46-th row token length - 49893\n47-th row token length - 44501\n48-th row token length - 70711\n49-th row token length - 59068\n50-th row token length - 59415\n51-th row token length - 43101\n52-th row token length - 64537\n53-th row token length - 85575\n54-th row token length - 54197\n55-th row token length - 53737\n56-th row token length - 57652\n57-th row token length - 43098\n58-th row token length - 40897\n59-th row token length - 88992\n60-th row token length - 50995\n61-th row token length - 38635\n62-th row token length - 55078\n63-th row token length - 87007\n64-th row token length - 41630\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from llama_cpp import Llama\n",
        "# from huggingface_hub import hf_hub_download\n",
        "\n",
        "# model_kwargs = {\n",
        "#   \"n_ctx\":4096,    # Context length to use\n",
        "#   \"n_threads\":8,   # Number of CPU threads to use\n",
        "#   \"n_gpu_layers\": 0,# Number of model layers to offload to GPU. Set to 0 if only using CPU\n",
        "#   \"verbose\": True,\n",
        "# }\n",
        "\n",
        "## Define model name and file name\n",
        "# model_name = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF\"\n",
        "# model_file = \"mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "\n",
        "# model_name = \"TheBloke/vicuna-13B-v1.5-GGUF\"\n",
        "# model_file = \"vicuna-13b-v1.5.Q5_K_M.gguf\"\n",
        "\n",
        "# model_path = hf_hub_download(model_name, filename=model_file)\n",
        "# ## Instantiate model from downloaded file\n",
        "# llm = Llama(model_path=model_path, **model_kwargs)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T14:09:39.62212Z",
          "iopub.status.idle": "2024-11-22T14:09:39.622588Z",
          "shell.execute_reply.started": "2024-11-22T14:09:39.622367Z",
          "shell.execute_reply": "2024-11-22T14:09:39.622386Z"
        },
        "id": "ZJil9VEaX655"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta Llama 3 8B Instruct Lite\n",
        "# Mistral (7B) Instruct"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T14:09:39.6251Z",
          "iopub.status.idle": "2024-11-22T14:09:39.62567Z",
          "shell.execute_reply.started": "2024-11-22T14:09:39.625417Z",
          "shell.execute_reply": "2024-11-22T14:09:39.625441Z"
        },
        "id": "JNnARdxhX656"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "import groq\n",
        "\n",
        "\n",
        "# Function to generate the market reaction for each prompt with proper formatting\n",
        "def generate_market_reaction(prompt, api_service, max_tokens=4096):\n",
        "    generation_kwargs = {\n",
        "        \"max_tokens\": max_tokens,  # Max number of new tokens to generate\n",
        "        \"stop\": [\"<|endoftext|>\", \"</s>\", \"<|eot_id|>\", \"<|eom_id|>\",\"[/INST]\"],  # Stop sequences\n",
        "        \"top_k\": 10,  #\n",
        "        \"top_p\": 0.7,\n",
        "        \"temperature\": 0.3,\n",
        "        \"repeat_penalty\": 1.0,\n",
        "    }\n",
        "\n",
        "    # call API\n",
        "    if api_service == \"Together\":\n",
        "        client = together.Together(api_key=together_api_key)\n",
        "        result = client.chat.completions.create(\n",
        "            model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=generation_kwargs[\"max_tokens\"],\n",
        "            temperature=generation_kwargs[\"temperature\"],\n",
        "            top_p=generation_kwargs[\"top_p\"],\n",
        "            top_k=generation_kwargs[\"top_k\"],\n",
        "            repetition_penalty=generation_kwargs[\"repeat_penalty\"],\n",
        "            stop=generation_kwargs[\"stop\"],\n",
        "            stream=False  # Get the full response at once\n",
        "        )\n",
        "    else:\n",
        "        client = groq.Client(api_key=groq_api_key)\n",
        "        result = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"llama3-8b-8192\",\n",
        "            max_tokens=generation_kwargs[\"max_tokens\"],\n",
        "            top_p=generation_kwargs[\"top_p\"],\n",
        "            temperature=generation_kwargs[\"temperature\"]\n",
        "        )\n",
        "\n",
        "    return result.choices[0].message.content\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T17:45:15.739704Z",
          "iopub.execute_input": "2024-11-22T17:45:15.740123Z",
          "iopub.status.idle": "2024-11-22T17:45:15.749939Z",
          "shell.execute_reply.started": "2024-11-22T17:45:15.740091Z",
          "shell.execute_reply": "2024-11-22T17:45:15.74857Z"
        },
        "id": "BMqVIQHHX656"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def generate_dataset(companies, api_service=\"Together\"):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-Instruct-v0.1')\n",
        "\n",
        "    data_columns = list(companies.features.keys())[2:26]\n",
        "    label_columns = list(companies.features.keys())[-12:-1]\n",
        "\n",
        "    datas = companies.select_columns(data_columns).to_pandas()\n",
        "    labels = companies.select_columns(label_columns).to_pandas()\n",
        "    new_ds = datas[[\"company\", \"date\"]].copy()\n",
        "\n",
        "    # Define columns for storing the return reasons\n",
        "    column_names = [\n",
        "        \"1_day_return_reason\",\n",
        "        \"5_day_return_reason\",\n",
        "        \"10_day_return_reason\",\n",
        "        \"20_day_return_reason\",\n",
        "        \"40_day_return_reason\",\n",
        "        \"60_day_return_reason\",\n",
        "        \"100_day_return_reason\",\n",
        "        \"150_day_return_reason\",\n",
        "        \"252_day_return_reason\"\n",
        "    ]\n",
        "\n",
        "    for column_name in column_names:\n",
        "        new_ds[column_name] = \"\"  # Initialize empty columns for reasons\n",
        "\n",
        "    # Iterate over rows (not columns)\n",
        "    for i, row in datas.iterrows():  # .iterrows() allows row iteration\n",
        "        prompt = generate_prompt(row, labels.iloc[i])  # Pass the correct row and corresponding label\n",
        "        tokens = tokenizer.encode(prompt)\n",
        "\n",
        "        print(f\"{i}-th prompt length - {len(prompt)}, data length {len(row),len(labels.iloc[i])} & tokern count {len(tokens)}\")\n",
        "\n",
        "        reason_data = generate_market_reaction(prompt, api_service)  # Get the reason data\n",
        "        try:\n",
        "            reason_data_dict = json.loads(reason_data)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Failed to parse reason_data at iteration {i}: {e}\")\n",
        "            reason_data_dict = {}  # Fallback to empty dictionary if parsing fails\n",
        "\n",
        "        # Fill the new dataset with the reason data\n",
        "        for key, value in reason_data_dict.items():\n",
        "            if key in new_ds.columns:  # Check if the key is in the columns\n",
        "                new_ds.loc[i, key] = value  # Assign the value to the corresponding cell\n",
        "\n",
        "    return new_ds\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T17:55:47.593708Z",
          "iopub.execute_input": "2024-11-22T17:55:47.594188Z",
          "iopub.status.idle": "2024-11-22T17:55:47.605124Z",
          "shell.execute_reply.started": "2024-11-22T17:55:47.594153Z",
          "shell.execute_reply": "2024-11-22T17:55:47.603703Z"
        },
        "id": "EPgFnpvZX656"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Card for 10-K Benchmark\n",
        "\n",
        "This is a synthetically generated financial benchmark for large language models based off of a wide range of 10-K documents. The benchmark was subsequently run on multiple LLMs, which provided candidate answers for evaluation based on both the context and the generated question.\n",
        "\n",
        "The original 10-K dataset contains annual reports of public US companies that filed with the SEC / the EDGAR system from 1993-2020. The link to the original dataset used for context extraction can be found here.\n",
        "\n",
        "GPT-4 was then used in conjunction with these pieces of extracted content to automatically generate an adverserial dataset of complex and challenging questions covering calculations, financial domain expertise and legal / regulatory knowledge. A wide variety of models including GPT-4-Turbo, GPT-3.5-Turbo, Llama-7b-chat-hf, Llama-2-70b-chat-hf, Zephyr-7b-Beta, Solar-10.7b-Instruct-v1.0, Mistral-7B-Instruct-v0.2, Mixtral-8x7B-Instruct-v0.1 were then used to answer the synthetically generated questions using both the \"golden context\" provided AND their own knowledge.\n",
        "\n",
        "Lastly, GPT-4 was used to evaluate the correctness of each answer that was generated by each of the models. GPT-4 provided (1) a binary \"Yes/No\" label (which is shared in the dataset), and (2) an accompanying justification (which was manually reviewed by our team, but not included in the dataset)."
      ],
      "metadata": {
        "id": "iQNotWZfX657"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ds1 = load_dataset(\"adumitrescu18/Financial10kBenchmark\",split=\"train\")\n",
        "# print(ds1.num_rows)\n",
        "# df1 = pd.DataFrame(ds1[:5])\n",
        "# df1.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T14:09:39.65091Z",
          "iopub.status.idle": "2024-11-22T14:09:39.651619Z",
          "shell.execute_reply.started": "2024-11-22T14:09:39.651273Z",
          "shell.execute_reply": "2024-11-22T14:09:39.651305Z"
        },
        "id": "UEKZ8Ta_X657"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Card for FinQABench: A New QA Benchmark for Finance applications\n",
        "This dataset contains queries and responses to evaluate financial AI chatbots for hallucinations and accuracy. The dataset was created using Lighthouz AutoBench, a no-code test case generator for LLM use cases, and then manually verified by two human annotators.\n",
        "#\n",
        "\n",
        "Dataset Details\n",
        "This dataset was created using Apple's 10K SEC filing from 2022. It has 100 test cases, each with a query and a response. Each row in the dataset represents a test case consistips://Lighthouz Eval Studio.\n",
        "\n",
        "When evaluating LLM responses for hallucinations, Lighthouz Eval Studio provides evaluation metrics and classifies responses into the following categories:\n",
        "\n",
        "Correct and complete\n",
        "Correct but incomplete\n",
        "Correct and extra information\n",
        "Hallucinations or Incorrect\n",
        "No Answer"
      ],
      "metadata": {
        "id": "m9We2zRlX657"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ds = load_dataset(\"lighthouzai/finqabench\")[\"train\"]\n",
        "# df = pd.DataFrame(ds[:5])\n",
        "# df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T14:09:39.653652Z",
          "iopub.status.idle": "2024-11-22T14:09:39.654313Z",
          "shell.execute_reply.started": "2024-11-22T14:09:39.653992Z",
          "shell.execute_reply": "2024-11-22T14:09:39.654027Z"
        },
        "id": "7JDusAjEX658"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}