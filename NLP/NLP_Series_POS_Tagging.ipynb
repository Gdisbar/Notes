{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Complete Blog post - https://medium.com/analytics-vidhya/part-of-speech-tagging-what-when-why-and-how-9d250e634df6"
      ],
      "metadata": {
        "id": "5VJIlxM60bq0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQWFO0PsoGOI"
      },
      "source": [
        "# So, it is time to learn to PoS Tag!\n",
        "\n",
        "In this notebook, I'll guide you through the steps of training some models to be further utilized in our NLP Tool to do PoS Tagging. Here we won't apply any state of the art algorithm, but we won't be far either!\n",
        "\n",
        "If you don't know how this *notebook* works, check this link: https://colab.research.google.com/notebooks/intro.ipynb#\n",
        "\n",
        "## Getting the data (Corpus)\n",
        "\n",
        "Let us start by where we'll get our data (our **corpus**). There are many sources, but two are the most commonly used:\n",
        "* **Penn Treebank** subset from nltk (you can buy the entire Treebank, if you want, but you'll have to invest some $700~).\n",
        "* The **Universal Dependencies** Treebanks, available (as of February 2020) for 90 languages (in different quality and quantity levels). \n",
        "\n",
        "These contain the hard work of many **annotators**, which went through selected sets of sentences and annotated each one by hand, forming a corpus to be used as **supervised** input for our **machine learning algorithms**.\n",
        "\n",
        "The following two cells will show how to import the corpus from each of these two sources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B5gdrh7Z_Ic",
        "outputId": "c73fc6ec-2277-4feb-da73-a59d41f9ed55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#This cell loads the Penn Treebank corpus from nltk into a list variable named penn_treebank.\n",
        "\n",
        "#No need to install nltk in google colab since it is preloaded in the environments.\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Ensure that the treebank corpus is downloaded\n",
        "nltk.download('treebank')\n",
        "\n",
        "#Load the treebank corpus class\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "#Now we iterate over all samples from the corpus (the fileids - that are equivalent to sentences) \n",
        "#and retrieve the word and the pre-labeled PoS tag. This will be added as a list of tuples with \n",
        "#a list of words and a list of their respective PoS tags (in the same order).\n",
        "penn_treebank = []\n",
        "for fileid in treebank.fileids():\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for word, tag in treebank.tagged_words(fileid):\n",
        "    tokens.append(word)\n",
        "    tags.append(tag)\n",
        "  penn_treebank.append((tokens, tags))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WwZYkNr1bPN",
        "outputId": "70c9180a-a09a-492d-edba-69aa3ace91df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#This cell loads the Universal Dependecies Treekbank corpus. It'll download all the packages, but we'll only use the GUM\n",
        "#english package. We'll also install the conllu package, that was developed to parse data in the conLLu format, a \n",
        "#format common of linguistic annotated files. We'll also have a list variable, but now named ud_treebank.\n",
        "\n",
        "#Install conllu package, download the UD Treebanks corpus and unpack it.\n",
        "!pip install conllu\n",
        "!wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz\n",
        "!tar zxf ud-treebanks-v2.5.tgz\n",
        "\n",
        "#The imports needed to open and parse (interpret) the conllu file. At the end we'll have a list of dicts.\n",
        "from io import open\n",
        "from conllu import parse_incr\n",
        "\n",
        "#Open the file and load the sentences to a list.\n",
        "data_file = open(\"ud-treebanks-v2.5/UD_English-GUM/en_gum-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "ud_files = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "    ud_files.append(tokenlist)\n",
        "\n",
        "#Now we iterate over all samples from the corpus and retrieve the word and the pre-labeled PoS tag (upostag). This will \n",
        "#be added as a list of tuples with a list of words and a list of their respective PoS tags (in the same order).\n",
        "ud_treebank = []\n",
        "for sentence in ud_files:\n",
        "  tokens = []\n",
        "  tags = []\n",
        "  for token in sentence:\n",
        "    tokens.append(token['form'])\n",
        "    tags.append(token['upostag'])\n",
        "  ud_treebank.append((tokens, tags))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-4.5.2\n",
            "--2023-05-06 15:53:10--  https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3105/ud-treebanks-v2.5.tgz\n",
            "Resolving lindat.mff.cuni.cz (lindat.mff.cuni.cz)... 195.113.20.140\n",
            "Connecting to lindat.mff.cuni.cz (lindat.mff.cuni.cz)|195.113.20.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 355216681 (339M) [application/x-gzip]\n",
            "Saving to: ‘ud-treebanks-v2.5.tgz’\n",
            "\n",
            "ud-treebanks-v2.5.t 100%[===================>] 338.76M  68.0MB/s    in 5.0s    \n",
            "\n",
            "2023-05-06 15:53:17 (68.3 MB/s) - ‘ud-treebanks-v2.5.tgz’ saved [355216681/355216681]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzzGnG10Ulv2"
      },
      "source": [
        "**Word of Caution!**\n",
        "\n",
        "Penn Treebank and UD Treebanks use *distinct tagsets*. \n",
        "\n",
        "We won't be able to interchange them unless we make a converter - also, we'll only be able to do so from Penn->UD, because Penn Treebank has tags more detailed than UD, and we won't be able to retrieve these details from the tags without a third function and a lot of effort.\n",
        "\n",
        "We'll only do that later, in our code.\n",
        "\n",
        "Let us continue with the explanation of the Tagger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfD5ujGijuUF"
      },
      "source": [
        "#Extracting Features form Words\n",
        "\n",
        "Next, we have to create a function that is able to extract features from our words. These features will be used to predict the PoS.\n",
        "\n",
        "For that,  for each word, we'll pass the sentence and word index, and we'll provide a dict with the features.\n",
        "\n",
        "To explain about the feature set (can be changed, if you want), it is composed by:\n",
        "* Word: the word itself. Some words are always one PoS, others not.\n",
        "* is_first, is_last: check if it is the first or last in the sentence.\n",
        "* is_capitalized: first letter is caps? Maybe it is a proper noun...\n",
        "* is_all_caps or is_all_lower: checks for acronyms (or common words).\n",
        "* prefixes/suffixes: check word initialization/termination\n",
        "* prev_word/next_word: checks the preceding and succeding word.\n",
        "* has-hyphen: words with '-' may be adjectives.\n",
        "* is_numeric: for numbers.\n",
        "* capitals_inside: weird cases. Maybe nouns.\n",
        "\n",
        "The basis of this feature extraction method comes from two nice articles:\n",
        "* https://nlpforhackers.io/training-pos-tagger/\n",
        "* https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31\n",
        "\n",
        "If you're wondering, yes, this encoding WILL need a lot of memory for training (if you're not using categorical variables).\n",
        "\n",
        "And we'll have to replicate this in our main code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IyIaTSwoo-V"
      },
      "source": [
        "#Regex module for checking alphanumeric values.\n",
        "import re\n",
        "def extract_features(sentence, index):\n",
        "  return {\n",
        "      'word':sentence[index],\n",
        "      'is_first':index==0,\n",
        "      'is_last':index ==len(sentence)-1,\n",
        "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
        "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
        "      'prefix-1':sentence[index][0],\n",
        "      'prefix-2':sentence[index][:2],\n",
        "      'prefix-3':sentence[index][:3],\n",
        "      'prefix-3':sentence[index][:4],\n",
        "      'suffix-1':sentence[index][-1],\n",
        "      'suffix-2':sentence[index][-2:],\n",
        "      'suffix-3':sentence[index][-3:],\n",
        "      'suffix-3':sentence[index][-4:],\n",
        "      'prev_word':'' if index == 0 else sentence[index-1],\n",
        "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
        "      'has_hyphen': '-' in sentence[index],\n",
        "      'is_numeric': sentence[index].isdigit(),\n",
        "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
        "  }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYS5r1_m6Yr9"
      },
      "source": [
        "We now prepare the dataset for use in Machine Learning algorithms.\n",
        "\n",
        "There are two steps (three, if we're doing deep learning, but that's for later) to it: \n",
        "* Defining a function to transform the corpus to a more datsetish format.\n",
        "* Then, divide the encoded data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hiniE_wzPOC"
      },
      "source": [
        "#Ater defining the extract_features, we define a simple function to transform our data in a more 'datasetish' format.\n",
        "#This function returns the data as two lists, one of Dicts of features and the other with the labels.\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "  X, y = [], []\n",
        "  for sentence, tags in tagged_sentences:\n",
        "    sent_word_features, sent_tags = [],[]\n",
        "    for index in range(len(sentence)):\n",
        "        sent_word_features.append(extract_features(sentence, index)),\n",
        "        sent_tags.append(tags[index])\n",
        "    X.append(sent_word_features)\n",
        "    y.append(sent_tags)\n",
        "  return X, y\n",
        "\n",
        "#We divide the set BEFORE encoding. Why? To have full sentences in training/testing sets. When we encode, we do not encode\n",
        "#a sentence, but its words instead.\n",
        "\n",
        "#First, for the Penn treebank.\n",
        "penn_train_size = int(0.8*len(penn_treebank))\n",
        "penn_training = penn_treebank[:penn_train_size]\n",
        "penn_testing = penn_treebank[penn_train_size:]\n",
        "X_penn_train, y_penn_train = transform_to_dataset(penn_training)\n",
        "X_penn_test, y_penn_test = transform_to_dataset(penn_testing)\n",
        "\n",
        "#Then, for UD Treebank.\n",
        "ud_train_size = int(0.8*len(ud_treebank))\n",
        "ud_training = ud_treebank[:ud_train_size]\n",
        "ud_testing = ud_treebank[ud_train_size:]\n",
        "X_ud_train, y_ud_train = transform_to_dataset(ud_training)\n",
        "X_ud_test, y_ud_test = transform_to_dataset(ud_testing)\n",
        "\n",
        "#Third step, vectorize datasets. For that we use sklearn DictVectorizer\n",
        "#WARNING"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFc51jNtDptW"
      },
      "source": [
        "# Training a Tagger\n",
        "\n",
        "Now, we can train supervised machine learning algorithms to PoS Tagging.\n",
        "\n",
        "We'll use the Conditional Random Fields (CRF) algorithm. Here's a brief explanation:\n",
        "\n",
        "* **CRF**: A variation of Markov Random Field. Okay, that might not have helped. It is a discriminative model that, in a quick summary, evaluates the probabilities that a set of states are dependant or not between themselves based on a set of observations. In this case, it evaluates the probabilities that a word observed in a context (defined by the above mentioned features) belongs to a specific PoS. In training time, it takes what is the best state given the set of current observations and probabilities.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/max/681/1*8hOWH7YF5INMF2OPhKjVxA.png\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Want more math? Read this: https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776\n",
        "\n",
        "So, to achieve this, we'll use scikit learn (sklearn) and a sklearn compatible crf suite (skleran_crfsuit). If you don't know what is sklearn, [read this](https://scikit-learn.org/stable/getting_started.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHTkotyWpd28",
        "outputId": "0916e133-15f3-46de-a3d7-2f876bdafb74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Ignoring some warnings for the sake of readability.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#First, install sklearn_crfsuite, as it is not preloaded into Colab. \n",
        "!pip install sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "\n",
        "#This loads the model. Specifics are: \n",
        "#algorithm: methodology used to check if results are improving. Default is lbfgs (gradient descent).\n",
        "#c1 and c2:  coefficients used for regularization.\n",
        "#max_iterations: max number of iterations (DUH!)\n",
        "#all_possible_transitions: since crf creates a \"network\", of probability transition states,\n",
        "#this option allows it to map even \"connections\" not present in the data.\n",
        "penn_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "#The fit method is the default name used by Machine Learning algorithms to start training.\n",
        "print(\"Started training on Penn Treebank corpus!\")\n",
        "penn_crf.fit(X_penn_train, y_penn_train)\n",
        "print(\"Finished training on Penn Treebank corpus!\")\n",
        "\n",
        "#Same for UD\n",
        "ud_crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.01,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "print(\"Started training on UD corpus!\")\n",
        "ud_crf.fit(X_ud_train, y_ud_train)\n",
        "print(\"Finished training on UD corpus!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.65.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n",
            "Started training on Penn Treebank corpus!\n",
            "Finished training on Penn Treebank corpus!\n",
            "Started training on UD corpus!\n",
            "Finished training on UD corpus!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQvig1nQBcbA"
      },
      "source": [
        "# Checking the Results\n",
        "\n",
        "For that, we'll use a score method named balanced f-score. This score takes into account *precision* and *recall*.\n",
        "\n",
        "* **precision**: Considering the universe of tagged words, how many were correctly tagged?\n",
        "* **recall**: Considering the universe of correct tags, how many words were really correctly tagged?\n",
        "\n",
        "The distinction is in the direction you look. Precision looks at all tagged words to find how many are ok; Recall looks at correct tags to find how many were able to be \"guessed\".\n",
        "\n",
        "F-score is then calculated using these two. I won't go into the maths of it.  If you want,\n",
        "* You can read the wikipedia article here: https://en.wikipedia.org/wiki/F1_score\n",
        "* Or watch a neat simple video here: https://www.youtube.com/watch?v=j-EB6RqqjGI&ab_channel=CodeEmporium\n",
        "\n",
        "Also, here's the wikipedia image to help you understand:\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\"/>\n",
        "</div>\n",
        "\n",
        "We won't go into the computations either. Let the package do its thing (after all, we're interested in NLP now, not in statistics):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlJwNkF_0zs",
        "outputId": "60a1d2b2-a8ad-401e-e032-e3ed2080706a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "source": [
        "#We'll use the sklearn_crfsuit own metrics to compute f1 score.\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers\n",
        "print(\"## Penn ##\")\n",
        "\n",
        "#First calculate a prediction from test data, then we print the metrics for f-1 using the .flat_f1_score method.\n",
        "y_penn_pred=penn_crf.predict(X_penn_test)\n",
        "print(\"F1 score on Test Data\")\n",
        "print(metrics.flat_f1_score(y_penn_test, y_penn_pred,average='weighted',labels=penn_crf.classes_))\n",
        "#For the sake of clarification, we do the same for train data.\n",
        "y_penn_pred_train=penn_crf.predict(X_penn_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_penn_train, y_penn_pred_train,average='weighted',labels=penn_crf.classes_))\n",
        "\n",
        "# This presents class wise score. Helps see which classes (tags) are the ones with most problems.\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_penn_test, y_penn_pred, labels=penn_crf.classes_, digits=3\n",
        "))\n",
        "\n",
        "#Same for UD\n",
        "print(\"## UD ##\")\n",
        "\n",
        "y_ud_pred=ud_crf.predict(X_ud_test)\n",
        "print(\"F1 score on Test Data \")\n",
        "print(metrics.flat_f1_score(y_ud_test, y_ud_pred,average='weighted',labels=ud_crf.classes_))\n",
        "y_ud_pred_train=ud_crf.predict(X_ud_train)\n",
        "print(\"F1 score on Training Data \")\n",
        "print(metrics.flat_f1_score(y_ud_train, y_ud_pred_train,average='weighted',labels=ud_crf.classes_))\n",
        "\n",
        "### Look at class wise score\n",
        "print(\"Class wise score:\")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_ud_test, y_ud_pred, labels=ud_crf.classes_, digits=3\n",
        "))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Penn ##\n",
            "F1 score on Test Data\n",
            "0.9668646324625245\n",
            "F1 score on Training Data \n",
            "0.9936643188628935\n",
            "Class wise score:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e425da49c92c>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# This presents class wise score. Helps see which classes (tags) are the ones with most problems.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class wise score:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m print(metrics.flat_classification_report(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0my_penn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_penn_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenn_crf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn_crfsuite/metrics.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(y_true, y_pred, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_true_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my_pred_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn_crfsuite/metrics.py\u001b[0m in \u001b[0;36mflat_classification_report\u001b[0;34m(y_true, y_pred, labels, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: classification_report() takes 2 positional arguments but 3 positional arguments (and 1 keyword-only argument) were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJUcgYacWTaa"
      },
      "source": [
        "Not too shabby!\n",
        "\n",
        "Remember that State of the Art results for Penn Treebank are at 97% f1.\n",
        "\n",
        "Now, notice how UD is worse (90%)? Probably because there aren't many tags, so less variation and less classes for probability distribution.\n",
        "\n",
        "---\n",
        "\n",
        "But, wouldn't it be better if we could see it actually working?\n",
        "\n",
        "That's what the following cell does. It also helps us understand what we'll have to implement in our main algorithm for it to work.\n",
        "\n",
        "Feel free to play with the input phrase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4rcQq-ubbuT",
        "outputId": "35cfa272-0ca4-419f-b74d-3502dc096d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#First, we pass the sentence and \"quickly tokenize it\" - we've already done it in our code, so I'll just mock here with a split:\n",
        "sent = \"The tagger produced good results\"\n",
        "features = [extract_features(sent.split(), idx) for idx in range(len(sent.split()))]\n",
        "\n",
        "#Then we tell the algorithm to make a prediction on a single input (sentence). I'll do once for Penn Treebank and once for UD.\n",
        "penn_results = penn_crf.predict_single(features)\n",
        "ud_results = ud_crf.predict_single(features)\n",
        "\n",
        "#These line magics are just there to make it a neaty print, making a (word, POS) style print;\n",
        "penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]\n",
        "ud_tups = [(sent.split()[idx], ud_results[idx]) for idx in range(len(sent.split()))]\n",
        "\n",
        "#The results come out here! Notice the difference in tags.\n",
        "print(penn_tups)\n",
        "print(ud_tups)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('tagger', 'NN'), ('produced', 'VBN'), ('good', 'JJ'), ('results', 'NNS')]\n",
            "[('The', 'DET'), ('tagger', 'NOUN'), ('produced', 'VERB'), ('good', 'ADJ'), ('results', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mn831Zxbckt"
      },
      "source": [
        "# Saving the Weights\n",
        "\n",
        "We will want to load this to our NLPTools, right? So we have to save the weights. This means saving the classifier we trained to be able to classify our tokens.\n",
        "\n",
        "To do it, we use Pickle, which is a Python package to save a readable binary file extension called \"pickle\". We'll later open this in our tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul2KlQu4c5-N"
      },
      "source": [
        "#import the pickle module\n",
        "import pickle\n",
        "\n",
        "#Simply dump! Use 'wb' in open to write bytes.\n",
        "\n",
        "penn_filename = 'penn_treebank_crf_postagger.sav'\n",
        "pickle.dump(penn_crf, open(penn_filename, 'wb'))\n",
        "\n",
        "ud_filename = 'ud_crf_postagger.sav'\n",
        "pickle.dump(ud_crf, open(ud_filename,'wb'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O2F5HijfWc8"
      },
      "source": [
        "To open the file, we just have to import the module and read the file using:\n",
        "\n",
        "`model = pickle.load(open(filename, 'rb'))`\n",
        "\n",
        "Great, we now have pickle files that can be loaded in our tool. Just download them using the lefthand file explorer and we're good to go!\n",
        "See you back at the article!"
      ]
    }
  ]
}