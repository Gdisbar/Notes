{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRm1VpBiYo9G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeXR-j6gYo9K"
      },
      "source": [
        "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
        "===============================================================================\n",
        "[Original Noitebook](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "\n",
        "    data path : /NLP/data/eng-fra.txt\n",
        "    ------------------------------\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "\n",
        "An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JpUjqezYo9W"
      },
      "source": [
        "Loading data files\n",
        "==================\n",
        "\n",
        "The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "``` {.sourceCode .sh}\n",
        "I am cold.    J'ai froid.\n",
        "```\n",
        "\n",
        "### Word level One-Hot Embedding\n",
        "\n",
        "each word in a language as is a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/word-encoding.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ayQMbA7Yo9Y"
      },
      "source": [
        "\n",
        "class Lang:\n",
        "\n",
        "    word → index (word2index)  \n",
        "    index → word (index2word)\n",
        "    word2count --> replace rare words later.\n",
        "\n",
        "\n",
        "dataset :  \n",
        "\n",
        "    input_lang - fra ,\n",
        "    output_lang - eng,\n",
        "    pairs - ['je suis toujours tres nerveux', 'i m always very nervous']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "# s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "# s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)"
      ],
      "metadata": {
        "id": "PnVWggQWFl4D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VndMqHJSYo9c"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9N_IZMdYo9l"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        " The encoder reads an input\n",
        "sequence and outputs a single vector, and the decoder reads that vector\n",
        "to produce an output sequence.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \\\"meaning\\\" of the input sequence into a single\n",
        "vector --- a single point in some N dimensional space of sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7JelOGVYo9l"
      },
      "source": [
        "The Encoder\n",
        "===========\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence.\n",
        "\n",
        "    EncoderRNN(\n",
        "      (embedding): Embedding(input_size, hidden_size)\n",
        "      (gru): GRU(hidden_size, hidden_size, batch_first=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "\n",
        "For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "    input_seq(input to Encoder): torch.Size([batch_size, seq_length])\n",
        "    Embeded: torch.Size([batch_size, seq_length, hidden_size])\n",
        "    Encoder output: torch.Size([batch_size, seq_length, hidden_size])\n",
        "    Encoder hidden state: torch.Size([1, batch_size, hidden_size])\n",
        "\n",
        "\n",
        "    input_size = Size of the input vocabulary\n",
        "    hidden_size = Size of the hidden state\n",
        "    seq_length = MAX_LENGTH of Sequence\n",
        "    \n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/encoder-network.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlVBkbBuYo9m"
      },
      "source": [
        "The Decoder\n",
        "===========\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "**Encoder output / context vector** : used as the initial hidden state of the decoder.\n",
        "\n",
        "    DecoderRNN(\n",
        "      (embedding): Embedding(output_size, hidden_size)\n",
        "      (gru): GRU(hidden_size, hidden_size, batch_first=True)\n",
        "      (out): Linear(in_features=hidden_size, out_features=output_size, bias=True)\n",
        "    )\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and hidden state.\n",
        "\n",
        "    Decoder Input(Initial):  torch.Size([batch_size, 1])\n",
        "    Decoder Hidden(Initial):  torch.Size([1, batch_size, hidden_size])\n",
        "\n",
        "The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state).\n",
        "\n",
        "    Forward-Step - total MAX_LENGTH no. of times\n",
        "    decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "    Input(Forward-Step):  torch.Size([batch_size, 1])\n",
        "    Output(Forward-Step : Embedding):  torch.Size([batch_size, 1, hidden_size])\n",
        "    Output(Forward-Step : GRU):  torch.Size([batch_size, 1, hidden_size])\n",
        "    Output(Forward-Step : Hidden):  torch.Size([1, batch_size, hidden_size])\n",
        "    Output(Forward-Step : Output):  torch.Size([batch_size, 1, output_size])\n",
        "    Decoder Output(each word):  torch.Size([batch_size, 1, output_size])\n",
        "    Decoder Hidden(each word):  torch.Size([1, batch_size, hidden_size])\n",
        "    Decoder Input (without teacher forcing):  torch.Size([batch_size, 1])\n",
        "\n",
        "if we use\n",
        "\n",
        "    target_tensor = torch.randint(0, output_size, (batch_size , MAX_LENGTH)).to(device)\n",
        "    decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "we'll have - teacher forcing\n",
        "\n",
        "    Decoder Input (teacher forcing):  torch.Size([batch_size, 1])\n",
        "\n",
        "\n",
        "**Teacher forcing** is a training technique for sequence-to-sequence models.\n",
        "\n",
        "During training, the model is fed the **actual target outputs** instead of its own predictions.\n",
        "\n",
        "This helps **stabilize training** and prevent error propagation.\n",
        "\n",
        "In inference, the model uses its **own predictions** from the previous time step.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/decoder-network.png)\n",
        "\n",
        "\n",
        "**Why a custom Feed-Forward method**\n",
        "\n",
        "The decoder's forward function is more complex because it operates in a conditional manner. It takes the context vector, the previous output (during training, this is the target sequence; during inference, this is the model's own predictions), and the previous hidden state to generate the next output. This process of generating outputs conditionally based on previous outputs requires a separate forward function in the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHDx0wgCYo9v"
      },
      "source": [
        "Attention Decoder\n",
        "=================\n",
        "\n",
        "![](https://i.imgur.com/1152PYf.png)\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer `attn`, using the decoder\\'s input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/attention-decoder-network.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    AttnDecoderRNN(\n",
        "      (embedding): Embedding(output_size, hidden_size)\n",
        "      (attention): BahdanauAttention(\n",
        "        (Wa): Linear(in_features=hidden_size, out_features=hidden_size, bias=True)\n",
        "        (Ua): Linear(in_features=hidden_size, out_features=hidden_size, bias=True)\n",
        "        (Va): Linear(in_features=hidden_size, out_features=1, bias=True)\n",
        "      )\n",
        "      (gru): GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "      (out): Linear(in_features=hidden_size, out_features=output_size, bias=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "\n",
        "Forward\n",
        "\n",
        "    Encoder Output(Initial):  torch.Size([batch_size, MAX_LENGTH, hidden_size])\n",
        "    Decoder Input(Initial):  torch.Size([batch_size, 1])\n",
        "    Decoder Hidden(Initial):  torch.Size([1, batch_size, hidden_size])\n",
        "\n",
        "Forward-Step : each word upto MAX_LENGTH\n",
        "\n",
        "    Embedded Shape(Forward-Step): torch.Size([batch_size, 1, hidden_size])\n",
        "    Scores Shape: torch.Size([batch_size, MAX_LENGTH, 1])\n",
        "    Scores re-Shape: torch.Size([batch_size, 1, MAX_LENGTH])\n",
        "    Weights Shape: torch.Size([batch_size, 1, MAX_LENGTH])\n",
        "    Context Shape: torch.Size([batch_size, 1, hidden_size])\n",
        "    Query Shape(Forward-Step): torch.Size([batch_size, 1, hidden_size])\n",
        "    Context Shape(Forward-Step): torch.Size([batch_size, 1, hidden_size])\n",
        "    Input GRU Shape(Forward-Step): torch.Size([batch_size, 1, 2 * hidden_size])\n",
        "    Output(Forward-Step : GRU):  torch.Size([batch_size, 1, hidden_size])\n",
        "    Output(Forward-Step : Hidden):  torch.Size([1, batch_size, hidden_size])\n",
        "    Output(Forward-Step : Output):  torch.Size([batch_size, 1, output_size])\n",
        "    Decoder Output(each word):  torch.Size([batch_size, 1, output_size])\n",
        "    Decoder Hidden(each word):  torch.Size([1, batch_size, hidden_size])\n",
        "    Attention Weights(each word):  torch.Size([batch_size, 1, MAX_LENGTH])\n",
        "    Decoder Input (teacher forcing):  torch.Size([batch_size, 1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap2f841BYo9x"
      },
      "source": [
        "[Local Attention](https://arxiv.org/abs/1508.04025) : uses relative position approch to limit length\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n",
        "indexesFromSentence(lang, sentence) : length of sentence\n",
        "\n",
        "    get each word of the sentence ---> return list of lang.word2index[word]\n",
        "\n",
        "tensorFromSentence(lang,sentence) : tensor.Size([1,,length_of_indFromSentence+1])\n",
        "\n",
        "    add EOS_token in the list of indexFromSentence(..) --> convert to tensor\n",
        "\n",
        "tensorsFromPair(pair) :  (inuttensor,output_tensor)\n",
        "\n",
        "    get each tensor using tensorFromSentence(..,device)\n",
        "  \n",
        "get_dataloader(batch_size):input_lang, output_lang, train_dataloader\n",
        "\n",
        "    get input_lang,output_lang,pairs  using prepareData(..)-->  \n",
        "    input_ids & target_ids : (len(pairs),MAX_LENGTH)\n",
        "    inp_ids = indexFromSentence(input_lang,pairs[0])\n",
        "    inp_ids.append(EOS_token)\n",
        "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "    train_data = TensorDataset(..,use device for both input_ids & target_ids)\n",
        "    train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCufKlJFYo9y"
      },
      "source": [
        "Training the Model\n",
        "==================\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation\n",
        "-intuitively it has learned to represent the output grammar and can\n",
        "\\\"pick up\\\" the meaning once the teacher tells it the first few words,\n",
        "but it has not properly learned how to create the sentence from the\n",
        "translation in the first place.\n",
        "\n",
        "Because of the freedom PyTorch\\'s autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "`teacher_forcing_ratio` up to use more of it.\n",
        "\n",
        "    loss = per epoch , run through entire dataset once forward & backward\n",
        "\n",
        "    train_epoch(dataloader, encoder, decoder, encoder_optimizer,decoder_optimizer, criterion):\n",
        "            add zero_grad() for both optimizers\n",
        "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "\n",
        "    Input:  torch.Size([batch_size, MAX_LENGTH])\n",
        "    Target: torch.Size([batch_size, MAX_LENGTH])\n",
        "    Embeded  torch.Size([batch_size, MAX_LENGTH, hidden_size])\n",
        "    output & hidden  torch.Size([batch_size, MAX_LENGTH, hidden_size]) torch.Size([1, batch_size, hidden_size])\n",
        "    Encoder Output/Context Vector:  torch.Size([batch_size, MAX_LENGTH, hidden_size])\n",
        "    Encoder Hidden:  torch.Size([1, batch_size, hidden_size])\n",
        "    Decoder Output: torch.Size([batch_size, MAX_LENGTH, output_lang.n_words])\n",
        "\n",
        "    input_lang.n_words = 4601,output_lang.n_words = 2991\n",
        "\n",
        "  loss function : criterion = nn.NLLLoss()\n",
        "\n",
        "  NLLLoss is often used to calculate the loss between the predicted probability distribution over words and the actual target word indices. It measures the negative log likelihood of the predicted word under the target distribution.\n",
        "\n",
        "    Decoder Output: torch.Size([batch_size * MAX_LENGTH, output_lang.n_words]) - each row contains the output probabilities for each word in the vocabulary for a given timestep and batch instance.\n",
        "    Target : torch.Size([batch_size * MAX_LENGTH]) - Each element in this tensor corresponds to the index of the target word at each timestep and batch instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URUrUHAXYo97"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "    input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "    encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "    decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "\n",
        "Train\n",
        "\n",
        "    Input:  torch.Size([batch_size, MAX_LENGTH])\n",
        "    Target: torch.Size([batch_size, MAX_LENGTH])\n",
        "    Embeded  torch.Size([batch_size, MAX_LENGTH, hidden_size])\n",
        "    output & hidden  torch.Size([batch_size, MAX_LENGTH, hidden_size]) torch.Size([1, batch_size, hidden_size])\n",
        "    Encoder Output/Context Vector:  torch.Size([batch_size, MAX_LENGTH, hidden_size])\n",
        "    Encoder Hidden:  torch.Size([1, batch_size, hidden_size])\n",
        "    Decoder Output: torch.Size([batch_size, MAX_LENGTH, output_lang.n_words])\n",
        "\n",
        "    input_lang.n_words = 4601,output_lang.n_words = 2991\n",
        "    \n",
        "\n",
        "\n",
        "Evaluation\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "    _, topi = decoder_outputs.topk(1)\n",
        "    decoded_ids = topi.squeeze()\n",
        "\n",
        "\n",
        "Set dropout layers to eval mode\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    > tu es celui que j attendais\n",
        "    = you re the one i ve been waiting for\n",
        "    Embeded(Encoder): torch.Size([1, len(sentence), hidden_size])\n",
        "    output & hidden(Encoder): torch.Size([1, len(sentence), hidden_size]), torch.Size([1, 1,hidden_size])\n",
        "    < i m not a good to be <EOS>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "n-22QHc2nRWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}