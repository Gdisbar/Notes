{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nR-vJR9wJxP-"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Orginal articles\n",
        "\n",
        "[Generating Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)\n",
        "\n",
        "[Classifying Names with a Character-Level RNN](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)\n",
        "\n",
        "\n",
        "RNN related articles :     \n",
        "[The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
        "\n",
        "[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "``` {.sourceCode .sh}\n",
        ">  Russian RUS\n",
        "Rovakov\n",
        "Uantov\n",
        "Shavakov\n",
        "\n",
        ">  German GER\n",
        "Gerren\n",
        "Ereng\n",
        "Rosher\n",
        "\n",
        "> Spanish SPA\n",
        "Salla\n",
        "Parer\n",
        "Allan\n",
        "\n",
        "> Chinese CHI\n",
        "Chan\n",
        "Hang\n",
        "Iun\n",
        "```\n",
        "\n",
        "file structure --> inside (Drive : /NLP/name_data.zip)\n",
        "\n",
        "    ./names/English.txt\n",
        "\n",
        "      Abbas\n",
        "      Abbey\n",
        "      Abbott\n",
        "      Abdi\n",
        "      Abel\n",
        "      Abraham"
      ],
      "metadata": {
        "id": "SVB2_9OH2tpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
        "print(n_letters,all_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "\n",
        "def unicodeToAscii(line):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', line)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# represent the data in - category_lines = {} & all_categories = []\n",
        "# category_lines['English'][:6] = ['Abbas', 'Abbey', 'Abbott', 'Abdi', 'Abel', 'Abraham']\n",
        "# all_categories = ['Dutch', 'Japanese', 'Irish', ...]  # total 18 category i.e n_categories\n"
      ],
      "metadata": {
        "id": "1nV8ZiKB4jwO",
        "outputId": "61c1d4d2-46ef-47ca-ff5a-ddf4b3abbc16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59 abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdQdQ5UtIorG"
      },
      "source": [
        "Creating the Network\n",
        "====================\n",
        "\n",
        "![By Using Linear layers](https://i.imgur.com/jzVrf7f.png)\n",
        "\n",
        "For each timestep (that is, for each letter in a training word) the\n",
        "inputs of the network will be `(category, current letter, hidden state)`\n",
        "and the outputs will be `(next letter, next hidden state)`. So for each\n",
        "training set, we\\'ll need the category, a set of input letters, and a\n",
        "set of output/target letters.\n",
        "\n",
        "Since we are predicting the next letter from the current letter for each\n",
        "timestep, the letter pairs are groups of consecutive letters from the\n",
        "line - e.g. for `\"ABCD<EOS>\"` we would create (\\\"A\\\", \\\"B\\\"), (\\\"B\\\",\n",
        "\\\"C\\\"), (\\\"C\\\", \\\"D\\\"), (\\\"D\\\", \\\"EOS\\\").\n",
        "\n",
        "![](https://i.imgur.com/JH58tXY.png)\n",
        "\n",
        "The category tensor is a [one-hot\n",
        "tensor](https://en.wikipedia.org/wiki/One-hot) of size\n",
        "`<1 x n_categories>`. When training we feed it to the network at every\n",
        "timestep - this is a design choice, it could have been included as part\n",
        "of initial hidden state or some other strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c17w374RIorJ"
      },
      "source": [
        "# Creating Embedding using One-Hot\n",
        "\n",
        "categoryTensor(category): **(1, n_categories)**\n",
        "\n",
        "    tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n",
        "    torch.Size([1, 18])\n",
        "\n",
        "inputTensor(line): **(len(line), 1, n_letters)** i.e 1st to last letter without <EOS> , line = single name/1 line in a category\n",
        "\n",
        "    Abdi --> torch.Size([4, 1, 59])\n",
        "\n",
        "targetTensor(line) : **(len(line))** i.e 2nd letter to <EOS>\n",
        "\n",
        "    Abdi ---> torch.Size([4])\n",
        "\n",
        "\n",
        "**Creating Embedding / Vector representation**\n",
        "\n",
        "    1. take a tensor of 0's of required size\n",
        "    2. categoryTensor : go to that index(li) matching the category i.e tensor[0][li]= 1\n",
        "    3. inputTensor : make tensor[li][0][all_letters.find(letter)] = 1, here letter = line[li]\n",
        "    4. targetTensor : letter_indexes = [all_letters.find(letter)] --> append(n_letters - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    RNN(\n",
        "      (i2h): Linear(in_features=205, out_features=128, bias=True)\n",
        "      (i2o): Linear(in_features=205, out_features=59, bias=True)\n",
        "      (o2o): Linear(in_features=187, out_features=59, bias=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "      (softmax): LogSoftmax(dim=1)\n",
        "    )\n",
        "\n",
        "hidden = rnn.initHidden()\n",
        "\n",
        "    torch.Size([1, hidden_size])\n",
        "\n",
        "hidden_size = 128\n",
        "\n",
        "n_letters/input_size = 59\n",
        "\n",
        "n_categories = 18\n",
        "\n",
        "\n",
        "n_categories + input_size + hidden_size = 205\n",
        "\n",
        "input_size + hidden_size = 187"
      ],
      "metadata": {
        "id": "P5HyHFi5IlHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward pass + Backward Pass + Optimizer with different epochs --->\n",
        "\n",
        "**Initialize**: rnn = RNN(n_letters, 128, n_letters),hidden = rnn.initHidden()\n",
        "\n",
        "**Forward** : In each pass a single letter of the word is passed ,at the end of complete word \"Abdi\" total loss is calculated\n",
        "\n",
        "**Backword & Optimizer** + : The backward pass calculates gradients (**loss.backward()**) and updates the model parameters (**p.data.add_(p.grad.data, alpha=-learning_rate)**).\n",
        "\n",
        "\n",
        "\n",
        "Once the loss calculation is done = 2*len(word) for both Forward & backword\n",
        "\n",
        "Based on the requirement convert target_line_tensor to --->  torch.Size([4, 1]) to match output size output size (torch.Size([1, n_letters])\n",
        "\n",
        "Below is the Forward pass after processing each letter,\n",
        "here it's \"Abdi\"\n",
        "\n",
        "**Epoch**: An epoch is a complete pass through the entire dataset(here the word \"Abdi\").Completing one epoch would mean processing each character in the word once.\n",
        "\n",
        "**Iteration**: each iteration corresponds to processing a single character of the word. So, to process the entire word \"Abdi\", you would need 4 iterations (one for each character).\n",
        "\n",
        "Example : iter = 2,i =4 , so total = 2*4 = 8 (end of all epochs)\n",
        "\n",
        "    input_combined(category, input, hidden) :  torch.Size([1, n_categories + input_size + hidden_size])\n",
        "    hidden(i2h): torch.Size([1, hidden_size])\n",
        "    output(i2o) : torch.Size([1, n_letters])\n",
        "    output_combined(hidden, output) : torch.Size([1, n_letters+hidden_size])\n",
        "    output(o2o) : torch.Size([1, n_letters])\n",
        "    dropout + softmax\n",
        "    each category belonging to one name i=[4] 4-th letter of that name - output, hidden torch.Size([1, 59]) torch.Size([1, 128])\n",
        "    output & loss at each iteration  torch.Size([1, 59]) 4.06482778276716\n",
        "\n",
        "If you set the number of epochs to 1, the training loop would process the word \"Abdi\" once, which means it would iterate through the word 4 times (4 iterations, 1 epoch).\n",
        "\n",
        "If you set the number of epochs to 2, the training loop would process the word \"Abdi\" twice, which means it would iterate through the word 8 times (4 iterations x 2 epochs).\n",
        "\n",
        "for each iteration when we run training for a single word \"Adbi\"\n",
        "\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "            output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "            l = criterion(output, target_line_tensor[i])\n",
        "            loss += l\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        for p in rnn.parameters():\n",
        "            p.data.add_(p.grad.data, alpha=-learning_rate)\n",
        "\n",
        "        avg_loss = loss.item() / input_line_tensor.size(0)\n"
      ],
      "metadata": {
        "id": "os_vD_jdHjpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "gthcyakxYVgI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}