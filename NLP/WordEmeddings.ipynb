{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJ7wRwXIVScO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwSMsq8csrYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82770252-0640-4740-acf9-0ee342c704f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. TF-IDF\n",
        "\n",
        "**Algorithm**: TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus).\n",
        "\n",
        "TF-IDF has a number of applications. It can be used as a weighting factor for:\n",
        "\n",
        "**Information retrieval**: Variations of TF-IDF are used as a weighting factor by search engines to help understand the relevance of a page to a user’s search query\n",
        "\n",
        "**Text mining**: TF-IDF can help quantify what a document is about, which is a central question in text mining\n",
        "\n",
        "**User modeling**: Another application of TF-IDF involves assisting in the creation of models for user behavior and interests, which can then be used by product and content recommendation engines\n"
      ],
      "metadata": {
        "id": "p4MtlZW_s4sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-Mining : TF-IDF"
      ],
      "metadata": {
        "id": "oflbu9Pnye-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "document1 = \"It is going to rain today.\"\n",
        "document2 = \"Today I am not going outside.\"\n",
        "document3 = \"I am going to watch the season premiere.\"\n",
        "documents = [document1, document2, document3]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Get feature names of final words that we will use to tag documents\n",
        "analyze_text = tfidf_vectorizer.build_analyzer()\n",
        "print(f\"Tokenized words in Document 1: {analyze_text(document1)}\")\n",
        "\n",
        "# Vectorize the documents and find feature names\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "print(f\"\\nTF-IDF Matrix representation of documents:\\n{tfidf_matrix.toarray()}\")\n",
        "\n",
        "# Get vocabulary (word indexes)\n",
        "word_indexes = tfidf_vectorizer.vocabulary_\n",
        "print(f\"\\nWord indexes: {word_indexes}\")\n",
        "\n",
        "# Get feature names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"\\nFeature names (terms): {feature_names}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inANvP6osxQx",
        "outputId": "bda17cc4-fcdf-455f-d96c-70b80329381e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words in Document 1: ['it', 'is', 'going', 'to', 'rain', 'today']\n",
            "\n",
            "TF-IDF Matrix representation of documents:\n",
            "[[0.         0.27824521 0.4711101  0.4711101  0.         0.\n",
            "  0.         0.4711101  0.         0.         0.35829137 0.35829137\n",
            "  0.        ]\n",
            " [0.40619178 0.31544415 0.         0.         0.53409337 0.53409337\n",
            "  0.         0.         0.         0.         0.         0.40619178\n",
            "  0.        ]\n",
            " [0.32412354 0.25171084 0.         0.         0.         0.\n",
            "  0.4261835  0.         0.4261835  0.4261835  0.32412354 0.\n",
            "  0.4261835 ]]\n",
            "\n",
            "Word indexes: {'it': 3, 'is': 2, 'going': 1, 'to': 10, 'rain': 7, 'today': 11, 'am': 0, 'not': 4, 'outside': 5, 'watch': 12, 'the': 9, 'season': 8, 'premiere': 6}\n",
            "\n",
            "Feature names (terms): ['am' 'going' 'is' 'it' 'not' 'outside' 'premiere' 'rain' 'season' 'the'\n",
            " 'to' 'today' 'watch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Retrieval : TF-IDF"
      ],
      "metadata": {
        "id": "_gNBa6ctyo1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Web pages\n",
        "documents = [\n",
        "    \"TF-IDF is a numerical statistic used to reflect the importance of a word to a document in a collection or corpus.\",\n",
        "    \"Search engines often use TF-IDF to determine the relevance of a document to a search query.\",\n",
        "    \"The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus.\",\n",
        "    \"In information retrieval, TF-IDF is a common technique used to weigh the importance of words in a document.\",\n",
        "    \"A higher TF-IDF score indicates that a word is more important to a document compared to other words.\",\n",
        "]\n",
        "\n",
        "# User's search query\n",
        "query = \"information retrieval TF-IDF relevance\"\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Transform the search query into TF-IDF representation\n",
        "query_tfidf = tfidf_vectorizer.transform([query])\n",
        "\n",
        "# Calculate cosine similarity between the query and documents\n",
        "cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n",
        "\n",
        "# Sort documents by relevance\n",
        "sorted_indices = cosine_similarities.argsort()[::-1]\n",
        "\n",
        "# Print the most relevant documents\n",
        "print(\"Search results:\")\n",
        "for i, idx in enumerate(sorted_indices):\n",
        "    print(f\"Document {i + 1}: Similarity Score: {round(cosine_similarities[idx],2)}, Content: {documents[idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqcUxVxhsxUS",
        "outputId": "8013ec2a-1989-4306-ea52-b30fd3e242e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search results:\n",
            "Document 1: Similarity Score: 0.41, Content: In information retrieval, TF-IDF is a common technique used to weigh the importance of words in a document.\n",
            "Document 2: Similarity Score: 0.22, Content: Search engines often use TF-IDF to determine the relevance of a document to a search query.\n",
            "Document 3: Similarity Score: 0.08, Content: TF-IDF is a numerical statistic used to reflect the importance of a word to a document in a collection or corpus.\n",
            "Document 4: Similarity Score: 0.07, Content: A higher TF-IDF score indicates that a word is more important to a document compared to other words.\n",
            "Document 5: Similarity Score: 0.05, Content: The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GloVe (Global Vectors for Word Representation)\n",
        "\n",
        "\n",
        "**Algorithm**: GloVe is an unsupervised learning algorithm for obtaining vector representations for words. It combines the global statistics of word co-occurrences in a corpus with a neural network model.\n",
        "\n",
        "https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "    Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download): glove.6B.zip\n",
        "    Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip\n",
        "    Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download): glove.840B.300d.zip\n",
        "    Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n",
        "\n",
        "**NER**: GloVe embeddings can be used as features in NER systems to identify and classify named entities within text documents. By leveraging the contextual information encoded in GloVe embeddings, NER models can achieve improved accuracy.\n",
        "\n",
        "**Text Generation**: In text generation tasks, such as language modeling and dialogue generation, GloVe embeddings can be used as input features to generate coherent and contextually relevant text. By leveraging the semantic relationships captured in GloVe embeddings, text generation models can produce more meaningful and fluent output.\n",
        "\n",
        "**Semantic Similarity**: GloVe embeddings enable computing semantic similarity between words, phrases, or sentences. This is useful in tasks such as duplicate detection, paraphrase identification, and question answering, where understanding the semantic similarity between text units is crucial.\n",
        "\n",
        "**Recommendation Systems**: GloVe embeddings can be used to represent textual content in recommendation systems, such as content-based filtering. By capturing the semantic meaning of items and user preferences, GloVe embeddings can help improve the accuracy and relevance of recommendations.\n",
        "\n"
      ],
      "metadata": {
        "id": "HahSoi9i0Ju3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER : GloVe"
      ],
      "metadata": {
        "id": "9JXtmYXy2o8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy English model with GloVe embeddings\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Sample text with named entities\n",
        "text = \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California. \" \\\n",
        "       \"It designs, manufactures, and sells consumer electronics, computer software, and online services.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print named entities and their labels\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"-\", ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdKXYY-AsxZ8",
        "outputId": "5cfe6f92-af95-481d-8ad3-7d65ed521f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "Apple Inc. - ORG\n",
            "American - NORP\n",
            "Cupertino - GPE\n",
            "California - GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET-Q1fKD2PSj",
        "outputId": "b7835dc2-eab4-4886-8110-605e93de1d51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content/drive/MyDrive/NLP/glove.6B.100d.txt\""
      ],
      "metadata": {
        "id": "37HFq6ZM6rPq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDwkXvr0fTpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paraphrase Detection Semantic Similarity : GloVe\n"
      ],
      "metadata": {
        "id": "u-fU63uYVqFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load SpaCy with GloVe embeddings\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "patterns = [\n",
        "    [{'POS':'ADJ'}, {'POS':'NOUN'}],\n",
        "    ]\n",
        "matcher.add(\"demo\", patterns)\n",
        "\n",
        "\n",
        "# Function to tokenize sentence into phrases\n",
        "def extract_phrases(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    matches = matcher(doc)\n",
        "    phrases = []\n",
        "    for match_id,start,end in matches:\n",
        "      span = doc[start:end]\n",
        "      phrases.append(span.text)\n",
        "    phrases.extend(chunk.text for chunk in doc.noun_chunks)\n",
        "    print(phrases,type(phrases))\n",
        "    return phrases\n",
        "\n",
        "# Function to calculate GloVe embedding for a phrase\n",
        "def phrase_embedding(phrase):\n",
        "    return np.mean([token.vector for token in nlp(phrase)], axis=0)\n",
        "\n",
        "# Function to find most similar paraphrase pair\n",
        "def find_paraphrase(sentence_list):\n",
        "    max_similarity = 0\n",
        "    paraphrase_similarity = dict()\n",
        "\n",
        "    for i in range(len(sentence_list)):\n",
        "        for j in range(i+1, len(sentence_list)):\n",
        "            phrases1 = extract_phrases(sentence_list[i])\n",
        "            phrases2 = extract_phrases(sentence_list[j])\n",
        "\n",
        "            for phrase1 in phrases1:\n",
        "                for phrase2 in phrases2:\n",
        "                    emb1 = phrase_embedding(phrase1)\n",
        "                    emb2 = phrase_embedding(phrase2)\n",
        "                    similarity = cosine_similarity([emb1], [emb2])[0][0]\n",
        "\n",
        "                    paraphrase_pair = (phrase1, phrase2)\n",
        "                    paraphrase_similarity[paraphrase_pair] = similarity\n",
        "\n",
        "    paraphrase_similarity = dict(sorted(paraphrase_similarity.items(),\n",
        "                                        key=lambda item: item[1],reverse=True))\n",
        "    return paraphrase_similarity\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    \"The cat sat on the mat and licked its paws.\",\n",
        "    \"A dog chased the ball and barked loudly.\",\n",
        "    \"The sun set behind the mountains, casting a golden glow.\"\n",
        "]\n",
        "\n",
        "# Find paraphrase pair and similarity\n",
        "paraphrase_similarity = find_paraphrase(sentences)\n",
        "\n",
        "# Print results\n",
        "# print(\"Sentences:\")\n",
        "# for i, sentence in enumerate(sentences):\n",
        "#     print(f\"{i+1}. {sentence}\")\n",
        "i = 0\n",
        "if len(paraphrase_similarity):\n",
        "    for key,val in paraphrase_similarity.items():\n",
        "      print(f\"Paraphrse : {key} Similarity : {val}\")\n",
        "      i+=1\n",
        "      if i>5: break\n",
        "else:\n",
        "    print(\"\\nNo paraphrase pair found.\")\n"
      ],
      "metadata": {
        "id": "xHGjl20osxrm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8e6cad-4c80-4cd2-e3f7-cd7c50db93ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The cat', 'the mat', 'its paws'] <class 'list'>\n",
            "['A dog', 'the ball'] <class 'list'>\n",
            "['The cat', 'the mat', 'its paws'] <class 'list'>\n",
            "['golden glow', 'The sun', 'the mountains', 'a golden glow'] <class 'list'>\n",
            "['A dog', 'the ball'] <class 'list'>\n",
            "['golden glow', 'The sun', 'the mountains', 'a golden glow'] <class 'list'>\n",
            "Paraphrse : ('the mat', 'the mountains') Similarity : 0.7576532363891602\n",
            "Paraphrse : ('the mat', 'the ball') Similarity : 0.7441527843475342\n",
            "Paraphrse : ('the ball', 'the mountains') Similarity : 0.7378790378570557\n",
            "Paraphrse : ('The cat', 'A dog') Similarity : 0.5905861854553223\n",
            "Paraphrse : ('The cat', 'The sun') Similarity : 0.5646107196807861\n",
            "Paraphrse : ('its paws', 'the mountains') Similarity : 0.56061851978302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Document Clustering : GloVe"
      ],
      "metadata": {
        "id": "5ZgNwjbiPbCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to load GloVe embeddings\n",
        "glove_embeddings = {}\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            glove_embeddings[word] = coefs\n",
        "\n",
        "# Function to calculate document embeddings\n",
        "def document_embedding(doc, embeddings_index):\n",
        "    words = doc.split()\n",
        "    word_embeddings = [embeddings_index.get(word, np.zeros(embeddings_index['a'].shape)) for word in words]\n",
        "    doc_embedding = np.mean(word_embeddings, axis=0)\n",
        "    return doc_embedding\n",
        "\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Calculate document embeddings\n",
        "document_vectors = [document_embedding(doc, glove_embeddings) for doc in documents]\n",
        "\n",
        "# Perform K-means clustering\n",
        "num_clusters = 2  # Number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters)\n",
        "kmeans.fit(document_vectors)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "# Print clusters\n",
        "for i in range(num_clusters):\n",
        "    cluster_docs = [documents[j] for j in range(len(documents)) if clusters[j] == i]\n",
        "    print(f'Cluster {i+1}:')\n",
        "    for doc in cluster_docs:\n",
        "        print(doc)\n",
        "    print()\n",
        "\n",
        "# Optionally, print documents in each cluster\n",
        "for cluster_id in range(num_clusters):\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    for i, doc in enumerate(documents):\n",
        "        if clusters[i] == cluster_id:\n",
        "            print(doc)\n",
        "\n",
        "# Reduce dimensionality for visualization\n",
        "pca = PCA(n_components=2)  # 2 principal components for 2D visualization\n",
        "document_embeddings_2d = pca.fit_transform(document_vectors)\n",
        "\n",
        "# Plot document clusters\n",
        "plt.figure(figsize=(4,3))\n",
        "for i in range(num_clusters):\n",
        "    cluster_docs = np.array([document_embeddings_2d[j] for j in range(len(documents)) if clusters[j] == i])\n",
        "    plt.scatter(cluster_docs[:, 0], cluster_docs[:, 1], label=f'Cluster {i+1}')\n",
        "plt.title('Document Clusters (2D PCA)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-FGtudx0PbxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debiasing Word Vectors : GloVe"
      ],
      "metadata": {
        "id": "Dx2L3jCToyQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def read_glove_vecs(path):\n",
        "  words = set()\n",
        "  word_to_vec_map = {}\n",
        "  with open(path,'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      words.add(word)\n",
        "      word_to_vec_map[word] = coefs\n",
        "\n",
        "  i = 1\n",
        "  words_to_index = {}\n",
        "  index_to_words = {}\n",
        "  for word in words:\n",
        "    words_to_index[word] = i\n",
        "    index_to_words[i] = word\n",
        "\n",
        "  return words,word_to_vec_map,words_to_index, index_to_words,\n",
        "\n",
        "words, word_to_vec_map,word_to_index, index_to_word = read_glove_vecs(path)"
      ],
      "metadata": {
        "id": "0tqK1NvcpxWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Female first names tend to have a positive cosine similarity with our constructed vector\n",
        ", while male first names tend to have a negative cosine similarity.\n",
        "\n",
        "We see “computer” is negative and is closer in value to male first names, while “literature” is positive and is closer to female first names."
      ],
      "metadata": {
        "id": "ekI7GgH3vQIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
        "print(f\"g shape : {g.shape}\")\n",
        "\n",
        "print ('List of names and their similarities with constructed vector:')\n",
        "\n",
        "# girls and boys name\n",
        "name_list = ['john', 'marie', 'sophie', 'ronaldo', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'yasmin']\n",
        "\n",
        "for w in name_list:\n",
        "    # Reshape vectors to be 2D arrays with a single row\n",
        "    similarity = cosine_similarity(word_to_vec_map[w].reshape(1, -1), g.reshape(1, -1))[0][0]\n",
        "    print(w, similarity)\n",
        "\n",
        "print(\"=\"*300)\n",
        "print ('List of names and their similarities with constructed vector:')\n",
        "\n",
        "# girls and boys name\n",
        "name_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist',\n",
        "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
        "for w in name_list:\n",
        "    # Reshape vectors to be 2D arrays with a single row\n",
        "    similarity = cosine_similarity(word_to_vec_map[w].reshape(1, -1), g.reshape(1, -1))[0][0]\n",
        "    print(w, similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kkKr3Q8vm7l",
        "outputId": "4cdad46a-36b7-4a73-fd61-da0215be086e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "g shape : (100,)\n",
            "List of names and their similarities with constructed vector:\n",
            "john -0.22835018\n",
            "marie 0.2453734\n",
            "sophie 0.20268358\n",
            "ronaldo -0.3328965\n",
            "priya 0.13922855\n",
            "rahul -0.06390728\n",
            "danielle 0.14913149\n",
            "reza -0.081926554\n",
            "katy 0.18688588\n",
            "yasmin 0.21136825\n",
            "============================================================================================================================================================================================================================================================================================================\n",
            "List of names and their similarities with constructed vector:\n",
            "lipstick 0.18037248\n",
            "guns -0.099644475\n",
            "science -0.021475762\n",
            "arts 0.014846751\n",
            "literature 0.08261855\n",
            "warrior -0.15634203\n",
            "doctor 0.10942282\n",
            "tree -0.0886836\n",
            "receptionist 0.2806876\n",
            "technology -0.14474528\n",
            "fashion 0.080974355\n",
            "teacher 0.15233697\n",
            "engineer -0.123000115\n",
            "pilot -0.041133933\n",
            "computer -0.115457155\n",
            "singer 0.113726415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Given an input embedding $e$, you can use the following formulas to compute $e^{debiased}$:\n",
        "\n",
        "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g\\tag{2}$$\n",
        "$$e^{debiased} = e - e^{bias\\_component}\\tag{3}$$\n",
        "\n",
        "recognize $e^{bias\\_component}$ is the projection of $e$ onto the direction $g$ ensuring that gender neutral words are zero in the gender subspace."
      ],
      "metadata": {
        "id": "awk6ROqWwZw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neutralize(word, g, word_to_vec_map):\n",
        "    e = word_to_vec_map[word]\n",
        "    e_biascomponent = np.dot(e, g) / np.square(np.linalg.norm(g, ord=2)) * g\n",
        "    e_debiased = e - e_biascomponent\n",
        "    return e_debiased\n",
        "\n",
        "e = \"receptionist\"\n",
        "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"].reshape(1,-1), g.reshape(1,-1)))\n",
        "\n",
        "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
        "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased.reshape(1,-1), g.reshape(1,-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acN8vKEouGM",
        "outputId": "a406ed9b-0c63-45aa-d7aa-97fe79db929e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine similarity between receptionist and g, before neutralizing:  [[0.2806876]]\n",
            "cosine similarity between receptionist and g, after neutralizing:  [[-8.1490725e-09]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The derivation of the linear algebra to do this is a bit more complex. (See Bolukbasi et al., 2016 in the References for details.) Here are the key equations:\n",
        "\n",
        "\n",
        "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}\\tag{4}$$\n",
        "\n",
        "$$ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "\\tag{5}$$\n",
        "\n",
        "$$\\mu_{\\perp} = \\mu - \\mu_{B} \\tag{6}$$\n",
        "\n",
        "$$ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "\\tag{7}$$\n",
        "$$ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
        "\\tag{8}$$\n",
        "\n",
        "\n",
        "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B||_2} \\tag{9}$$\n",
        "\n",
        "\n",
        "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {||(e_{w2} - \\mu_{\\perp}) - \\mu_B||_2} \\tag{10}$$\n",
        "\n",
        "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} \\tag{11}$$\n",
        "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} \\tag{12}$$\n"
      ],
      "metadata": {
        "id": "27w9fY4EyOGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equalize(pair, bias_axis, word_to_vec_map):\n",
        "    w1, w2 = pair\n",
        "    e_w1, e_w2 = [word_to_vec_map[w] for w in [w1, w2]]\n",
        "    mu = np.mean(np.concatenate((e_w1, e_w2), axis=0))\n",
        "    mu_B = np.dot(mu, bias_axis) / np.square(np.linalg.norm(bias_axis, ord=2)) * bias_axis\n",
        "    mu_orth = mu - mu_B\n",
        "    e_w1B = np.dot(e_w1, bias_axis) / np.square(np.linalg.norm(bias_axis, ord=2)) * bias_axis\n",
        "    e_w2B = np.dot(e_w2, bias_axis) / np.square(np.linalg.norm(bias_axis, ord=2)) * bias_axis\n",
        "    corrected_e_w1B = np.sqrt(np.abs(1 - np.square(np.linalg.norm(mu_orth)))) * (e_w1B - mu_B) / np.linalg.norm(e_w1 - mu_orth - mu_B)\n",
        "    corrected_e_w2B = np.sqrt(np.abs(1 - np.square(np.linalg.norm(mu_orth)))) * (e_w2B - mu_B) / np.linalg.norm(e_w2 - mu_orth - mu_B)\n",
        "    e1 = corrected_e_w1B + mu_orth\n",
        "    e2 = corrected_e_w2B + mu_orth\n",
        "    return e1, e2\n",
        "\n",
        "print(\"cosine similarities before equalizing:\")\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"].reshape(1,-1), g.reshape(1,-1)))\n",
        "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"].reshape(1,-1), g.reshape(1,-1)))\n",
        "print(\"=\"*300)\n",
        "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
        "print(\"cosine similarities after equalizing:\")\n",
        "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1.reshape(1,-1), g.reshape(1,-1)))\n",
        "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2.reshape(1,-1), g.reshape(1,-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOU-bi_IouNe",
        "outputId": "92df65ee-0937-4e39-bb9d-0c179e1f0e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine similarities before equalizing:\n",
            "cosine_similarity(word_to_vec_map[\"man\"], gender) =  [[-0.18769068]]\n",
            "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  [[0.388177]]\n",
            "============================================================================================================================================================================================================================================================================================================\n",
            "cosine similarities after equalizing:\n",
            "cosine_similarity(e1, gender) =  [[-0.13731493]]\n",
            "cosine_similarity(e2, gender) =  [[0.55300444]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-Answer : GloVe"
      ],
      "metadata": {
        "id": "lliMJs3Jk9yX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText Embeddings\n",
        "\n",
        "**Document Clustering**: FastText embeddings facilitate document clustering tasks by capturing the semantic similarity between documents. This enables grouping similar documents together, which is useful in various applications such as organizing news articles, customer reviews, or academic papers.\n",
        "\n",
        "**Machine Translation**: FastText embeddings are employed in machine translation systems to improve translation quality, especially for translating languages with complex morphology or a high degree of inflection. By capturing subword information, FastText embeddings help handle rare or unseen words effectively.\n",
        "\n",
        "**Recommendation Systems**: FastText embeddings can be utilized in recommendation systems to model user preferences and item similarities. They enable the system to understand the semantics of user-item interactions, leading to more personalized and accurate recommendations.\n"
      ],
      "metadata": {
        "id": "QZzBnqm3eGbZ"
      }
    }
  ]
}