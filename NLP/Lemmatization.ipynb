{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TJWHhSe0DzOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('content/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8pWjy3-DATv",
        "outputId": "25d1c1a9-219e-495b-b125-725cf2b712f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here are some advanced linguistic processing techniques that can be used to enhance lemmatization:\n",
        "\n",
        "1. Part-of-Speech (POS) Tagging: POS tagging is the process of assigning grammatical tags to words in a sentence, such as noun, verb, adjective, etc. POS tags can provide valuable information for disambiguating the correct lemmas. For example, the word \"running\" can be a verb or a noun, but knowing its POS tag can help determine the appropriate lemma.\n",
        "\n",
        "2. Morphological Analysis: Morphological analysis involves breaking down words into their constituent morphemes, such as prefixes, suffixes, and roots. By identifying and manipulating these morphemes, lemmatizers can handle inflectional and derivational forms more accurately. Tools like morphological analyzers and finite-state transducers can be utilized for this purpose.\n",
        "\n",
        "3. Statistical Lemmatization: Statistical approaches leverage machine learning algorithms to learn lemmatization patterns from annotated training data. These models can capture complex relationships between word forms and their corresponding lemmas, improving accuracy for irregular and less common words. Techniques like sequence labeling (e.g., Conditional Random Fields) and sequence-to-sequence models (e.g., Recurrent Neural Networks, Transformers) can be employed.\n",
        "\n",
        "4. Language-Specific Resources: Language-specific linguistic resources, such as dictionaries, lexicons, and specialized corpora, can provide valuable insights into lemmatization. These resources may include irregular word forms, exceptions to general rules, and specific linguistic phenomena unique to a particular language.\n",
        "\n",
        "5. Contextual Information: Incorporating contextual information, such as surrounding words or syntactic structure, can aid in disambiguating the correct lemmas. Context-aware lemmatizers can take advantage of contextual cues to make more informed decisions. Dependency parsing, word embeddings, and contextual language models (e.g., BERT, GPT) can assist in capturing and utilizing contextual information.\n",
        "\n",
        "6. Evaluation and Error Analysis: It is crucial to evaluate the performance of lemmatizers on annotated test data and conduct error analysis to identify and address common errors. This iterative process helps refine the lemmatization techniques and fine-tune parameters, ensuring better accuracy and coverage.\n",
        "\n",
        "It's important to note that implementing these techniques can be complex, and it often requires specialized libraries, linguistic resources, and domain expertise. Several open-source NLP libraries, such as NLTK, spaCy, and StanfordNLP, offer pre-built components and models for various linguistic tasks, including lemmatization, that incorporate many of these advanced techniques.\n",
        "\n",
        "By combining multiple approaches and techniques, you can develop a more accurate and robust lemmatizer that handles a wide range of word forms, linguistic phenomena, and language-specific challenges."
      ],
      "metadata": {
        "id": "ZI5Zudx4bsEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLWVRh3bbUNB",
        "outputId": "f2d040cc-8152-44af-a438-5b20d0255474"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "class CorpusBasedLemmatizer:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = nltk.WordNetLemmatizer()\n",
        "    \n",
        "    def lemmatize(self, word):\n",
        "        lemma = self.lemmatizer.lemmatize(word)\n",
        "        if lemma == word:\n",
        "            synsets = wn.synsets(word)\n",
        "            if synsets:\n",
        "                lemma = synsets[0].lemmas()[0].name()\n",
        "        return lemma\n",
        "\n",
        "\n",
        "corpus_lemmatizer = CorpusBasedLemmatizer()\n",
        "words = \"The striped bat are hanging on their foot for best\".split()\n",
        "\n",
        "for word in words:\n",
        "    # rule_lemma = rule_lemmatizer.lemmatize(word)\n",
        "    corpus_lemma = corpus_lemmatizer.lemmatize(word)\n",
        "    print(f\"Word: {word}\\tCorpus Lemma: {corpus_lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NhVdTUXDAr0",
        "outputId": "84149614-a868-460f-d1f4-f0e710cbfeb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: The\tCorpus Lemma: The\n",
            "Word: striped\tCorpus Lemma: stripe\n",
            "Word: bat\tCorpus Lemma: bat\n",
            "Word: are\tCorpus Lemma: are\n",
            "Word: hanging\tCorpus Lemma: hanging\n",
            "Word: on\tCorpus Lemma: on\n",
            "Word: their\tCorpus Lemma: their\n",
            "Word: foot\tCorpus Lemma: foot\n",
            "Word: for\tCorpus Lemma: for\n",
            "Word: best\tCorpus Lemma: best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On sentence identifies POS and lemmatize accordingly "
      ],
      "metadata": {
        "id": "619vYgw9vZx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "class RuleBasedLemmatizer:\n",
        "    def __init__(self):\n",
        "        self.rules = [\n",
        "            (r\"(s|es|ies)$\", \"\"),\n",
        "            (r\"([^aeiou])s$\", r\"\\1\"),\n",
        "            (r\"ing$\", \"\"),\n",
        "            (r\"([aeiou].+)ed$\", r\"\\1\"),\n",
        "            (r\"([aeiou].+)ed$\", r\"\\1\"),\n",
        "            (r\"([aeiou].+)y$\", r\"\\1\"),\n",
        "        ]\n",
        "    \n",
        "    def lemmatize(self, word, pos):\n",
        "        for rule in self.rules:\n",
        "            pattern, replacement = rule\n",
        "            if re.search(pattern, word):\n",
        "                return re.sub(pattern, replacement, word)\n",
        "        \n",
        "        # Handle complex cases based on POS\n",
        "        if pos.startswith('V'):  # Verb\n",
        "            return self.lemmatize_verb(word)\n",
        "        elif pos.startswith('N'):  # Noun\n",
        "            return self.lemmatize_noun(word)\n",
        "        elif pos.startswith('J'):  # Adjective\n",
        "            return self.lemmatize_adjective(word)\n",
        "        elif pos.startswith('R'):  # Adverb\n",
        "            return self.lemmatize_adverb(word)\n",
        "        \n",
        "        return word\n",
        "    \n",
        "    def lemmatize_verb(self, word):\n",
        "        lemmatizer = nltk.WordNetLemmatizer()\n",
        "        lemma = lemmatizer.lemmatize(word, 'v')\n",
        "        return lemma\n",
        "    \n",
        "    def lemmatize_noun(self, word):\n",
        "        lemmatizer = nltk.WordNetLemmatizer()\n",
        "        lemma = lemmatizer.lemmatize(word, 'n')\n",
        "        return lemma\n",
        "    \n",
        "    def lemmatize_adjective(self, word):\n",
        "        lemmatizer = nltk.WordNetLemmatizer()\n",
        "        lemma = lemmatizer.lemmatize(word, 'a')\n",
        "        return lemma\n",
        "    \n",
        "    def lemmatize_adverb(self, word):\n",
        "        lemmatizer = nltk.WordNetLemmatizer()\n",
        "        lemma = lemmatizer.lemmatize(word, 'r')\n",
        "        return lemma\n",
        "\n",
        "\n",
        "rule_lemmatizer = RuleBasedLemmatizer()\n",
        "\n",
        "sentence = \"The cats are running and jumping playfully.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "lemmas = []\n",
        "for token in tokens:\n",
        "    pos = nltk.pos_tag([token])[0][1]\n",
        "    rule_lemma = rule_lemmatizer.lemmatize(token, pos)\n",
        "    lemmas.append((token, rule_lemma))\n",
        "\n",
        "print(\"Original sentence:\", sentence)\n",
        "print(\"Token\\tRule Lemma\")\n",
        "for lemma in lemmas:\n",
        "    print(f\"{lemma[0]}\\t{lemma[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRNdwjZlywhn",
        "outputId": "198e5b50-6b11-49e1-9700-68d1e67c223e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: The cats are running and jumping playfully.\n",
            "Token\tRule Lemma\n",
            "The\tThe\n",
            "cats\tcat\n",
            "are\tbe\n",
            "running\trunn\n",
            "and\tand\n",
            "jumping\tjump\n",
            "playfully\tplayfull\n",
            ".\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Spacy"
      ],
      "metadata": {
        "id": "FgcFPxwC0MP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
        "\n",
        "sentence = \"The cats are running and jumping playfully.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "print(\"Original sentence:\", sentence)\n",
        "print(\"Token\\tRule Lemma\")\n",
        "for token in doc:\n",
        "  print(f\"{token} \\t {token.lemma_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6qFNy620OfD",
        "outputId": "2ee8a70f-73fb-4e6e-ebc2-c357cda72c46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: The cats are running and jumping playfully.\n",
            "Token\tRule Lemma\n",
            "The \t the\n",
            "cats \t cat\n",
            "are \t be\n",
            "running \t run\n",
            "and \t and\n",
            "jumping \t jump\n",
            "playfully \t playfully\n",
            ". \t .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RuleBasedLemmatizer & CorpusBasedLemmatizer"
      ],
      "metadata": {
        "id": "kEh30odUZCwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class RuleBasedLemmatizer:\n",
        "    def __init__(self):\n",
        "        self.rules = [\n",
        "            (r\"(s|es|ies)$\", \"\"),\n",
        "            (r\"([^aeiou])s$\", r\"\\1\"),\n",
        "            (r\"ing$\", \"\"),\n",
        "            (r\"([aeiou].+)ed$\", r\"\\1\"),\n",
        "            (r\"([aeiou].+)ed$\", r\"\\1\"),\n",
        "            (r\"([aeiou].+)y$\", r\"\\1\"),\n",
        "        ]\n",
        "    \n",
        "    def lemmatize(self, word):\n",
        "        for rule in self.rules:\n",
        "            pattern, replacement = rule\n",
        "            if re.search(pattern, word):\n",
        "                return re.sub(pattern, replacement, word)\n",
        "        return word\n",
        "\n",
        "\n",
        "class CorpusBasedLemmatizer:\n",
        "    def __init__(self, corpus):\n",
        "        self.lemmas = {}\n",
        "        for line in corpus:\n",
        "            word, lemma = line.strip().split(\"\\t\")\n",
        "            self.lemmas[word] = lemma\n",
        "    \n",
        "    def lemmatize(self, word):\n",
        "        return self.lemmas.get(word, word)\n",
        "\n",
        "\n",
        "rule_lemmatizer = RuleBasedLemmatizer()\n",
        "corpus = [\n",
        "    \"cats\\tcat\",\n",
        "    \"dogs\\tdog\",\n",
        "    \"running\\trun\",\n",
        "    \"jumping\\tjump\",\n",
        "    \"children\\tchild\",\n",
        "]\n",
        "corpus_lemmatizer = CorpusBasedLemmatizer(corpus)\n",
        "\n",
        "words = [\"cats\", \"running\", \"jumping\", \"children\"]\n",
        "\n",
        "for word in words:\n",
        "    rule_lemma = rule_lemmatizer.lemmatize(word)\n",
        "    corpus_lemma = corpus_lemmatizer.lemmatize(word)\n",
        "    print(f\"Word: {word}\\tRule Lemma: {rule_lemma}\\tCorpus Lemma: {corpus_lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMbfLuLaDAvM",
        "outputId": "eecd4ebe-8bcd-4a5a-f643-cb358dba888a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: cats\tRule Lemma: cat\tCorpus Lemma: cat\n",
            "Word: running\tRule Lemma: runn\tCorpus Lemma: run\n",
            "Word: jumping\tRule Lemma: jump\tCorpus Lemma: jump\n",
            "Word: children\tRule Lemma: children\tCorpus Lemma: child\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJO7Bg5AzxM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdJz4aPLzxaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Morphological Analysis using Finite-State Transducers (FSTs)\n",
        "\n",
        "Finite-State Transducers (FSTs) can be used for morphological analysis by encoding morphological rules and transformations into a computational model. Here's an example of how to use the OpenFST library in Python to construct and apply an FST for morphological analysis:\n",
        "\n",
        "First, you'll need to install the OpenFST library. You can find installation instructions specific to your operating system on the OpenFST website (http://www.openfst.org).\n",
        "\n",
        "Once OpenFST is installed, you can use the pywrapfst module in Python to work with FSTs. Here's an example code snippet:"
      ],
      "metadata": {
        "id": "alKccLGIY_Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pynini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlKW-VwKZbFL",
        "outputId": "7ec42d7c-1f41-4231-cd49-de7a69a6ecbb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we use the pynini.Fst() class to create an FST and add transitions based on the provided rules. The morphological_analysis() function takes an input word and the morphological FST, composes them, and retrieves the morphological analysis using the shortest path algorithm.\n",
        "\n",
        "Please note that pynini may have some differences in usage compared to hfst, but the basic concepts and functionality for working with FSTs remain similar. This example demonstrates a simple morphological analysis where \"a\" remains unchanged, \"b\" is replaced with \"c,\" and \"c\" is replaced with \"d\" when the word \"abc\" is analyzed."
      ],
      "metadata": {
        "id": "aR6WW8K0au_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pynini\n",
        "\n",
        "def build_morphological_fst(rules):\n",
        "    morph_fst = pynini.Fst()\n",
        "\n",
        "    # Define the input and output symbol tables\n",
        "    input_syms = pynini.SymbolTable()\n",
        "    output_syms = pynini.SymbolTable()\n",
        "\n",
        "    # Iterate over the rules and add transitions to the FST\n",
        "    for rule in rules:\n",
        "        input_label, output_label = rule\n",
        "\n",
        "        # Add the labels to the symbol tables\n",
        "        input_id = input_syms.add_symbol(input_label)\n",
        "        output_id = output_syms.add_symbol(output_label)\n",
        "\n",
        "        # Add a state and transition to the FST\n",
        "        state = morph_fst.add_state()\n",
        "        morph_fst.add_arc(state, pynini.Arc(input_id, output_id, None, state))\n",
        "\n",
        "    # Set the start and final states\n",
        "    morph_fst.set_start(0)\n",
        "    morph_fst.set_final(morph_fst.add_state())\n",
        "\n",
        "    # Set the input and output symbol tables\n",
        "    morph_fst.set_input_symbols(input_syms)\n",
        "    morph_fst.set_output_symbols(output_syms)\n",
        "\n",
        "    return morph_fst\n",
        "\n",
        "def morphological_analysis(word, morph_fst):\n",
        "    input_fst = pynini.Fst()\n",
        "    input_fst.add_state()\n",
        "    input_fst.set_start(0)\n",
        "\n",
        "    # Add transitions to the FST based on input word\n",
        "    for i, char in enumerate(word):\n",
        "        input_id = morph_fst.input_symbols().find(char)\n",
        "        input_fst.add_arc(i, pynini.Arc(input_id, input_id, None, i + 1))\n",
        "        input_fst.set_final(i + 1)\n",
        "\n",
        "    # Compose the input FST with the morphological FST\n",
        "    composed_fst = pynini.compose(input_fst, morph_fst)\n",
        "\n",
        "    # Get the shortest path to extract the analysis\n",
        "    shortest_path = pynini.shortestpath(composed_fst)\n",
        "\n",
        "    # Extract the analysis results\n",
        "    analysis = shortest_path.paths().ostrings()\n",
        "    return analysis\n",
        "\n",
        "# Example usage\n",
        "rules = [\n",
        "    ('a', 'a'),   # Rule: a -> a\n",
        "    ('b', 'c'),   # Rule: b -> c\n",
        "    ('c', 'd')    # Rule: c -> d\n",
        "]\n",
        "\n",
        "morph_fst = build_morphological_fst(rules)\n",
        "\n",
        "word = \"abc\"\n",
        "analysis = morphological_analysis(word, morph_fst)\n",
        "print(f\"Morphological analysis of '{word}': {analysis}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "AU5dby3_Y7fp",
        "outputId": "3607e514-7f5d-4138-ff30-79d764a468ee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FstIndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFstIndexError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4644301e65f8>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"abc\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorphological_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph_fst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Morphological analysis of '{word}': {analysis}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-4644301e65f8>\u001b[0m in \u001b[0;36mmorphological_analysis\u001b[0;34m(word, morph_fst)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmorph_fst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0minput_fst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_arc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpynini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0minput_fst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Compose the input FST with the morphological FST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mextensions/_pywrapfst.pyx\u001b[0m in \u001b[0;36m_pywrapfst.MutableFst.set_final\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mextensions/_pywrapfst.pyx\u001b[0m in \u001b[0;36m_pywrapfst.MutableFst._set_final\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFstIndexError\u001b[0m: State index out of range"
          ]
        }
      ]
    }
  ]
}